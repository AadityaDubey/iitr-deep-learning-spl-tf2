{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 16 – Natural Language Processing with RNNs and Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting a sequence into batches of shuffled windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's split the sequence 0 to 14 into windows of length 5, each shifted by 2 (e.g.,`[0, 1, 2, 3, 4]`, `[2, 3, 4, 5, 6]`, etc.), then shuffle them, and split them into inputs (the first 4 steps) and targets (the last 4 steps) (e.g., `[2, 3, 4, 5, 6]` would be split into `[[2, 3, 4, 5], [3, 4, 5, 6]]`), then create batches of 3 such input/target pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Batch 0 \n",
      "X_batch\n",
      "[[6 7 8 9]\n",
      " [2 3 4 5]\n",
      " [4 5 6 7]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 7  8  9 10]\n",
      " [ 3  4  5  6]\n",
      " [ 5  6  7  8]]\n",
      "____________________ Batch 1 \n",
      "X_batch\n",
      "[[ 0  1  2  3]\n",
      " [ 8  9 10 11]\n",
      " [10 11 12 13]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 1  2  3  4]\n",
      " [ 9 10 11 12]\n",
      " [11 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n_steps = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.batch(3).prefetch(1)\n",
    "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
    "    print(X_batch.numpy())\n",
    "    print(\"=\" * 5, \"\\nY_batch\")\n",
    "    print(Y_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 31370 steps\n",
      "Epoch 1/10\n",
      "31370/31370 [==============================] - 6052s 193ms/step - loss: 1.4671TA - ETA: 6:04 - loss: 1.4 - ETA:\n",
      "Epoch 2/10\n",
      "31370/31370 [==============================] - 6052s 193ms/step - loss: 1.3614\n",
      "Epoch 3/10\n",
      "31370/31370 [==============================] - 6048s 193ms/step - loss: 1.34049:49 - - ETA: 9:3 - ETA: 9:33  -  - ETA: 8:50 - loss:  - ETA: 8:4  - ETA: 8:40 - lo - ETA: 8:33 - lo - ETA: - ETA: 8:17 -  - ETA: 8:15 - loss: 1.34 - ETA: 8:14  - ETA: 8:01 - ETA: 7:58 - loss: - ETA: 7:52 - los - ET - ETA: 7:41 - loss: 1 - ETA: 7:40 - loss: - ETA: 7:38 - loss: 1.3 - ETA: 7:33 - ETA: 7:24 - ETA: 7:16 - lo - ETA: 7:14 - - ETA: 7: - ETA: 7:03  - ETA: 7:01 - loss:  - ETA: 7:00 - loss: - - ETA: 6:4 - ETA: 6:40 - loss:  - ETA: 6:39 -  - ETA:  - ETA: 6:22 - loss: 1.34 - ETA: 6:22 - loss - ETA - ETA: 6:16 - loss: 1.340 - ETA: 6:16 - loss: 1.3 - ETA: 6:16 - loss: 1.340 - ETA: 6:15 - l - ETA: 6:13 - los - ETA: 6:06 - loss: 1.34 - ETA: - ETA: 3s - loss: 1.3 - ETA: 3s - ETA: 0s - loss: 1.340\n",
      "Epoch 4/10\n",
      "31370/31370 [==============================] - 6048s 193ms/step - loss: 1.3311\n",
      "Epoch 5/10\n",
      "31370/31370 [==============================] - ETA: 0s - loss: 1.3246  - ETA: 4:41 -  - ETA:  -  - ETA: 4:20 - los - ETA: 4: - ETA: 4:04 - loss: 1.32 - ETA:  - ETA: 4 - ETA: 3 - ETA: 0s - loss: 1.32 - 6044s 193ms/step - loss: 1.3246\n",
      "Epoch 6/10\n",
      "31370/31370 [==============================] - 6041s 193ms/step - loss: 1.319110:0 - ETA: 10:03 - ETA: 9  - ETA: 9:35  - ET - ETA:  - ETA:  - ETA: 9:21 - l - ETA: 9:19 - loss: 1.320 - ETA: 9:18 - loss: 1 - E - ETA: 9:13 - loss: 1 - ET - ETA: 9:03  - ETA: 9:00 - loss: 1.32  - ETA: 8:28 - loss: 1.320 - ETA: 8:28 - loss: 1. - ETA:  - ETA: 7:53 - loss: - ETA: 7:51 - l - ETA: 7:44 - loss - ETA: 7:32 - loss: 1.319 - ETA: 7:32 - loss: - ETA: 7:25 - ETA: 7:22 - loss: 1.31 - ETA: 7:22 - l - ETA: 7:20 - loss: 1.3 - ETA: 7:19 - loss: 1.31 - ETA: 7:19 - loss: 1.31 - ETA: 7:19 - loss: 1 - ETA: 7:18 - loss: 1.319 - ET - ETA: 7:08 - los -  - ETA: 5:54 - loss: 1. - ETA: - ETA: 5:49 - loss: 1.3 - ETA: 5:49 - lo  - ETA: 5:37 - loss - ETA: 5:35 - lo - ETA: 5:33 - loss: 1 - ETA: 5:32 -  - ETA: 5:30 -  - ETA: 5:28 - loss: 1.31 - ETA: - ETA: 5:23 - loss: 1 - ETA: 5:22 - loss: 1.31 - ETA: 5: - ETA: 5:19 - loss: 1.3 - ETA: 5:18 - los - ETA: 5:16 - loss - ETA: 5:15 - loss:  - ETA:  - ETA: 3:47 - loss: - ETA: 3:46 - ETA: 3:43 - loss: 1 - ETA: 3 - E - ETA: - ETA: 3: - ETA: 3:22 - l - ETA: 3:19 - loss: 1 - ETA: 3:18 - loss: 1 - ETA: 3:17 - loss: 1.31 - ETA: - ETA: 3:13 - loss: - ETA: 3:12 - loss:  - ETA: 3:11 - loss - ETA: 3:09 - los -  - ETA: 2:42 - loss: 1.31 - ETA: 2:42 - loss: 1. - ETA: 2:41 - - ETA:  - - ETA: 2:30 - loss:  - ETA: 9s - loss: 1.31 - ETA: 9s - l - ETA: 2s - lo - ETA: 0s - loss: 1.31\n",
      "Epoch 7/10\n",
      "31370/31370 [==============================] - 6043s 193ms/step - loss: 1.315910:04 - loss: 1. - ETA: 10:03 - loss: 1. - ETA: 10:00 - los - ETA: 9:59 - loss - ETA: 9:58 - loss: 1.3 - ETA: 9:57 - loss: 1.31 - ETA: 9:57 - loss: 1.317 - ETA: 9:56 - l - - ETA: 9:44  - ETA: 9 - ETA: 9:28 - loss: 1. - ETA: 9:22 -  - ETA: 9:19 - loss - ETA: - ETA: 9:14 - loss: 1.31 - ET - ETA: 9:09 - lo - ETA: 9:08 - loss: 1.3 - ETA: 9:07 - loss: 1.3 - ETA: 9:06 - l - - ETA: 8:59  - ETA: 8:57 - loss: 1.31 - ETA: 8:56 - los - ETA: 8:55 - loss:  - ETA: 8:48 - loss:  - ETA - ETA: 8:38 - - ETA: 8:35 - ETA: 8:27 - l - ETA: 8 - ETA: 8:22 -  - ETA: 8:14 - los - ETA: 8 - ETA: 6:35 - loss: - ETA: 6:34 - loss - ETA: 6:32 - loss: 1. - ETA: 6:32 - loss: 1 - E - ETA: 6:26 - loss: 1 - E - ETA: 6:21 - loss: - ETA: 4 - ETA: 4:11 - - ETA:  - ETA: 4:05 - lo - ETA: 4:03 - loss: - ETA:  - ETA: 3:53 - los - ETA: 3:51 - loss - ETA: 3:50 - loss: - ETA: 3:48 - los - ETA: 3:47 - loss - ETA: 3:45 - loss: 1.3 - ETA: 3: - - ETA: 3:26 - l - ETA: 3:19 -  -  - ETA: 3:07  - - ETA: 2:49 - loss:  - ETA: 2:47  - ETA: 2:45 -  - ETA: 2:42 - loss:  - ETA: - ETA: 2:32 - loss: 1.3 - ETA:  - ETA: 2 - ETA: 2:25 - loss: - ETA: 2:23 - loss: - ETA:  - ETA: 2:18 - loss: 1. - ETA: 2:17 - loss: 1.3 - ETA: 2:17 - loss: 1. - ETA: 2:16 - loss: 1 - ETA:  - ETA: 2:11 - loss: 1.315 - - ETA: 1:56 - loss: - ETA: 1:55 - loss: - ETA: 1:53 - loss: 1 -  - ETA: 1:27 - loss: 1.315 - ETA: 1:27 - loss: 1 -  - ETA: 1:16 - loss: 1 - ETA: 1 - ETA: 1:12 - - ETA: 31s  - ETA: 30s - loss: 1. - ETA: 30s - loss:  - ETA: 26s - loss:  - ETA - - ETA: 22 - ET - ETA: 7s - loss: 1 - ETA: 6s - loss:  - ETA: 5s - l - ETA: 3s - loss: 1.3 - ETA: 2s - loss: - ETA: 1s - loss: \n",
      "Epoch 8/10\n",
      "31370/31370 [==============================] - 6044s 193ms/step - loss: 1.313410:55 - loss - ETA - E - ETA: 10:47 -  - ETA: 10:46 - loss: 1.31 - ETA: 10:02 -  - ETA: 9:57 - loss:  - ETA: 9:56 - loss: 1.3 - ETA: 9: - ETA: 9:52 - loss - ETA: 9:51 - loss: 1.31 - ETA: 9:51 - los - ETA: 9:49 - loss: 1 - ETA: 9:48 - los - ETA: 9:46 - loss: 1 - ETA: 9:40 - loss: 1. - ETA: 9:39 - loss: 1 - ETA: 9:38 -  - ETA: 9:36 - loss - ET - ETA: 8:43  - ETA: 8:41 - ETA: 8:38 - loss - ETA: 8:36 - loss: 1. - ETA: 8:35 - loss: 1.3 - ETA: 8:04  - ETA: 8:01 -  - ETA: 7: - E - ETA: 7:25 -  - - ETA: 6:47 - loss: 1. - ETA: 6:4 - ETA: 6 - - ETA: 6:29 - - ETA: 6:16 - loss: 1.3 - ETA: 6: - ETA: 6:12 - l - ETA: 6:10 - loss: 1.313 - ET - ETA: 6:06 - loss:  - ETA: 6:05 - ETA: 6:02 - loss: 1.312 - ETA: 6:02 - loss: 1.3 - ETA:  - ETA - ETA: 5:48 - loss: 1.312 - ETA: 5 - ETA: 5:45 - loss: 1.3 - ETA: 5:44 -  - ETA: 5:42 - ETA: 5:39 - los - ETA: 5:2 - ETA: 5:24 - lo - ETA: 5:22 - l - ETA: 5:19 - loss: 1.3 - ETA: 5:19 - loss: 1.312 - ETA: - ETA: - ETA: 5:11 - loss: - ETA: 5:10 - loss - ETA: 5:08 - loss: 1.312 - ETA: 5:0 - ETA: 5:05 - loss: 1. - ETA:  - ETA: 4:55 - loss: 1. - ETA: 4:54 - loss: - ETA: 4:53 - los - ETA: 4: - ETA: 4:4 - ETA: 4:45 - loss: 1.311 - ETA: 4:4 -  - ETA: 4: - ETA: 4:23 - loss: 1 - ETA: 4:22 - los - ETA: - ETA: 4:17 - loss: 1. - ETA: 4:1 - ETA: 4:13 - loss: 1.31 - E - ETA: 4:08 - loss: 1 - ETA: 4:07 - loss -  - ETA: 4:01 - - ETA: 3:59  - ETA: - ETA: - ETA: 3:38 - loss: - ETA: 3:36  - ETA: 3:34 - loss: 1.3 - ETA: 3 - ETA: 3:30 - l - ETA: 3:28  - ETA: 3:25 - loss - ETA: 3:23  - ETA: 3:21 - ETA: 3:18 - los - ETA:  - - ETA: 3:08 - loss: 1. - ETA: 3:07 - loss: 1 - ETA: 3:06 - loss: 1.3 - ETA: 3:05 - loss: 1.31  - ETA: 3:00 - loss:  - ETA: 2:59 - los - E - ETA: 2: - ETA: - ET - ETA - ETA: 2:37 -  - - ETA: 2:25 - loss: 1 - ETA: 2:24 - loss: 1.3 - ETA: 2:23 - lo - ETA: 2:21 - loss: 1.3 - ETA: 2:21 - lo - ETA: 2:19 - loss: 1.31 - ETA: 2:18  - ETA: 2:16 - loss - ETA: 2:1 - ETA: 2:11 - loss: 1.31 - ETA: 2: - ETA: 2:07 - loss - ETA: 2:06  - ETA: 2:03 - loss: 1.31 - ETA: 2:03 - lo - ETA: 2:01 - loss:  - ETA: 2:00 - loss - ETA: 1:58 - loss: 1. - ETA: 1:57 - loss: 1.31 - ETA: 1:57 - loss: 1.31 - ETA: 1:57 - ETA: 1:54 - lo - ETA: 1:52 - loss: 1.3 - ETA: 1:51 - loss: 1.3 - ETA: 1:51 - - ETA: 1:48 - loss: 1.31 - ETA: 1:48 - loss:  - ETA: 1:47 - loss: - ETA: 1:45  - ETA: 1:43 - loss: - ETA: 1:41 - ETA: 1:33 - loss - ETA: 1:32 - loss: 1.313 - ETA: 1:31 - loss: 1.313 - ETA: 1:31 - loss:  - ETA: 1:30 - loss: - ETA: 1:29 -  - ETA: 1:26 - loss: 1. - ETA: 1:0 - - ETA: - ETA: 3s - loss: 1. - ETA: 2s - loss: 1.31 - ETA: 2s - loss - ETA: 0s - loss: 1.3\n",
      "Epoch 9/10\n",
      "31370/31370 [==============================] - 6042s 193ms/step - loss: 1.311610:2 - ETA: 9:41 - lo - ETA: 9:39 - loss: 1. - ETA: 9:38 - loss: 1.3 - ETA: 9:38 - loss: 1.3 - ETA: 9:37 - lo - ETA: - ETA: 9:26 - loss:  - ETA: 9:25 - ETA: 9:22 - loss: 1.312 - ETA: 9:2 - ETA: 9:19 - loss: 1 - ETA:  - ETA: 8:43 - loss:  - ETA: 8:05 - loss: 1.3 - ETA: 8:05 - loss: - E - - ETA: 7:49 - loss: 1.3 -  - ETA: 6:52 - loss: 1.3 - ETA: 6 - ETA: 6: - ETA: 6:45 - loss: 1 - ET - ETA: - ETA: 6:36 - loss:  - ETA: 6:34 - l - ETA: 6:06 - loss: 1 - ETA: 6:05 - lo - ETA: 6:03 -  - ETA: 6:01 - loss: 1. - ETA: 6:00 - loss: 1.311 - ETA: 6:00 - loss:  - ETA: 5:59 - loss: 1.3 - ETA: 5:5 - ETA: - ETA: 5:46 - loss: 1.31 - ETA: 5:4 - ETA: 5:43 - loss: 1 - ETA: 5:42 - loss: 1.31 - E - ETA: 5:37  - ETA: 5:34 - loss: 1.3 - ETA: 5:34 - loss: 1.310 - ETA: 5:33 - loss: 1 - ETA: 5: - ETA: 5:2 - ETA: 5:26 - ETA: 5:23 - loss:  - ETA: 5:17 - loss:  - ETA:  - ETA: 4 - ETA: 4:43 - loss: - ETA: 4:41 - loss: 1 - ETA: 4 - ETA: 4:32 - loss: 1.3 - ETA:  - ETA: 4:27 - loss: 1.31 - ETA: 4:27 - ETA: 4:24 - lo - ETA: 4:22 - lo - ETA: 4:20 - los - ETA: 4:18 - loss: 1.310 - ETA: 4:18  -  - ETA: 3:55 - loss - ETA: 3:54 - ETA: 3:51 - loss: 1. - ETA: 3:50 - los - ETA: 3:48 - loss: 1.3 - ETA: 3:3 - ETA: 3:29 - loss: 1 - ETA: 3:28 - ETA: - ETA: 9s - loss: 1 - ETA: 8s - loss: 1.31 - ETA: 7s - ETA: 4s - loss: 1.3 - ETA: 4s  - ETA: 1s - loss\n",
      "Epoch 10/10\n",
      "31370/31370 [==============================] - 6043s 193ms/step - loss: 1.31019:52 - ETA: 9 - ETA: 9:46 - l - ET - ETA: 9:39  - ETA: 9:05 - los - ETA: 9:04 -  - ETA: 9:01 - loss: 1.3 - ETA: 9:01 - loss: 1 - ETA:  - ETA: 8:35 - ETA: 8:32 - lo - ETA: 8:25 - loss: 1.31 - ETA: 8:25 - loss: 1 - ETA: 8:19 - loss: 1.310 - ETA: 8:18 - loss: 1. - ETA: 8:18 - loss: 1.31 - ETA: 8:17 - l - ETA: 7:49 - loss: 1. -  - ETA: 7:44 - loss: - ETA: - ETA: 7:39  - ETA: 7:36 - - ETA: 7:33 - - ETA: 7:31 - loss: 1. - ETA - ETA: 7:26 - los - ETA: 7:24 -  - ETA: 7:22 - loss: - - ETA: 7:11 - loss: - ETA: 7:09 - l -  - ETA: - ETA: 6:01 - ETA: 5:38 - ETA: 5:24 - loss: 1.309 - ETA: - ETA: 5:15 - ETA: 5:12 - loss: 1.30 - ETA: 5:12 - loss: 1.3 - ETA: 5:1 - ETA: 5:08 - los - ETA: 5:0 - ETA - ETA: - ETA: 4:50 - loss - ETA: 4:49 - loss: 1 - ETA: 4:48 - ETA: 4:03 - loss: 1  - ETA: 3:57 - loss: 1.30 - ETA\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
    "                    epochs=10)\n",
    "np.save('history/shakespeare.npy', history.history)\n",
    "model.save(\"models/shakespeare.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('models/shakespeare.h5')\n",
    "history = np.load('history/shakespeare.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "next_char(\"How are yo\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 8 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 9 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 10 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 12 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E506DFA4C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "the state, the state and they are the belly\n",
      "and the\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to companio;\n",
      "and they are spoke at jot, trivants al\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpeniom,\n",
      "that?' tigl'd him frevils dear. ruli-hape-\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.repeat().batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "steps_per_epoch = train_size // batch_size // n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 313 steps\n",
      "Epoch 1/50\n",
      "313/313 [==============================] - 58s 186ms/step - loss: 2.61752s -\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 2.19443s -  - ETA: 1s - los\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 57s 182ms/step - loss: 2.30811s - loss: \n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 56s 179ms/step - loss: 2.2082\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 56s 178ms/step - loss: 2.1247 9s - loss:   \n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 2.0825\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 58s 186ms/step - loss: 2.0365- ETA: 1s - los\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 60s 191ms/step - loss: 1.95382s -\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 57s 181ms/step - loss: 1.9183\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.9242\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.88131s - loss\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.85361s - loss\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.8313\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.8118\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.7963\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.78036s - \n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.7691TA: 0s - loss: 1\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.7571\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.7460\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.7375\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.72996s \n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.7210\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.71584s - ETA: 1s - l\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.7075\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.70414s - loss: 1.7 - ETA: 4s - loss: 1.7 - ET\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6973\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.69170s - loss: 1\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6851\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.6824\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6788\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.6713\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.6675\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6629\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.65791s - l\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6547\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6497\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.6446\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6422\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.6374\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6340\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6313\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6290\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6241\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.62091s - loss\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.6172\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.61652s - loss: 1.616 - ETA: 2s -\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.6116\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6088\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.6057\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.60446s - loss: 1.6 - ET - ETA: 2s -\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50,\n",
    "                    callbacks=[ResetStatesCallback()])\n",
    "np.save('history/stateful_rnn.npy', history.history)\n",
    "model.save(\"models/stateful_rnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('models/stateful_rnn.h5')\n",
    "history = np.load('history/stateful_rnn.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model with different batch sizes, we need to create a stateless copy. We can get rid of dropout since it is only used during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the weights, we first need to build the model (so the weights get created):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None, max_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function _make_execution_function.<locals>.distributed_function at 0x000001E512CB8EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "think it,\n",
      "did thou art to joy all. believe i not\n",
      "th\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the IMDB dataset easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_test), (X_valid, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to C:\\Users\\com\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55568167d7504b4384b11eb6151a2efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Completed...', max=1, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5858f477d8be482c97eb82b077553181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Size...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\Users\\com\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteX9NZL8\\imdb_reviews-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42946068e5a48dca06c117d0c439306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\Users\\com\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteX9NZL8\\imdb_reviews-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7173832dfdc34c1c8bf277a6d989cd12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\Users\\com\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteX9NZL8\\imdb_reviews-unsupervised.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dce783909a4a82b27adc16780a66dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb_reviews downloaded and prepared to C:\\Users\\com\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\n",
      "\r"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'unsupervised'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = info.splits[\"train\"].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This is a big step down after the surprisingly enjoyable original. This sequel isn't nearly as fun as part one, and it instead spends too much time on plot development. Tim Thomerson is still the best ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: Perhaps because I was so young, innocent and BRAINWASHED when I saw it, this movie was the cause of many sleepless nights for me. I haven't seen it since I was in seventh grade at a Presbyterian schoo ...\n",
      "Label: 0 = Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 60), dtype=string, numpy=\n",
       " array([[b'This', b'is', b'a', b'big', b'step', b'down', b'after', b'the',\n",
       "         b'surprisingly', b'enjoyable', b'original', b'This', b'sequel',\n",
       "         b\"isn't\", b'nearly', b'as', b'fun', b'as', b'part', b'one',\n",
       "         b'and', b'it', b'instead', b'spends', b'too', b'much', b'time',\n",
       "         b'on', b'plot', b'development', b'Tim', b'Thomerson', b'is',\n",
       "         b'still', b'the', b'best', b'thing', b'about', b'this',\n",
       "         b'series', b'but', b'his', b'wisecracking', b'is', b'toned',\n",
       "         b'down', b'in', b'this', b'entry', b'The', b'performances',\n",
       "         b'are', b'all', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "         b'<pad>', b'<pad>'],\n",
       "        [b'Perhaps', b'because', b'I', b'was', b'so', b'young',\n",
       "         b'innocent', b'and', b'BRAINWASHED', b'when', b'I', b'saw',\n",
       "         b'it', b'this', b'movie', b'was', b'the', b'cause', b'of',\n",
       "         b'many', b'sleepless', b'nights', b'for', b'me', b'I',\n",
       "         b\"haven't\", b'seen', b'it', b'since', b'I', b'was', b'in',\n",
       "         b'seventh', b'grade', b'at', b'a', b'Presbyterian', b'school',\n",
       "         b'so', b'I', b'am', b'not', b'sure', b'what', b'effect', b'it',\n",
       "         b'would', b'have', b'on', b'me', b'now', b'However', b'I',\n",
       "         b'will', b'say', b'that', b'it', b'left', b'an', b'impress']],\n",
       "       dtype=object)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0], dtype=int64)>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214741), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53893"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "12\n",
      "11\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
    "for word in b\"This movie was faaaaaantastic\".split():\n",
    "    print(word_to_id.get(word) or vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]], dtype=int64)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].repeat().batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   22     7     2 ...     0     0     0]\n",
      " [ 1239    82     6 ...   418    28  4245]\n",
      " [ 4246     3     1 ...     0     0     0]\n",
      " ...\n",
      " [   22     7    23 ...     0     0     0]\n",
      " [ 1297  3744     7 ...     0     0     0]\n",
      " [10928 10687  4537 ...     0     0     0]], shape=(32, 60), dtype=int64)\n",
      "tf.Tensor([0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 781 steps\n",
      "Epoch 1/5\n",
      "541/781 [===================>..........] - ETA: 1:19:45 - loss: 0.6923 - accuracy: 0.656 - ETA: 40:10 - loss: 0.6972 - accuracy: 0.4531  - ETA: 26:59 - loss: 0.6952 - accuracy: 0.489 - ETA: 13:35 - loss: 0.6935 - accuracy: 0.531 - ETA: 9:07 - loss: 0.6938 - accuracy: 0.520 - ETA: 6:53 - loss: 0.6936 - accuracy: 0.52 - ETA: 5:32 - loss: 0.6934 - accuracy: 0.52 - ETA: 4:39 - loss: 0.6923 - accuracy: 0.54 - ETA: 4:01 - loss: 0.6942 - accuracy: 0.52 - ETA: 3:32 - loss: 0.6945 - accuracy: 0.51 - ETA: 3:09 - loss: 0.6944 - accuracy: 0.51 - ETA: 2:51 - loss: 0.6945 - accuracy: 0.50 - ETA: 2:37 - loss: 0.6943 - accuracy: 0.50 - ETA: 2:24 - loss: 0.6941 - accuracy: 0.50 - ETA: 2:14 - loss: 0.6944 - accuracy: 0.50 - ETA: 2:05 - loss: 0.6939 - accuracy: 0.50 - ETA: 1:58 - loss: 0.6938 - accuracy: 0.50 - ETA: 1:51 - loss: 0.6932 - accuracy: 0.51 - ETA: 1:45 - loss: 0.6929 - accuracy: 0.51 - ETA: 1:39 - loss: 0.6929 - accuracy: 0.51 - ETA: 1:35 - loss: 0.6932 - accuracy: 0.51 - ETA: 1:30 - loss: 0.6928 - accuracy: 0.51 - ETA: 1:26 - loss: 0.6920 - accuracy: 0.51 - ETA: 1:23 - loss: 0.6914 - accuracy: 0.51 - ETA: 1:20 - loss: 0.6922 - accuracy: 0.51 - ETA: 1:17 - loss: 0.6919 - accuracy: 0.52 - ETA: 1:14 - loss: 0.6904 - accuracy: 0.52 - ETA: 1:11 - loss: 0.6909 - accuracy: 0.52 - ETA: 1:09 - loss: 0.6906 - accuracy: 0.52 - ETA: 1:07 - loss: 0.6904 - accuracy: 0.53 - ETA: 1:05 - loss: 0.6903 - accuracy: 0.52 - ETA: 1:03 - loss: 0.6894 - accuracy: 0.53 - ETA: 1:01 - loss: 0.6885 - accuracy: 0.53 - ETA: 59s - loss: 0.6872 - accuracy: 0.5374 - ETA: 58s - loss: 0.6860 - accuracy: 0.540 - ETA: 56s - loss: 0.6799 - accuracy: 0.548 - ETA: 55s - loss: 0.6900 - accuracy: 0.550 - ETA: 54s - loss: 0.6888 - accuracy: 0.553 - ETA: 52s - loss: 0.6882 - accuracy: 0.554 - ETA: 51s - loss: 0.6881 - accuracy: 0.554 - ETA: 50s - loss: 0.6879 - accuracy: 0.555 - ETA: 49s - loss: 0.6875 - accuracy: 0.557 - ETA: 48s - loss: 0.6864 - accuracy: 0.560 - ETA: 47s - loss: 0.6854 - accuracy: 0.562 - ETA: 46s - loss: 0.6852 - accuracy: 0.562 - ETA: 45s - loss: 0.6838 - accuracy: 0.565 - ETA: 44s - loss: 0.6841 - accuracy: 0.566 - ETA: 43s - loss: 0.6833 - accuracy: 0.567 - ETA: 43s - loss: 0.6817 - accuracy: 0.570 - ETA: 42s - loss: 0.6809 - accuracy: 0.571 - ETA: 41s - loss: 0.6803 - accuracy: 0.572 - ETA: 40s - loss: 0.6795 - accuracy: 0.574 - ETA: 40s - loss: 0.6773 - accuracy: 0.576 - ETA: 39s - loss: 0.6773 - accuracy: 0.577 - ETA: 38s - loss: 0.6764 - accuracy: 0.578 - ETA: 38s - loss: 0.6736 - accuracy: 0.581 - ETA: 37s - loss: 0.6722 - accuracy: 0.583 - ETA: 36s - loss: 0.6701 - accuracy: 0.585 - ETA: 36s - loss: 0.6690 - accuracy: 0.587 - ETA: 35s - loss: 0.6676 - accuracy: 0.589 - ETA: 35s - loss: 0.6653 - accuracy: 0.591 - ETA: 34s - loss: 0.6639 - accuracy: 0.592 - ETA: 34s - loss: 0.6616 - accuracy: 0.595 - ETA: 33s - loss: 0.6590 - accuracy: 0.598 - ETA: 33s - loss: 0.6566 - accuracy: 0.601 - ETA: 33s - loss: 0.6561 - accuracy: 0.601 - ETA: 32s - loss: 0.6555 - accuracy: 0.602 - ETA: 32s - loss: 0.6535 - accuracy: 0.604 - ETA: 31s - loss: 0.6507 - accuracy: 0.607 - ETA: 31s - loss: 0.6506 - accuracy: 0.609 - ETA: 30s - loss: 0.6496 - accuracy: 0.609 - ETA: 30s - loss: 0.6480 - accuracy: 0.612 - ETA: 30s - loss: 0.6472 - accuracy: 0.613 - ETA: 29s - loss: 0.6462 - accuracy: 0.614 - ETA: 29s - loss: 0.6446 - accuracy: 0.615 - ETA: 28s - loss: 0.6435 - accuracy: 0.617 - ETA: 28s - loss: 0.6413 - accuracy: 0.619 - ETA: 28s - loss: 0.6397 - accuracy: 0.621 - ETA: 27s - loss: 0.6385 - accuracy: 0.622 - ETA: 27s - loss: 0.6369 - accuracy: 0.623 - ETA: 27s - loss: 0.6351 - accuracy: 0.625 - ETA: 26s - loss: 0.6333 - accuracy: 0.627 - ETA: 26s - loss: 0.6321 - accuracy: 0.628 - ETA: 26s - loss: 0.6305 - accuracy: 0.630 - ETA: 25s - loss: 0.6293 - accuracy: 0.631 - ETA: 25s - loss: 0.6280 - accuracy: 0.634 - ETA: 25s - loss: 0.6262 - accuracy: 0.636 - ETA: 25s - loss: 0.6246 - accuracy: 0.637 - ETA: 24s - loss: 0.6228 - accuracy: 0.639 - ETA: 24s - loss: 0.6215 - accuracy: 0.640 - ETA: 24s - loss: 0.6201 - accuracy: 0.641 - ETA: 23s - loss: 0.6181 - accuracy: 0.643 - ETA: 23s - loss: 0.6171 - accuracy: 0.644 - ETA: 23s - loss: 0.6159 - accuracy: 0.645 - ETA: 23s - loss: 0.6155 - accuracy: 0.646 - ETA: 22s - loss: 0.6146 - accuracy: 0.646 - ETA: 22s - loss: 0.6138 - accuracy: 0.648 - ETA: 22s - loss: 0.6119 - accuracy: 0.649 - ETA: 22s - loss: 0.6116 - accuracy: 0.650 - ETA: 21s - loss: 0.6113 - accuracy: 0.650 - ETA: 21s - loss: 0.6106 - accuracy: 0.651 - ETA: 21s - loss: 0.6094 - accuracy: 0.652 - ETA: 21s - loss: 0.6078 - accuracy: 0.654 - ETA: 21s - loss: 0.6085 - accuracy: 0.654 - ETA: 20s - loss: 0.6075 - accuracy: 0.655 - ETA: 20s - loss: 0.6059 - accuracy: 0.657 - ETA: 20s - loss: 0.6044 - accuracy: 0.658 - ETA: 20s - loss: 0.6045 - accuracy: 0.658 - ETA: 19s - loss: 0.6042 - accuracy: 0.658 - ETA: 19s - loss: 0.6038 - accuracy: 0.659 - ETA: 19s - loss: 0.6039 - accuracy: 0.659 - ETA: 19s - loss: 0.6031 - accuracy: 0.660 - ETA: 19s - loss: 0.6019 - accuracy: 0.661 - ETA: 18s - loss: 0.6009 - accuracy: 0.662 - ETA: 18s - loss: 0.6001 - accuracy: 0.662 - ETA: 18s - loss: 0.5990 - accuracy: 0.664 - ETA: 18s - loss: 0.5979 - accuracy: 0.665 - ETA: 18s - loss: 0.5978 - accuracy: 0.665 - ETA: 17s - loss: 0.5972 - accuracy: 0.666 - ETA: 17s - loss: 0.5964 - accuracy: 0.667 - ETA: 17s - loss: 0.5959 - accuracy: 0.667 - ETA: 17s - loss: 0.5955 - accuracy: 0.668 - ETA: 17s - loss: 0.5954 - accuracy: 0.668 - ETA: 16s - loss: 0.5938 - accuracy: 0.670 - ETA: 16s - loss: 0.5928 - accuracy: 0.671 - ETA: 16s - loss: 0.5919 - accuracy: 0.672 - ETA: 16s - loss: 0.5900 - accuracy: 0.673 - ETA: 16s - loss: 0.5894 - accuracy: 0.673 - ETA: 16s - loss: 0.5897 - accuracy: 0.673 - ETA: 16s - loss: 0.5892 - accuracy: 0.674 - ETA: 15s - loss: 0.5887 - accuracy: 0.674 - ETA: 15s - loss: 0.5883 - accuracy: 0.675 - ETA: 15s - loss: 0.5870 - accuracy: 0.676 - ETA: 15s - loss: 0.5864 - accuracy: 0.676 - ETA: 15s - loss: 0.5866 - accuracy: 0.677 - ETA: 14s - loss: 0.5862 - accuracy: 0.677 - ETA: 14s - loss: 0.5856 - accuracy: 0.678 - ETA: 14s - loss: 0.5852 - accuracy: 0.678 - ETA: 14s - loss: 0.5845 - accuracy: 0.679 - ETA: 14s - loss: 0.5842 - accuracy: 0.679 - ETA: 14s - loss: 0.5836 - accuracy: 0.680 - ETA: 14s - loss: 0.5833 - accuracy: 0.680 - ETA: 13s - loss: 0.5823 - accuracy: 0.681 - ETA: 13s - loss: 0.5815 - accuracy: 0.682 - ETA: 13s - loss: 0.5810 - accuracy: 0.682 - ETA: 13s - loss: 0.5806 - accuracy: 0.683 - ETA: 13s - loss: 0.5799 - accuracy: 0.683 - ETA: 13s - loss: 0.5792 - accuracy: 0.684 - ETA: 12s - loss: 0.5784 - accuracy: 0.685 - ETA: 12s - loss: 0.5775 - accuracy: 0.686 - ETA: 12s - loss: 0.5760 - accuracy: 0.687 - ETA: 12s - loss: 0.5761 - accuracy: 0.687 - ETA: 12s - loss: 0.5755 - accuracy: 0.688 - ETA: 12s - loss: 0.5749 - accuracy: 0.688 - ETA: 12s - loss: 0.5741 - accuracy: 0.689 - ETA: 12s - loss: 0.5731 - accuracy: 0.690 - ETA: 11s - loss: 0.5728 - accuracy: 0.690 - ETA: 11s - loss: 0.5720 - accuracy: 0.690 - ETA: 11s - loss: 0.5722 - accuracy: 0.691 - ETA: 11s - loss: 0.5719 - accuracy: 0.691 - ETA: 11s - loss: 0.5710 - accuracy: 0.692 - ETA: 11s - loss: 0.5708 - accuracy: 0.693 - ETA: 11s - loss: 0.5704 - accuracy: 0.693 - ETA: 10s - loss: 0.5697 - accuracy: 0.694 - ETA: 10s - loss: 0.5700 - accuracy: 0.693 - ETA: 10s - loss: 0.5689 - accuracy: 0.694 - ETA: 10s - loss: 0.5682 - accuracy: 0.695 - ETA: 10s - loss: 0.5672 - accuracy: 0.696 - ETA: 10s - loss: 0.5669 - accuracy: 0.696 - ETA: 10s - loss: 0.5657 - accuracy: 0.697 - ETA: 10s - loss: 0.5654 - accuracy: 0.697 - ETA: 9s - loss: 0.5655 - accuracy: 0.698 - ETA: 9s - loss: 0.5651 - accuracy: 0.69 - ETA: 9s - loss: 0.5648 - accuracy: 0.69 - ETA: 9s - loss: 0.5641 - accuracy: 0.69 - ETA: 9s - loss: 0.5637 - accuracy: 0.69 - ETA: 9s - loss: 0.5631 - accuracy: 0.70 - ETA: 9s - loss: 0.5625 - accuracy: 0.70 - ETA: 9s - loss: 0.5624 - accuracy: 0.70 - ETA: 8s - loss: 0.5621 - accuracy: 0.70 - ETA: 8s - loss: 0.5616 - accuracy: 0.70 - ETA: 8s - loss: 0.5615 - accuracy: 0.70 - ETA: 8s - loss: 0.5610 - accuracy: 0.70 - ETA: 8s - loss: 0.5610 - accuracy: 0.70 - ETA: 8s - loss: 0.5608 - accuracy: 0.70781/781 [==============================] - ETA: 8s - loss: 0.5602 - accuracy: 0.70 - ETA: 8s - loss: 0.5598 - accuracy: 0.70 - ETA: 7s - loss: 0.5594 - accuracy: 0.70 - ETA: 7s - loss: 0.5588 - accuracy: 0.70 - ETA: 7s - loss: 0.5584 - accuracy: 0.70 - ETA: 7s - loss: 0.5580 - accuracy: 0.70 - ETA: 7s - loss: 0.5577 - accuracy: 0.70 - ETA: 7s - loss: 0.5576 - accuracy: 0.70 - ETA: 7s - loss: 0.5569 - accuracy: 0.70 - ETA: 7s - loss: 0.5565 - accuracy: 0.70 - ETA: 7s - loss: 0.5558 - accuracy: 0.70 - ETA: 6s - loss: 0.5552 - accuracy: 0.70 - ETA: 6s - loss: 0.5552 - accuracy: 0.70 - ETA: 6s - loss: 0.5549 - accuracy: 0.70 - ETA: 6s - loss: 0.5546 - accuracy: 0.70 - ETA: 6s - loss: 0.5545 - accuracy: 0.70 - ETA: 6s - loss: 0.5539 - accuracy: 0.70 - ETA: 6s - loss: 0.5533 - accuracy: 0.70 - ETA: 6s - loss: 0.5527 - accuracy: 0.70 - ETA: 6s - loss: 0.5519 - accuracy: 0.70 - ETA: 5s - loss: 0.5522 - accuracy: 0.70 - ETA: 5s - loss: 0.5521 - accuracy: 0.70 - ETA: 5s - loss: 0.5521 - accuracy: 0.70 - ETA: 5s - loss: 0.5521 - accuracy: 0.70 - ETA: 5s - loss: 0.5523 - accuracy: 0.70 - ETA: 5s - loss: 0.5519 - accuracy: 0.71 - ETA: 5s - loss: 0.5516 - accuracy: 0.71 - ETA: 5s - loss: 0.5516 - accuracy: 0.71 - ETA: 5s - loss: 0.5509 - accuracy: 0.71 - ETA: 4s - loss: 0.5506 - accuracy: 0.71 - ETA: 4s - loss: 0.5501 - accuracy: 0.71 - ETA: 4s - loss: 0.5496 - accuracy: 0.71 - ETA: 4s - loss: 0.5494 - accuracy: 0.71 - ETA: 4s - loss: 0.5489 - accuracy: 0.71 - ETA: 4s - loss: 0.5483 - accuracy: 0.71 - ETA: 4s - loss: 0.5480 - accuracy: 0.71 - ETA: 4s - loss: 0.5478 - accuracy: 0.71 - ETA: 4s - loss: 0.5472 - accuracy: 0.71 - ETA: 3s - loss: 0.5472 - accuracy: 0.71 - ETA: 3s - loss: 0.5465 - accuracy: 0.71 - ETA: 3s - loss: 0.5462 - accuracy: 0.71 - ETA: 3s - loss: 0.5459 - accuracy: 0.71 - ETA: 3s - loss: 0.5454 - accuracy: 0.71 - ETA: 3s - loss: 0.5452 - accuracy: 0.71 - ETA: 3s - loss: 0.5450 - accuracy: 0.71 - ETA: 3s - loss: 0.5447 - accuracy: 0.71 - ETA: 3s - loss: 0.5447 - accuracy: 0.71 - ETA: 3s - loss: 0.5444 - accuracy: 0.71 - ETA: 2s - loss: 0.5443 - accuracy: 0.71 - ETA: 2s - loss: 0.5441 - accuracy: 0.71 - ETA: 2s - loss: 0.5440 - accuracy: 0.71 - ETA: 2s - loss: 0.5440 - accuracy: 0.71 - ETA: 2s - loss: 0.5437 - accuracy: 0.71 - ETA: 2s - loss: 0.5434 - accuracy: 0.71 - ETA: 2s - loss: 0.5435 - accuracy: 0.71 - ETA: 2s - loss: 0.5434 - accuracy: 0.71 - ETA: 2s - loss: 0.5428 - accuracy: 0.71 - ETA: 2s - loss: 0.5421 - accuracy: 0.71 - ETA: 1s - loss: 0.5420 - accuracy: 0.71 - ETA: 1s - loss: 0.5421 - accuracy: 0.71 - ETA: 1s - loss: 0.5415 - accuracy: 0.71 - ETA: 1s - loss: 0.5414 - accuracy: 0.71 - ETA: 1s - loss: 0.5410 - accuracy: 0.71 - ETA: 1s - loss: 0.5411 - accuracy: 0.71 - ETA: 1s - loss: 0.5407 - accuracy: 0.71 - ETA: 1s - loss: 0.5407 - accuracy: 0.71 - ETA: 1s - loss: 0.5401 - accuracy: 0.71 - ETA: 1s - loss: 0.5398 - accuracy: 0.72 - ETA: 1s - loss: 0.5391 - accuracy: 0.72 - ETA: 0s - loss: 0.5385 - accuracy: 0.72 - ETA: 0s - loss: 0.5386 - accuracy: 0.72 - ETA: 0s - loss: 0.5388 - accuracy: 0.72 - ETA: 0s - loss: 0.5389 - accuracy: 0.72 - ETA: 0s - loss: 0.5385 - accuracy: 0.72 - ETA: 0s - loss: 0.5383 - accuracy: 0.72 - ETA: 0s - loss: 0.5381 - accuracy: 0.72 - ETA: 0s - loss: 0.5379 - accuracy: 0.72 - ETA: 0s - loss: 0.5381 - accuracy: 0.72 - ETA: 0s - loss: 0.5383 - accuracy: 0.72 - 24s 31ms/step - loss: 0.5381 - accuracy: 0.7222\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562/781 [====================>.........] - ETA: 18s - loss: 0.6274 - accuracy: 0.656 - ETA: 17s - loss: 0.4561 - accuracy: 0.804 - ETA: 17s - loss: 0.4399 - accuracy: 0.799 - ETA: 17s - loss: 0.4549 - accuracy: 0.787 - ETA: 17s - loss: 0.4830 - accuracy: 0.774 - ETA: 17s - loss: 0.4949 - accuracy: 0.765 - ETA: 17s - loss: 0.4857 - accuracy: 0.773 - ETA: 17s - loss: 0.4890 - accuracy: 0.768 - ETA: 17s - loss: 0.4849 - accuracy: 0.771 - ETA: 17s - loss: 0.4845 - accuracy: 0.773 - ETA: 17s - loss: 0.4832 - accuracy: 0.771 - ETA: 16s - loss: 0.4827 - accuracy: 0.773 - ETA: 16s - loss: 0.4756 - accuracy: 0.776 - ETA: 16s - loss: 0.4713 - accuracy: 0.778 - ETA: 16s - loss: 0.4746 - accuracy: 0.778 - ETA: 16s - loss: 0.4691 - accuracy: 0.782 - ETA: 16s - loss: 0.4666 - accuracy: 0.786 - ETA: 16s - loss: 0.4625 - accuracy: 0.787 - ETA: 16s - loss: 0.4650 - accuracy: 0.786 - ETA: 16s - loss: 0.4610 - accuracy: 0.788 - ETA: 16s - loss: 0.4557 - accuracy: 0.790 - ETA: 16s - loss: 0.4566 - accuracy: 0.789 - ETA: 16s - loss: 0.4550 - accuracy: 0.790 - ETA: 16s - loss: 0.4568 - accuracy: 0.788 - ETA: 16s - loss: 0.4534 - accuracy: 0.789 - ETA: 15s - loss: 0.4485 - accuracy: 0.793 - ETA: 15s - loss: 0.4444 - accuracy: 0.795 - ETA: 15s - loss: 0.4439 - accuracy: 0.797 - ETA: 15s - loss: 0.4412 - accuracy: 0.799 - ETA: 15s - loss: 0.4392 - accuracy: 0.800 - ETA: 15s - loss: 0.4372 - accuracy: 0.802 - ETA: 15s - loss: 0.4320 - accuracy: 0.805 - ETA: 15s - loss: 0.4265 - accuracy: 0.808 - ETA: 15s - loss: 0.4202 - accuracy: 0.811 - ETA: 15s - loss: 0.4127 - accuracy: 0.814 - ETA: 15s - loss: 0.4085 - accuracy: 0.817 - ETA: 15s - loss: 0.4087 - accuracy: 0.817 - ETA: 15s - loss: 0.4083 - accuracy: 0.817 - ETA: 15s - loss: 0.4127 - accuracy: 0.813 - ETA: 15s - loss: 0.4155 - accuracy: 0.812 - ETA: 14s - loss: 0.4156 - accuracy: 0.813 - ETA: 14s - loss: 0.4157 - accuracy: 0.812 - ETA: 14s - loss: 0.4164 - accuracy: 0.812 - ETA: 14s - loss: 0.4151 - accuracy: 0.813 - ETA: 14s - loss: 0.4142 - accuracy: 0.813 - ETA: 14s - loss: 0.4135 - accuracy: 0.814 - ETA: 14s - loss: 0.4121 - accuracy: 0.815 - ETA: 14s - loss: 0.4124 - accuracy: 0.814 - ETA: 14s - loss: 0.4120 - accuracy: 0.814 - ETA: 14s - loss: 0.4106 - accuracy: 0.814 - ETA: 14s - loss: 0.4081 - accuracy: 0.815 - ETA: 14s - loss: 0.4065 - accuracy: 0.816 - ETA: 14s - loss: 0.4061 - accuracy: 0.817 - ETA: 14s - loss: 0.4064 - accuracy: 0.817 - ETA: 14s - loss: 0.4029 - accuracy: 0.819 - ETA: 14s - loss: 0.4025 - accuracy: 0.819 - ETA: 13s - loss: 0.4041 - accuracy: 0.819 - ETA: 13s - loss: 0.4028 - accuracy: 0.820 - ETA: 13s - loss: 0.4018 - accuracy: 0.821 - ETA: 13s - loss: 0.3991 - accuracy: 0.823 - ETA: 13s - loss: 0.3978 - accuracy: 0.824 - ETA: 13s - loss: 0.3947 - accuracy: 0.825 - ETA: 13s - loss: 0.3926 - accuracy: 0.826 - ETA: 13s - loss: 0.3927 - accuracy: 0.825 - ETA: 13s - loss: 0.3924 - accuracy: 0.825 - ETA: 13s - loss: 0.3903 - accuracy: 0.826 - ETA: 13s - loss: 0.3885 - accuracy: 0.828 - ETA: 13s - loss: 0.3889 - accuracy: 0.827 - ETA: 13s - loss: 0.3884 - accuracy: 0.828 - ETA: 13s - loss: 0.3875 - accuracy: 0.829 - ETA: 12s - loss: 0.3874 - accuracy: 0.829 - ETA: 12s - loss: 0.3871 - accuracy: 0.829 - ETA: 12s - loss: 0.3869 - accuracy: 0.830 - ETA: 12s - loss: 0.3871 - accuracy: 0.830 - ETA: 12s - loss: 0.3859 - accuracy: 0.831 - ETA: 12s - loss: 0.3852 - accuracy: 0.831 - ETA: 12s - loss: 0.3844 - accuracy: 0.831 - ETA: 12s - loss: 0.3820 - accuracy: 0.833 - ETA: 12s - loss: 0.3819 - accuracy: 0.833 - ETA: 12s - loss: 0.3798 - accuracy: 0.834 - ETA: 12s - loss: 0.3792 - accuracy: 0.834 - ETA: 12s - loss: 0.3779 - accuracy: 0.835 - ETA: 12s - loss: 0.3768 - accuracy: 0.836 - ETA: 12s - loss: 0.3764 - accuracy: 0.836 - ETA: 12s - loss: 0.3749 - accuracy: 0.837 - ETA: 11s - loss: 0.3732 - accuracy: 0.839 - ETA: 11s - loss: 0.3723 - accuracy: 0.839 - ETA: 11s - loss: 0.3725 - accuracy: 0.839 - ETA: 11s - loss: 0.3716 - accuracy: 0.839 - ETA: 11s - loss: 0.3702 - accuracy: 0.840 - ETA: 11s - loss: 0.3693 - accuracy: 0.841 - ETA: 11s - loss: 0.3682 - accuracy: 0.842 - ETA: 11s - loss: 0.3678 - accuracy: 0.842 - ETA: 11s - loss: 0.3663 - accuracy: 0.843 - ETA: 11s - loss: 0.3661 - accuracy: 0.843 - ETA: 11s - loss: 0.3653 - accuracy: 0.843 - ETA: 11s - loss: 0.3657 - accuracy: 0.843 - ETA: 11s - loss: 0.3655 - accuracy: 0.844 - ETA: 11s - loss: 0.3650 - accuracy: 0.844 - ETA: 11s - loss: 0.3642 - accuracy: 0.844 - ETA: 10s - loss: 0.3630 - accuracy: 0.845 - ETA: 10s - loss: 0.3640 - accuracy: 0.845 - ETA: 10s - loss: 0.3631 - accuracy: 0.845 - ETA: 10s - loss: 0.3632 - accuracy: 0.846 - ETA: 10s - loss: 0.3641 - accuracy: 0.846 - ETA: 10s - loss: 0.3645 - accuracy: 0.845 - ETA: 10s - loss: 0.3649 - accuracy: 0.845 - ETA: 10s - loss: 0.3648 - accuracy: 0.845 - ETA: 10s - loss: 0.3645 - accuracy: 0.845 - ETA: 10s - loss: 0.3650 - accuracy: 0.845 - ETA: 10s - loss: 0.3638 - accuracy: 0.846 - ETA: 10s - loss: 0.3642 - accuracy: 0.845 - ETA: 10s - loss: 0.3637 - accuracy: 0.846 - ETA: 10s - loss: 0.3641 - accuracy: 0.845 - ETA: 10s - loss: 0.3637 - accuracy: 0.846 - ETA: 9s - loss: 0.3642 - accuracy: 0.845 - ETA: 9s - loss: 0.3644 - accuracy: 0.84 - ETA: 9s - loss: 0.3642 - accuracy: 0.84 - ETA: 9s - loss: 0.3639 - accuracy: 0.84 - ETA: 9s - loss: 0.3642 - accuracy: 0.84 - ETA: 9s - loss: 0.3639 - accuracy: 0.84 - ETA: 9s - loss: 0.3631 - accuracy: 0.84 - ETA: 9s - loss: 0.3620 - accuracy: 0.84 - ETA: 9s - loss: 0.3614 - accuracy: 0.84 - ETA: 9s - loss: 0.3599 - accuracy: 0.84 - ETA: 9s - loss: 0.3608 - accuracy: 0.84 - ETA: 9s - loss: 0.3609 - accuracy: 0.84 - ETA: 9s - loss: 0.3614 - accuracy: 0.84 - ETA: 9s - loss: 0.3617 - accuracy: 0.84 - ETA: 9s - loss: 0.3612 - accuracy: 0.84 - ETA: 8s - loss: 0.3605 - accuracy: 0.84 - ETA: 8s - loss: 0.3603 - accuracy: 0.84 - ETA: 8s - loss: 0.3610 - accuracy: 0.84 - ETA: 8s - loss: 0.3614 - accuracy: 0.84 - ETA: 8s - loss: 0.3615 - accuracy: 0.84 - ETA: 8s - loss: 0.3612 - accuracy: 0.84 - ETA: 8s - loss: 0.3613 - accuracy: 0.84 - ETA: 8s - loss: 0.3619 - accuracy: 0.84 - ETA: 8s - loss: 0.3618 - accuracy: 0.84 - ETA: 8s - loss: 0.3613 - accuracy: 0.84 - ETA: 8s - loss: 0.3611 - accuracy: 0.84 - ETA: 8s - loss: 0.3612 - accuracy: 0.84 - ETA: 8s - loss: 0.3607 - accuracy: 0.84 - ETA: 8s - loss: 0.3612 - accuracy: 0.84 - ETA: 7s - loss: 0.3614 - accuracy: 0.84 - ETA: 7s - loss: 0.3607 - accuracy: 0.84 - ETA: 7s - loss: 0.3601 - accuracy: 0.84 - ETA: 7s - loss: 0.3592 - accuracy: 0.84 - ETA: 7s - loss: 0.3586 - accuracy: 0.85 - ETA: 7s - loss: 0.3578 - accuracy: 0.85 - ETA: 7s - loss: 0.3578 - accuracy: 0.85 - ETA: 7s - loss: 0.3573 - accuracy: 0.85 - ETA: 7s - loss: 0.3567 - accuracy: 0.85 - ETA: 7s - loss: 0.3565 - accuracy: 0.85 - ETA: 7s - loss: 0.3555 - accuracy: 0.85 - ETA: 7s - loss: 0.3566 - accuracy: 0.85 - ETA: 7s - loss: 0.3563 - accuracy: 0.85 - ETA: 7s - loss: 0.3565 - accuracy: 0.85 - ETA: 7s - loss: 0.3566 - accuracy: 0.85 - ETA: 6s - loss: 0.3573 - accuracy: 0.85 - ETA: 6s - loss: 0.3571 - accuracy: 0.85 - ETA: 6s - loss: 0.3576 - accuracy: 0.85 - ETA: 6s - loss: 0.3574 - accuracy: 0.85 - ETA: 6s - loss: 0.3572 - accuracy: 0.85 - ETA: 6s - loss: 0.3566 - accuracy: 0.85 - ETA: 6s - loss: 0.3563 - accuracy: 0.85 - ETA: 6s - loss: 0.3556 - accuracy: 0.85 - ETA: 6s - loss: 0.3557 - accuracy: 0.85 - ETA: 6s - loss: 0.3562 - accuracy: 0.85 - ETA: 6s - loss: 0.3562 - accuracy: 0.85 - ETA: 6s - loss: 0.3559 - accuracy: 0.85 - ETA: 6s - loss: 0.3558 - accuracy: 0.85 - ETA: 6s - loss: 0.3559 - accuracy: 0.85 - ETA: 6s - loss: 0.3557 - accuracy: 0.85 - ETA: 5s - loss: 0.3555 - accuracy: 0.85 - ETA: 5s - loss: 0.3552 - accuracy: 0.85 - ETA: 5s - loss: 0.3550 - accuracy: 0.85 - ETA: 5s - loss: 0.3544 - accuracy: 0.85 - ETA: 5s - loss: 0.3548 - accuracy: 0.85 - ETA: 5s - loss: 0.3546 - accuracy: 0.85 - ETA: 5s - loss: 0.3553 - accuracy: 0.85 - ETA: 5s - loss: 0.3558 - accuracy: 0.85 - ETA: 5s - loss: 0.3556 - accuracy: 0.85 - ETA: 5s - loss: 0.3558 - accuracy: 0.85 - ETA: 5s - loss: 0.3560 - accuracy: 0.85 - ETA: 5s - loss: 0.3551 - accuracy: 0.85 - ETA: 5s - loss: 0.3554 - accuracy: 0.85 - ETA: 5s - loss: 0.3553 - accuracy: 0.85 - ETA: 4s - loss: 0.3552 - accuracy: 0.8525"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 4s - loss: 0.3551 - accuracy: 0.85 - ETA: 4s - loss: 0.3546 - accuracy: 0.85 - ETA: 4s - loss: 0.3541 - accuracy: 0.85 - ETA: 4s - loss: 0.3540 - accuracy: 0.85 - ETA: 4s - loss: 0.3532 - accuracy: 0.85 - ETA: 4s - loss: 0.3531 - accuracy: 0.85 - ETA: 4s - loss: 0.3531 - accuracy: 0.85 - ETA: 4s - loss: 0.3529 - accuracy: 0.85 - ETA: 4s - loss: 0.3533 - accuracy: 0.85 - ETA: 4s - loss: 0.3533 - accuracy: 0.85 - ETA: 4s - loss: 0.3531 - accuracy: 0.85 - ETA: 4s - loss: 0.3531 - accuracy: 0.85 - ETA: 4s - loss: 0.3526 - accuracy: 0.85 - ETA: 4s - loss: 0.3527 - accuracy: 0.85 - ETA: 3s - loss: 0.3526 - accuracy: 0.85 - ETA: 3s - loss: 0.3531 - accuracy: 0.85 - ETA: 3s - loss: 0.3534 - accuracy: 0.85 - ETA: 3s - loss: 0.3542 - accuracy: 0.85 - ETA: 3s - loss: 0.3544 - accuracy: 0.85 - ETA: 3s - loss: 0.3548 - accuracy: 0.85 - ETA: 3s - loss: 0.3553 - accuracy: 0.85 - ETA: 3s - loss: 0.3550 - accuracy: 0.85 - ETA: 3s - loss: 0.3553 - accuracy: 0.85 - ETA: 3s - loss: 0.3548 - accuracy: 0.85 - ETA: 3s - loss: 0.3548 - accuracy: 0.85 - ETA: 3s - loss: 0.3553 - accuracy: 0.85 - ETA: 3s - loss: 0.3551 - accuracy: 0.85 - ETA: 3s - loss: 0.3550 - accuracy: 0.85 - ETA: 3s - loss: 0.3547 - accuracy: 0.85 - ETA: 2s - loss: 0.3547 - accuracy: 0.85 - ETA: 2s - loss: 0.3542 - accuracy: 0.85 - ETA: 2s - loss: 0.3543 - accuracy: 0.85 - ETA: 2s - loss: 0.3540 - accuracy: 0.85 - ETA: 2s - loss: 0.3537 - accuracy: 0.85 - ETA: 2s - loss: 0.3534 - accuracy: 0.85 - ETA: 2s - loss: 0.3533 - accuracy: 0.85 - ETA: 2s - loss: 0.3532 - accuracy: 0.85 - ETA: 2s - loss: 0.3534 - accuracy: 0.85 - ETA: 2s - loss: 0.3531 - accuracy: 0.85 - ETA: 2s - loss: 0.3535 - accuracy: 0.85 - ETA: 2s - loss: 0.3535 - accuracy: 0.85 - ETA: 2s - loss: 0.3534 - accuracy: 0.85 - ETA: 2s - loss: 0.3535 - accuracy: 0.85 - ETA: 1s - loss: 0.3535 - accuracy: 0.85 - ETA: 1s - loss: 0.3535 - accuracy: 0.85 - ETA: 1s - loss: 0.3533 - accuracy: 0.85 - ETA: 1s - loss: 0.3534 - accuracy: 0.85 - ETA: 1s - loss: 0.3540 - accuracy: 0.85 - ETA: 1s - loss: 0.3541 - accuracy: 0.85 - ETA: 1s - loss: 0.3538 - accuracy: 0.85 - ETA: 1s - loss: 0.3535 - accuracy: 0.85 - ETA: 1s - loss: 0.3530 - accuracy: 0.85 - ETA: 1s - loss: 0.3531 - accuracy: 0.85 - ETA: 1s - loss: 0.3525 - accuracy: 0.85 - ETA: 1s - loss: 0.3532 - accuracy: 0.85 - ETA: 1s - loss: 0.3528 - accuracy: 0.85 - ETA: 1s - loss: 0.3532 - accuracy: 0.85 - ETA: 1s - loss: 0.3533 - accuracy: 0.85 - ETA: 0s - loss: 0.3534 - accuracy: 0.85 - ETA: 0s - loss: 0.3529 - accuracy: 0.85 - ETA: 0s - loss: 0.3528 - accuracy: 0.85 - ETA: 0s - loss: 0.3523 - accuracy: 0.85 - ETA: 0s - loss: 0.3519 - accuracy: 0.85 - ETA: 0s - loss: 0.3519 - accuracy: 0.85 - ETA: 0s - loss: 0.3520 - accuracy: 0.85 - ETA: 0s - loss: 0.3521 - accuracy: 0.85 - ETA: 0s - loss: 0.3520 - accuracy: 0.85 - ETA: 0s - loss: 0.3522 - accuracy: 0.85 - ETA: 0s - loss: 0.3522 - accuracy: 0.85 - ETA: 0s - loss: 0.3522 - accuracy: 0.85 - ETA: 0s - loss: 0.3521 - accuracy: 0.85 - ETA: 0s - loss: 0.3523 - accuracy: 0.85 - ETA: 0s - loss: 0.3525 - accuracy: 0.85 - 18s 23ms/step - loss: 0.3526 - accuracy: 0.8548\n",
      "Epoch 3/5\n",
      "559/781 [====================>.........] - ETA: 18s - loss: 0.4208 - accuracy: 0.812 - ETA: 17s - loss: 0.3275 - accuracy: 0.867 - ETA: 17s - loss: 0.3069 - accuracy: 0.870 - ETA: 17s - loss: 0.3178 - accuracy: 0.868 - ETA: 17s - loss: 0.3231 - accuracy: 0.875 - ETA: 17s - loss: 0.3252 - accuracy: 0.875 - ETA: 17s - loss: 0.3413 - accuracy: 0.866 - ETA: 17s - loss: 0.3426 - accuracy: 0.862 - ETA: 17s - loss: 0.3444 - accuracy: 0.860 - ETA: 16s - loss: 0.3415 - accuracy: 0.862 - ETA: 16s - loss: 0.3375 - accuracy: 0.866 - ETA: 16s - loss: 0.3324 - accuracy: 0.869 - ETA: 16s - loss: 0.3307 - accuracy: 0.869 - ETA: 16s - loss: 0.3265 - accuracy: 0.871 - ETA: 16s - loss: 0.3306 - accuracy: 0.870 - ETA: 16s - loss: 0.3228 - accuracy: 0.875 - ETA: 16s - loss: 0.3202 - accuracy: 0.875 - ETA: 16s - loss: 0.3167 - accuracy: 0.875 - ETA: 16s - loss: 0.3168 - accuracy: 0.876 - ETA: 16s - loss: 0.3129 - accuracy: 0.878 - ETA: 16s - loss: 0.3088 - accuracy: 0.880 - ETA: 16s - loss: 0.3089 - accuracy: 0.880 - ETA: 15s - loss: 0.3078 - accuracy: 0.880 - ETA: 15s - loss: 0.3063 - accuracy: 0.880 - ETA: 15s - loss: 0.3050 - accuracy: 0.881 - ETA: 15s - loss: 0.3026 - accuracy: 0.881 - ETA: 15s - loss: 0.2978 - accuracy: 0.884 - ETA: 15s - loss: 0.2945 - accuracy: 0.886 - ETA: 15s - loss: 0.2895 - accuracy: 0.889 - ETA: 15s - loss: 0.2847 - accuracy: 0.891 - ETA: 15s - loss: 0.2812 - accuracy: 0.892 - ETA: 15s - loss: 0.2790 - accuracy: 0.893 - ETA: 15s - loss: 0.2736 - accuracy: 0.895 - ETA: 15s - loss: 0.2685 - accuracy: 0.898 - ETA: 15s - loss: 0.2637 - accuracy: 0.900 - ETA: 15s - loss: 0.2579 - accuracy: 0.903 - ETA: 15s - loss: 0.2540 - accuracy: 0.904 - ETA: 15s - loss: 0.2521 - accuracy: 0.904 - ETA: 14s - loss: 0.2530 - accuracy: 0.903 - ETA: 14s - loss: 0.2554 - accuracy: 0.902 - ETA: 14s - loss: 0.2574 - accuracy: 0.902 - ETA: 14s - loss: 0.2598 - accuracy: 0.901 - ETA: 14s - loss: 0.2635 - accuracy: 0.900 - ETA: 14s - loss: 0.2683 - accuracy: 0.897 - ETA: 14s - loss: 0.2688 - accuracy: 0.897 - ETA: 14s - loss: 0.2709 - accuracy: 0.895 - ETA: 14s - loss: 0.2703 - accuracy: 0.895 - ETA: 14s - loss: 0.2708 - accuracy: 0.895 - ETA: 14s - loss: 0.2718 - accuracy: 0.894 - ETA: 14s - loss: 0.2719 - accuracy: 0.894 - ETA: 14s - loss: 0.2696 - accuracy: 0.895 - ETA: 14s - loss: 0.2665 - accuracy: 0.896 - ETA: 14s - loss: 0.2655 - accuracy: 0.897 - ETA: 14s - loss: 0.2637 - accuracy: 0.898 - ETA: 13s - loss: 0.2611 - accuracy: 0.899 - ETA: 13s - loss: 0.2595 - accuracy: 0.899 - ETA: 13s - loss: 0.2582 - accuracy: 0.899 - ETA: 13s - loss: 0.2578 - accuracy: 0.900 - ETA: 13s - loss: 0.2561 - accuracy: 0.900 - ETA: 13s - loss: 0.2547 - accuracy: 0.901 - ETA: 13s - loss: 0.2546 - accuracy: 0.901 - ETA: 13s - loss: 0.2524 - accuracy: 0.902 - ETA: 13s - loss: 0.2499 - accuracy: 0.903 - ETA: 13s - loss: 0.2503 - accuracy: 0.903 - ETA: 13s - loss: 0.2483 - accuracy: 0.903 - ETA: 13s - loss: 0.2472 - accuracy: 0.904 - ETA: 13s - loss: 0.2451 - accuracy: 0.905 - ETA: 13s - loss: 0.2437 - accuracy: 0.906 - ETA: 13s - loss: 0.2424 - accuracy: 0.906 - ETA: 13s - loss: 0.2411 - accuracy: 0.907 - ETA: 12s - loss: 0.2401 - accuracy: 0.908 - ETA: 12s - loss: 0.2401 - accuracy: 0.908 - ETA: 12s - loss: 0.2385 - accuracy: 0.908 - ETA: 12s - loss: 0.2388 - accuracy: 0.908 - ETA: 12s - loss: 0.2369 - accuracy: 0.909 - ETA: 12s - loss: 0.2353 - accuracy: 0.909 - ETA: 12s - loss: 0.2336 - accuracy: 0.910 - ETA: 12s - loss: 0.2326 - accuracy: 0.911 - ETA: 12s - loss: 0.2314 - accuracy: 0.911 - ETA: 12s - loss: 0.2296 - accuracy: 0.912 - ETA: 12s - loss: 0.2283 - accuracy: 0.913 - ETA: 12s - loss: 0.2266 - accuracy: 0.913 - ETA: 12s - loss: 0.2258 - accuracy: 0.914 - ETA: 12s - loss: 0.2257 - accuracy: 0.914 - ETA: 11s - loss: 0.2246 - accuracy: 0.914 - ETA: 11s - loss: 0.2238 - accuracy: 0.915 - ETA: 11s - loss: 0.2225 - accuracy: 0.915 - ETA: 11s - loss: 0.2215 - accuracy: 0.915 - ETA: 11s - loss: 0.2213 - accuracy: 0.916 - ETA: 11s - loss: 0.2197 - accuracy: 0.916 - ETA: 11s - loss: 0.2194 - accuracy: 0.916 - ETA: 11s - loss: 0.2187 - accuracy: 0.917 - ETA: 11s - loss: 0.2183 - accuracy: 0.917 - ETA: 11s - loss: 0.2173 - accuracy: 0.918 - ETA: 11s - loss: 0.2166 - accuracy: 0.918 - ETA: 11s - loss: 0.2158 - accuracy: 0.918 - ETA: 11s - loss: 0.2175 - accuracy: 0.918 - ETA: 11s - loss: 0.2165 - accuracy: 0.918 - ETA: 11s - loss: 0.2161 - accuracy: 0.918 - ETA: 10s - loss: 0.2154 - accuracy: 0.918 - ETA: 10s - loss: 0.2153 - accuracy: 0.918 - ETA: 10s - loss: 0.2157 - accuracy: 0.918 - ETA: 10s - loss: 0.2151 - accuracy: 0.919 - ETA: 10s - loss: 0.2150 - accuracy: 0.919 - ETA: 10s - loss: 0.2145 - accuracy: 0.919 - ETA: 10s - loss: 0.2146 - accuracy: 0.919 - ETA: 10s - loss: 0.2155 - accuracy: 0.919 - ETA: 10s - loss: 0.2151 - accuracy: 0.919 - ETA: 10s - loss: 0.2155 - accuracy: 0.919 - ETA: 10s - loss: 0.2162 - accuracy: 0.918 - ETA: 10s - loss: 0.2155 - accuracy: 0.919 - ETA: 10s - loss: 0.2151 - accuracy: 0.919 - ETA: 10s - loss: 0.2144 - accuracy: 0.920 - ETA: 10s - loss: 0.2145 - accuracy: 0.920 - ETA: 9s - loss: 0.2142 - accuracy: 0.920 - ETA: 9s - loss: 0.2161 - accuracy: 0.91 - ETA: 9s - loss: 0.2159 - accuracy: 0.91 - ETA: 9s - loss: 0.2157 - accuracy: 0.91 - ETA: 9s - loss: 0.2159 - accuracy: 0.91 - ETA: 9s - loss: 0.2160 - accuracy: 0.91 - ETA: 9s - loss: 0.2158 - accuracy: 0.91 - ETA: 9s - loss: 0.2155 - accuracy: 0.91 - ETA: 9s - loss: 0.2148 - accuracy: 0.92 - ETA: 9s - loss: 0.2145 - accuracy: 0.92 - ETA: 9s - loss: 0.2139 - accuracy: 0.92 - ETA: 9s - loss: 0.2141 - accuracy: 0.92 - ETA: 9s - loss: 0.2141 - accuracy: 0.92 - ETA: 9s - loss: 0.2147 - accuracy: 0.92 - ETA: 9s - loss: 0.2148 - accuracy: 0.92 - ETA: 9s - loss: 0.2148 - accuracy: 0.92 - ETA: 8s - loss: 0.2140 - accuracy: 0.92 - ETA: 8s - loss: 0.2137 - accuracy: 0.92 - ETA: 8s - loss: 0.2143 - accuracy: 0.92 - ETA: 8s - loss: 0.2141 - accuracy: 0.92 - ETA: 8s - loss: 0.2137 - accuracy: 0.92 - ETA: 8s - loss: 0.2142 - accuracy: 0.92 - ETA: 8s - loss: 0.2137 - accuracy: 0.92 - ETA: 8s - loss: 0.2142 - accuracy: 0.92 - ETA: 8s - loss: 0.2143 - accuracy: 0.92 - ETA: 8s - loss: 0.2141 - accuracy: 0.92 - ETA: 8s - loss: 0.2134 - accuracy: 0.92 - ETA: 8s - loss: 0.2135 - accuracy: 0.92 - ETA: 8s - loss: 0.2138 - accuracy: 0.92 - ETA: 8s - loss: 0.2136 - accuracy: 0.92 - ETA: 8s - loss: 0.2135 - accuracy: 0.92 - ETA: 7s - loss: 0.2131 - accuracy: 0.92 - ETA: 7s - loss: 0.2121 - accuracy: 0.92 - ETA: 7s - loss: 0.2115 - accuracy: 0.92 - ETA: 7s - loss: 0.2104 - accuracy: 0.92 - ETA: 7s - loss: 0.2096 - accuracy: 0.92 - ETA: 7s - loss: 0.2088 - accuracy: 0.92 - ETA: 7s - loss: 0.2090 - accuracy: 0.92 - ETA: 7s - loss: 0.2081 - accuracy: 0.92 - ETA: 7s - loss: 0.2075 - accuracy: 0.92 - ETA: 7s - loss: 0.2073 - accuracy: 0.92 - ETA: 7s - loss: 0.2070 - accuracy: 0.92 - ETA: 7s - loss: 0.2076 - accuracy: 0.92 - ETA: 7s - loss: 0.2079 - accuracy: 0.92 - ETA: 7s - loss: 0.2078 - accuracy: 0.92 - ETA: 6s - loss: 0.2082 - accuracy: 0.92 - ETA: 6s - loss: 0.2081 - accuracy: 0.92 - ETA: 6s - loss: 0.2078 - accuracy: 0.92 - ETA: 6s - loss: 0.2084 - accuracy: 0.92 - ETA: 6s - loss: 0.2077 - accuracy: 0.92 - ETA: 6s - loss: 0.2079 - accuracy: 0.92 - ETA: 6s - loss: 0.2075 - accuracy: 0.92 - ETA: 6s - loss: 0.2073 - accuracy: 0.92 - ETA: 6s - loss: 0.2070 - accuracy: 0.92 - ETA: 6s - loss: 0.2070 - accuracy: 0.92 - ETA: 6s - loss: 0.2076 - accuracy: 0.92 - ETA: 6s - loss: 0.2078 - accuracy: 0.92 - ETA: 6s - loss: 0.2083 - accuracy: 0.92 - ETA: 6s - loss: 0.2084 - accuracy: 0.92 - ETA: 6s - loss: 0.2088 - accuracy: 0.92 - ETA: 5s - loss: 0.2087 - accuracy: 0.92 - ETA: 5s - loss: 0.2086 - accuracy: 0.92 - ETA: 5s - loss: 0.2087 - accuracy: 0.92 - ETA: 5s - loss: 0.2087 - accuracy: 0.92 - ETA: 5s - loss: 0.2080 - accuracy: 0.92 - ETA: 5s - loss: 0.2083 - accuracy: 0.92 - ETA: 5s - loss: 0.2083 - accuracy: 0.92 - ETA: 5s - loss: 0.2086 - accuracy: 0.92 - ETA: 5s - loss: 0.2083 - accuracy: 0.92 - ETA: 5s - loss: 0.2082 - accuracy: 0.92 - ETA: 5s - loss: 0.2081 - accuracy: 0.92 - ETA: 5s - loss: 0.2083 - accuracy: 0.92 - ETA: 5s - loss: 0.2078 - accuracy: 0.92 - ETA: 5s - loss: 0.2076 - accuracy: 0.92 - ETA: 5s - loss: 0.2073 - accuracy: 0.9253781/781 [==============================] - ETA: 4s - loss: 0.2068 - accuracy: 0.92 - ETA: 4s - loss: 0.2067 - accuracy: 0.92 - ETA: 4s - loss: 0.2061 - accuracy: 0.92 - ETA: 4s - loss: 0.2055 - accuracy: 0.92 - ETA: 4s - loss: 0.2047 - accuracy: 0.92 - ETA: 4s - loss: 0.2039 - accuracy: 0.92 - ETA: 4s - loss: 0.2037 - accuracy: 0.92 - ETA: 4s - loss: 0.2039 - accuracy: 0.92 - ETA: 4s - loss: 0.2033 - accuracy: 0.92 - ETA: 4s - loss: 0.2032 - accuracy: 0.92 - ETA: 4s - loss: 0.2030 - accuracy: 0.92 - ETA: 4s - loss: 0.2024 - accuracy: 0.92 - ETA: 4s - loss: 0.2028 - accuracy: 0.92 - ETA: 4s - loss: 0.2025 - accuracy: 0.92 - ETA: 4s - loss: 0.2025 - accuracy: 0.92 - ETA: 3s - loss: 0.2025 - accuracy: 0.92 - ETA: 3s - loss: 0.2024 - accuracy: 0.92 - ETA: 3s - loss: 0.2031 - accuracy: 0.92 - ETA: 3s - loss: 0.2041 - accuracy: 0.92 - ETA: 3s - loss: 0.2043 - accuracy: 0.92 - ETA: 3s - loss: 0.2049 - accuracy: 0.92 - ETA: 3s - loss: 0.2054 - accuracy: 0.92 - ETA: 3s - loss: 0.2054 - accuracy: 0.92 - ETA: 3s - loss: 0.2055 - accuracy: 0.92 - ETA: 3s - loss: 0.2050 - accuracy: 0.92 - ETA: 3s - loss: 0.2048 - accuracy: 0.92 - ETA: 3s - loss: 0.2049 - accuracy: 0.92 - ETA: 3s - loss: 0.2046 - accuracy: 0.92 - ETA: 3s - loss: 0.2041 - accuracy: 0.92 - ETA: 2s - loss: 0.2044 - accuracy: 0.92 - ETA: 2s - loss: 0.2042 - accuracy: 0.92 - ETA: 2s - loss: 0.2036 - accuracy: 0.92 - ETA: 2s - loss: 0.2036 - accuracy: 0.92 - ETA: 2s - loss: 0.2033 - accuracy: 0.92 - ETA: 2s - loss: 0.2029 - accuracy: 0.92 - ETA: 2s - loss: 0.2025 - accuracy: 0.92 - ETA: 2s - loss: 0.2022 - accuracy: 0.92 - ETA: 2s - loss: 0.2020 - accuracy: 0.92 - ETA: 2s - loss: 0.2024 - accuracy: 0.92 - ETA: 2s - loss: 0.2023 - accuracy: 0.92 - ETA: 2s - loss: 0.2028 - accuracy: 0.92 - ETA: 2s - loss: 0.2027 - accuracy: 0.92 - ETA: 2s - loss: 0.2028 - accuracy: 0.92 - ETA: 2s - loss: 0.2031 - accuracy: 0.92 - ETA: 1s - loss: 0.2027 - accuracy: 0.92 - ETA: 1s - loss: 0.2025 - accuracy: 0.92 - ETA: 1s - loss: 0.2024 - accuracy: 0.92 - ETA: 1s - loss: 0.2022 - accuracy: 0.92 - ETA: 1s - loss: 0.2029 - accuracy: 0.92 - ETA: 1s - loss: 0.2029 - accuracy: 0.92 - ETA: 1s - loss: 0.2026 - accuracy: 0.92 - ETA: 1s - loss: 0.2026 - accuracy: 0.92 - ETA: 1s - loss: 0.2022 - accuracy: 0.92 - ETA: 1s - loss: 0.2017 - accuracy: 0.92 - ETA: 1s - loss: 0.2016 - accuracy: 0.92 - ETA: 1s - loss: 0.2017 - accuracy: 0.92 - ETA: 1s - loss: 0.2013 - accuracy: 0.92 - ETA: 1s - loss: 0.2019 - accuracy: 0.92 - ETA: 1s - loss: 0.2020 - accuracy: 0.92 - ETA: 0s - loss: 0.2022 - accuracy: 0.92 - ETA: 0s - loss: 0.2020 - accuracy: 0.92 - ETA: 0s - loss: 0.2020 - accuracy: 0.92 - ETA: 0s - loss: 0.2019 - accuracy: 0.92 - ETA: 0s - loss: 0.2018 - accuracy: 0.92 - ETA: 0s - loss: 0.2017 - accuracy: 0.92 - ETA: 0s - loss: 0.2015 - accuracy: 0.92 - ETA: 0s - loss: 0.2015 - accuracy: 0.92 - ETA: 0s - loss: 0.2015 - accuracy: 0.92 - ETA: 0s - loss: 0.2019 - accuracy: 0.92 - ETA: 0s - loss: 0.2020 - accuracy: 0.92 - ETA: 0s - loss: 0.2022 - accuracy: 0.92 - ETA: 0s - loss: 0.2022 - accuracy: 0.92 - ETA: 0s - loss: 0.2025 - accuracy: 0.92 - 18s 23ms/step - loss: 0.2026 - accuracy: 0.9277\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562/781 [====================>.........] - ETA: 17s - loss: 0.2390 - accuracy: 0.937 - ETA: 17s - loss: 0.1717 - accuracy: 0.960 - ETA: 17s - loss: 0.1643 - accuracy: 0.955 - ETA: 17s - loss: 0.1548 - accuracy: 0.953 - ETA: 17s - loss: 0.1608 - accuracy: 0.951 - ETA: 17s - loss: 0.1647 - accuracy: 0.949 - ETA: 17s - loss: 0.1826 - accuracy: 0.944 - ETA: 17s - loss: 0.1686 - accuracy: 0.947 - ETA: 17s - loss: 0.1648 - accuracy: 0.948 - ETA: 17s - loss: 0.1644 - accuracy: 0.946 - ETA: 16s - loss: 0.1597 - accuracy: 0.947 - ETA: 16s - loss: 0.1595 - accuracy: 0.949 - ETA: 16s - loss: 0.1536 - accuracy: 0.952 - ETA: 16s - loss: 0.1543 - accuracy: 0.953 - ETA: 16s - loss: 0.1563 - accuracy: 0.952 - ETA: 16s - loss: 0.1530 - accuracy: 0.953 - ETA: 16s - loss: 0.1498 - accuracy: 0.954 - ETA: 16s - loss: 0.1475 - accuracy: 0.955 - ETA: 16s - loss: 0.1478 - accuracy: 0.954 - ETA: 16s - loss: 0.1478 - accuracy: 0.953 - ETA: 16s - loss: 0.1475 - accuracy: 0.954 - ETA: 16s - loss: 0.1476 - accuracy: 0.955 - ETA: 16s - loss: 0.1481 - accuracy: 0.954 - ETA: 16s - loss: 0.1466 - accuracy: 0.954 - ETA: 16s - loss: 0.1470 - accuracy: 0.954 - ETA: 15s - loss: 0.1445 - accuracy: 0.955 - ETA: 15s - loss: 0.1428 - accuracy: 0.956 - ETA: 15s - loss: 0.1440 - accuracy: 0.955 - ETA: 15s - loss: 0.1406 - accuracy: 0.956 - ETA: 15s - loss: 0.1402 - accuracy: 0.956 - ETA: 15s - loss: 0.1381 - accuracy: 0.957 - ETA: 15s - loss: 0.1360 - accuracy: 0.958 - ETA: 15s - loss: 0.1364 - accuracy: 0.958 - ETA: 15s - loss: 0.1360 - accuracy: 0.957 - ETA: 15s - loss: 0.1331 - accuracy: 0.958 - ETA: 15s - loss: 0.1301 - accuracy: 0.959 - ETA: 15s - loss: 0.1330 - accuracy: 0.957 - ETA: 15s - loss: 0.1310 - accuracy: 0.958 - ETA: 15s - loss: 0.1327 - accuracy: 0.957 - ETA: 14s - loss: 0.1344 - accuracy: 0.956 - ETA: 14s - loss: 0.1346 - accuracy: 0.956 - ETA: 14s - loss: 0.1355 - accuracy: 0.955 - ETA: 14s - loss: 0.1361 - accuracy: 0.955 - ETA: 14s - loss: 0.1371 - accuracy: 0.955 - ETA: 14s - loss: 0.1375 - accuracy: 0.954 - ETA: 14s - loss: 0.1387 - accuracy: 0.953 - ETA: 14s - loss: 0.1376 - accuracy: 0.953 - ETA: 14s - loss: 0.1375 - accuracy: 0.953 - ETA: 14s - loss: 0.1383 - accuracy: 0.953 - ETA: 14s - loss: 0.1395 - accuracy: 0.952 - ETA: 14s - loss: 0.1390 - accuracy: 0.952 - ETA: 14s - loss: 0.1397 - accuracy: 0.952 - ETA: 14s - loss: 0.1408 - accuracy: 0.952 - ETA: 14s - loss: 0.1431 - accuracy: 0.951 - ETA: 13s - loss: 0.1432 - accuracy: 0.951 - ETA: 13s - loss: 0.1429 - accuracy: 0.951 - ETA: 13s - loss: 0.1426 - accuracy: 0.951 - ETA: 13s - loss: 0.1416 - accuracy: 0.952 - ETA: 13s - loss: 0.1401 - accuracy: 0.952 - ETA: 13s - loss: 0.1392 - accuracy: 0.952 - ETA: 13s - loss: 0.1376 - accuracy: 0.953 - ETA: 13s - loss: 0.1378 - accuracy: 0.953 - ETA: 13s - loss: 0.1376 - accuracy: 0.953 - ETA: 13s - loss: 0.1373 - accuracy: 0.953 - ETA: 13s - loss: 0.1367 - accuracy: 0.953 - ETA: 13s - loss: 0.1361 - accuracy: 0.953 - ETA: 13s - loss: 0.1356 - accuracy: 0.953 - ETA: 13s - loss: 0.1350 - accuracy: 0.954 - ETA: 13s - loss: 0.1354 - accuracy: 0.954 - ETA: 12s - loss: 0.1350 - accuracy: 0.954 - ETA: 12s - loss: 0.1353 - accuracy: 0.953 - ETA: 12s - loss: 0.1344 - accuracy: 0.954 - ETA: 12s - loss: 0.1346 - accuracy: 0.953 - ETA: 12s - loss: 0.1343 - accuracy: 0.954 - ETA: 12s - loss: 0.1337 - accuracy: 0.954 - ETA: 12s - loss: 0.1330 - accuracy: 0.954 - ETA: 12s - loss: 0.1323 - accuracy: 0.954 - ETA: 12s - loss: 0.1325 - accuracy: 0.954 - ETA: 12s - loss: 0.1326 - accuracy: 0.954 - ETA: 12s - loss: 0.1325 - accuracy: 0.954 - ETA: 12s - loss: 0.1330 - accuracy: 0.954 - ETA: 12s - loss: 0.1317 - accuracy: 0.954 - ETA: 12s - loss: 0.1310 - accuracy: 0.954 - ETA: 12s - loss: 0.1308 - accuracy: 0.954 - ETA: 11s - loss: 0.1308 - accuracy: 0.955 - ETA: 11s - loss: 0.1298 - accuracy: 0.955 - ETA: 11s - loss: 0.1290 - accuracy: 0.955 - ETA: 11s - loss: 0.1285 - accuracy: 0.956 - ETA: 11s - loss: 0.1283 - accuracy: 0.956 - ETA: 11s - loss: 0.1276 - accuracy: 0.956 - ETA: 11s - loss: 0.1271 - accuracy: 0.956 - ETA: 11s - loss: 0.1265 - accuracy: 0.957 - ETA: 11s - loss: 0.1260 - accuracy: 0.956 - ETA: 11s - loss: 0.1252 - accuracy: 0.957 - ETA: 11s - loss: 0.1246 - accuracy: 0.957 - ETA: 11s - loss: 0.1245 - accuracy: 0.957 - ETA: 11s - loss: 0.1253 - accuracy: 0.957 - ETA: 11s - loss: 0.1244 - accuracy: 0.957 - ETA: 11s - loss: 0.1246 - accuracy: 0.957 - ETA: 10s - loss: 0.1240 - accuracy: 0.957 - ETA: 10s - loss: 0.1247 - accuracy: 0.957 - ETA: 10s - loss: 0.1243 - accuracy: 0.957 - ETA: 10s - loss: 0.1239 - accuracy: 0.958 - ETA: 10s - loss: 0.1241 - accuracy: 0.958 - ETA: 10s - loss: 0.1240 - accuracy: 0.958 - ETA: 10s - loss: 0.1248 - accuracy: 0.957 - ETA: 10s - loss: 0.1245 - accuracy: 0.957 - ETA: 10s - loss: 0.1246 - accuracy: 0.957 - ETA: 10s - loss: 0.1245 - accuracy: 0.957 - ETA: 10s - loss: 0.1249 - accuracy: 0.958 - ETA: 10s - loss: 0.1241 - accuracy: 0.958 - ETA: 10s - loss: 0.1240 - accuracy: 0.958 - ETA: 10s - loss: 0.1241 - accuracy: 0.958 - ETA: 10s - loss: 0.1245 - accuracy: 0.958 - ETA: 9s - loss: 0.1246 - accuracy: 0.957 - ETA: 9s - loss: 0.1245 - accuracy: 0.95 - ETA: 9s - loss: 0.1255 - accuracy: 0.95 - ETA: 9s - loss: 0.1259 - accuracy: 0.95 - ETA: 9s - loss: 0.1261 - accuracy: 0.95 - ETA: 9s - loss: 0.1265 - accuracy: 0.95 - ETA: 9s - loss: 0.1264 - accuracy: 0.95 - ETA: 9s - loss: 0.1264 - accuracy: 0.95 - ETA: 9s - loss: 0.1260 - accuracy: 0.95 - ETA: 9s - loss: 0.1257 - accuracy: 0.95 - ETA: 9s - loss: 0.1260 - accuracy: 0.95 - ETA: 9s - loss: 0.1259 - accuracy: 0.95 - ETA: 9s - loss: 0.1252 - accuracy: 0.95 - ETA: 9s - loss: 0.1250 - accuracy: 0.95 - ETA: 9s - loss: 0.1248 - accuracy: 0.95 - ETA: 8s - loss: 0.1243 - accuracy: 0.95 - ETA: 8s - loss: 0.1241 - accuracy: 0.95 - ETA: 8s - loss: 0.1243 - accuracy: 0.95 - ETA: 8s - loss: 0.1251 - accuracy: 0.95 - ETA: 8s - loss: 0.1247 - accuracy: 0.95 - ETA: 8s - loss: 0.1248 - accuracy: 0.95 - ETA: 8s - loss: 0.1252 - accuracy: 0.95 - ETA: 8s - loss: 0.1256 - accuracy: 0.95 - ETA: 8s - loss: 0.1255 - accuracy: 0.95 - ETA: 8s - loss: 0.1251 - accuracy: 0.95 - ETA: 8s - loss: 0.1248 - accuracy: 0.95 - ETA: 8s - loss: 0.1249 - accuracy: 0.95 - ETA: 8s - loss: 0.1258 - accuracy: 0.95 - ETA: 8s - loss: 0.1257 - accuracy: 0.95 - ETA: 8s - loss: 0.1254 - accuracy: 0.95 - ETA: 7s - loss: 0.1256 - accuracy: 0.95 - ETA: 7s - loss: 0.1253 - accuracy: 0.95 - ETA: 7s - loss: 0.1253 - accuracy: 0.95 - ETA: 7s - loss: 0.1247 - accuracy: 0.95 - ETA: 7s - loss: 0.1241 - accuracy: 0.95 - ETA: 7s - loss: 0.1235 - accuracy: 0.95 - ETA: 7s - loss: 0.1233 - accuracy: 0.95 - ETA: 7s - loss: 0.1230 - accuracy: 0.95 - ETA: 7s - loss: 0.1225 - accuracy: 0.95 - ETA: 7s - loss: 0.1228 - accuracy: 0.95 - ETA: 7s - loss: 0.1231 - accuracy: 0.95 - ETA: 7s - loss: 0.1230 - accuracy: 0.95 - ETA: 7s - loss: 0.1231 - accuracy: 0.95 - ETA: 7s - loss: 0.1235 - accuracy: 0.95 - ETA: 6s - loss: 0.1237 - accuracy: 0.95 - ETA: 6s - loss: 0.1236 - accuracy: 0.95 - ETA: 6s - loss: 0.1238 - accuracy: 0.95 - ETA: 6s - loss: 0.1240 - accuracy: 0.95 - ETA: 6s - loss: 0.1241 - accuracy: 0.95 - ETA: 6s - loss: 0.1244 - accuracy: 0.95 - ETA: 6s - loss: 0.1242 - accuracy: 0.95 - ETA: 6s - loss: 0.1239 - accuracy: 0.95 - ETA: 6s - loss: 0.1235 - accuracy: 0.95 - ETA: 6s - loss: 0.1236 - accuracy: 0.95 - ETA: 6s - loss: 0.1243 - accuracy: 0.95 - ETA: 6s - loss: 0.1242 - accuracy: 0.95 - ETA: 6s - loss: 0.1241 - accuracy: 0.95 - ETA: 6s - loss: 0.1245 - accuracy: 0.95 - ETA: 6s - loss: 0.1246 - accuracy: 0.95 - ETA: 5s - loss: 0.1251 - accuracy: 0.95 - ETA: 5s - loss: 0.1254 - accuracy: 0.95 - ETA: 5s - loss: 0.1253 - accuracy: 0.95 - ETA: 5s - loss: 0.1252 - accuracy: 0.95 - ETA: 5s - loss: 0.1254 - accuracy: 0.95 - ETA: 5s - loss: 0.1254 - accuracy: 0.95 - ETA: 5s - loss: 0.1254 - accuracy: 0.95 - ETA: 5s - loss: 0.1251 - accuracy: 0.95 - ETA: 5s - loss: 0.1254 - accuracy: 0.95 - ETA: 5s - loss: 0.1253 - accuracy: 0.95 - ETA: 5s - loss: 0.1253 - accuracy: 0.95 - ETA: 5s - loss: 0.1254 - accuracy: 0.95 - ETA: 5s - loss: 0.1254 - accuracy: 0.95 - ETA: 5s - loss: 0.1251 - accuracy: 0.95 - ETA: 5s - loss: 0.1246 - accuracy: 0.95 - ETA: 4s - loss: 0.1245 - accuracy: 0.9584"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 4s - loss: 0.1244 - accuracy: 0.95 - ETA: 4s - loss: 0.1240 - accuracy: 0.95 - ETA: 4s - loss: 0.1236 - accuracy: 0.95 - ETA: 4s - loss: 0.1231 - accuracy: 0.95 - ETA: 4s - loss: 0.1229 - accuracy: 0.95 - ETA: 4s - loss: 0.1234 - accuracy: 0.95 - ETA: 4s - loss: 0.1235 - accuracy: 0.95 - ETA: 4s - loss: 0.1234 - accuracy: 0.95 - ETA: 4s - loss: 0.1239 - accuracy: 0.95 - ETA: 4s - loss: 0.1246 - accuracy: 0.95 - ETA: 4s - loss: 0.1251 - accuracy: 0.95 - ETA: 4s - loss: 0.1252 - accuracy: 0.95 - ETA: 4s - loss: 0.1250 - accuracy: 0.95 - ETA: 4s - loss: 0.1253 - accuracy: 0.95 - ETA: 3s - loss: 0.1257 - accuracy: 0.95 - ETA: 3s - loss: 0.1257 - accuracy: 0.95 - ETA: 3s - loss: 0.1259 - accuracy: 0.95 - ETA: 3s - loss: 0.1263 - accuracy: 0.95 - ETA: 3s - loss: 0.1266 - accuracy: 0.95 - ETA: 3s - loss: 0.1272 - accuracy: 0.95 - ETA: 3s - loss: 0.1274 - accuracy: 0.95 - ETA: 3s - loss: 0.1272 - accuracy: 0.95 - ETA: 3s - loss: 0.1276 - accuracy: 0.95 - ETA: 3s - loss: 0.1276 - accuracy: 0.95 - ETA: 3s - loss: 0.1274 - accuracy: 0.95 - ETA: 3s - loss: 0.1272 - accuracy: 0.95 - ETA: 3s - loss: 0.1272 - accuracy: 0.95 - ETA: 3s - loss: 0.1272 - accuracy: 0.95 - ETA: 3s - loss: 0.1272 - accuracy: 0.95 - ETA: 2s - loss: 0.1271 - accuracy: 0.95 - ETA: 2s - loss: 0.1267 - accuracy: 0.95 - ETA: 2s - loss: 0.1271 - accuracy: 0.95 - ETA: 2s - loss: 0.1270 - accuracy: 0.95 - ETA: 2s - loss: 0.1267 - accuracy: 0.95 - ETA: 2s - loss: 0.1266 - accuracy: 0.95 - ETA: 2s - loss: 0.1263 - accuracy: 0.95 - ETA: 2s - loss: 0.1261 - accuracy: 0.95 - ETA: 2s - loss: 0.1260 - accuracy: 0.95 - ETA: 2s - loss: 0.1261 - accuracy: 0.95 - ETA: 2s - loss: 0.1259 - accuracy: 0.95 - ETA: 2s - loss: 0.1259 - accuracy: 0.95 - ETA: 2s - loss: 0.1257 - accuracy: 0.95 - ETA: 2s - loss: 0.1261 - accuracy: 0.95 - ETA: 1s - loss: 0.1259 - accuracy: 0.95 - ETA: 1s - loss: 0.1257 - accuracy: 0.95 - ETA: 1s - loss: 0.1264 - accuracy: 0.95 - ETA: 1s - loss: 0.1263 - accuracy: 0.95 - ETA: 1s - loss: 0.1265 - accuracy: 0.95 - ETA: 1s - loss: 0.1267 - accuracy: 0.95 - ETA: 1s - loss: 0.1268 - accuracy: 0.95 - ETA: 1s - loss: 0.1267 - accuracy: 0.95 - ETA: 1s - loss: 0.1266 - accuracy: 0.95 - ETA: 1s - loss: 0.1268 - accuracy: 0.95 - ETA: 1s - loss: 0.1266 - accuracy: 0.95 - ETA: 1s - loss: 0.1264 - accuracy: 0.95 - ETA: 1s - loss: 0.1263 - accuracy: 0.95 - ETA: 1s - loss: 0.1262 - accuracy: 0.95 - ETA: 1s - loss: 0.1260 - accuracy: 0.95 - ETA: 0s - loss: 0.1263 - accuracy: 0.95 - ETA: 0s - loss: 0.1264 - accuracy: 0.95 - ETA: 0s - loss: 0.1262 - accuracy: 0.95 - ETA: 0s - loss: 0.1261 - accuracy: 0.95 - ETA: 0s - loss: 0.1261 - accuracy: 0.95 - ETA: 0s - loss: 0.1260 - accuracy: 0.95 - ETA: 0s - loss: 0.1259 - accuracy: 0.95 - ETA: 0s - loss: 0.1257 - accuracy: 0.95 - ETA: 0s - loss: 0.1254 - accuracy: 0.95 - ETA: 0s - loss: 0.1252 - accuracy: 0.95 - ETA: 0s - loss: 0.1254 - accuracy: 0.95 - ETA: 0s - loss: 0.1260 - accuracy: 0.95 - ETA: 0s - loss: 0.1259 - accuracy: 0.95 - ETA: 0s - loss: 0.1258 - accuracy: 0.95 - ETA: 0s - loss: 0.1258 - accuracy: 0.95 - 18s 23ms/step - loss: 0.1257 - accuracy: 0.9572\n",
      "Epoch 5/5\n",
      "561/781 [====================>.........] - ETA: 17s - loss: 0.0689 - accuracy: 0.968 - ETA: 17s - loss: 0.0591 - accuracy: 0.984 - ETA: 17s - loss: 0.0566 - accuracy: 0.986 - ETA: 17s - loss: 0.0875 - accuracy: 0.968 - ETA: 17s - loss: 0.0876 - accuracy: 0.971 - ETA: 17s - loss: 0.0989 - accuracy: 0.966 - ETA: 17s - loss: 0.1131 - accuracy: 0.960 - ETA: 17s - loss: 0.1109 - accuracy: 0.960 - ETA: 17s - loss: 0.1093 - accuracy: 0.958 - ETA: 17s - loss: 0.1102 - accuracy: 0.958 - ETA: 17s - loss: 0.1145 - accuracy: 0.957 - ETA: 16s - loss: 0.1148 - accuracy: 0.958 - ETA: 16s - loss: 0.1099 - accuracy: 0.960 - ETA: 16s - loss: 0.1081 - accuracy: 0.960 - ETA: 16s - loss: 0.1126 - accuracy: 0.958 - ETA: 16s - loss: 0.1117 - accuracy: 0.958 - ETA: 16s - loss: 0.1089 - accuracy: 0.959 - ETA: 16s - loss: 0.1075 - accuracy: 0.961 - ETA: 16s - loss: 0.1081 - accuracy: 0.962 - ETA: 16s - loss: 0.1084 - accuracy: 0.962 - ETA: 16s - loss: 0.1053 - accuracy: 0.963 - ETA: 16s - loss: 0.1081 - accuracy: 0.963 - ETA: 16s - loss: 0.1102 - accuracy: 0.963 - ETA: 16s - loss: 0.1083 - accuracy: 0.963 - ETA: 16s - loss: 0.1102 - accuracy: 0.962 - ETA: 15s - loss: 0.1079 - accuracy: 0.962 - ETA: 15s - loss: 0.1050 - accuracy: 0.963 - ETA: 15s - loss: 0.1027 - accuracy: 0.964 - ETA: 15s - loss: 0.1028 - accuracy: 0.965 - ETA: 15s - loss: 0.1011 - accuracy: 0.965 - ETA: 15s - loss: 0.1002 - accuracy: 0.965 - ETA: 15s - loss: 0.1002 - accuracy: 0.965 - ETA: 15s - loss: 0.0997 - accuracy: 0.965 - ETA: 15s - loss: 0.0975 - accuracy: 0.966 - ETA: 15s - loss: 0.0966 - accuracy: 0.966 - ETA: 15s - loss: 0.0955 - accuracy: 0.967 - ETA: 15s - loss: 0.0957 - accuracy: 0.967 - ETA: 15s - loss: 0.0946 - accuracy: 0.967 - ETA: 15s - loss: 0.0942 - accuracy: 0.968 - ETA: 15s - loss: 0.0978 - accuracy: 0.966 - ETA: 15s - loss: 0.0980 - accuracy: 0.966 - ETA: 14s - loss: 0.0989 - accuracy: 0.966 - ETA: 14s - loss: 0.1005 - accuracy: 0.965 - ETA: 14s - loss: 0.1012 - accuracy: 0.965 - ETA: 14s - loss: 0.1010 - accuracy: 0.965 - ETA: 14s - loss: 0.1032 - accuracy: 0.964 - ETA: 14s - loss: 0.1044 - accuracy: 0.964 - ETA: 14s - loss: 0.1037 - accuracy: 0.964 - ETA: 14s - loss: 0.1034 - accuracy: 0.964 - ETA: 14s - loss: 0.1033 - accuracy: 0.964 - ETA: 14s - loss: 0.1039 - accuracy: 0.964 - ETA: 14s - loss: 0.1024 - accuracy: 0.965 - ETA: 14s - loss: 0.1025 - accuracy: 0.965 - ETA: 14s - loss: 0.1024 - accuracy: 0.965 - ETA: 14s - loss: 0.1013 - accuracy: 0.966 - ETA: 14s - loss: 0.1052 - accuracy: 0.965 - ETA: 13s - loss: 0.1050 - accuracy: 0.965 - ETA: 13s - loss: 0.1048 - accuracy: 0.965 - ETA: 13s - loss: 0.1048 - accuracy: 0.965 - ETA: 13s - loss: 0.1062 - accuracy: 0.964 - ETA: 13s - loss: 0.1062 - accuracy: 0.964 - ETA: 13s - loss: 0.1051 - accuracy: 0.964 - ETA: 13s - loss: 0.1060 - accuracy: 0.964 - ETA: 13s - loss: 0.1078 - accuracy: 0.963 - ETA: 13s - loss: 0.1087 - accuracy: 0.962 - ETA: 13s - loss: 0.1079 - accuracy: 0.962 - ETA: 13s - loss: 0.1066 - accuracy: 0.963 - ETA: 13s - loss: 0.1083 - accuracy: 0.962 - ETA: 13s - loss: 0.1086 - accuracy: 0.962 - ETA: 13s - loss: 0.1093 - accuracy: 0.962 - ETA: 12s - loss: 0.1086 - accuracy: 0.962 - ETA: 12s - loss: 0.1085 - accuracy: 0.962 - ETA: 12s - loss: 0.1092 - accuracy: 0.962 - ETA: 12s - loss: 0.1091 - accuracy: 0.962 - ETA: 12s - loss: 0.1088 - accuracy: 0.962 - ETA: 12s - loss: 0.1091 - accuracy: 0.962 - ETA: 12s - loss: 0.1096 - accuracy: 0.961 - ETA: 12s - loss: 0.1094 - accuracy: 0.961 - ETA: 12s - loss: 0.1086 - accuracy: 0.962 - ETA: 12s - loss: 0.1096 - accuracy: 0.962 - ETA: 12s - loss: 0.1101 - accuracy: 0.961 - ETA: 12s - loss: 0.1096 - accuracy: 0.961 - ETA: 12s - loss: 0.1095 - accuracy: 0.961 - ETA: 12s - loss: 0.1094 - accuracy: 0.961 - ETA: 12s - loss: 0.1089 - accuracy: 0.961 - ETA: 11s - loss: 0.1080 - accuracy: 0.961 - ETA: 11s - loss: 0.1075 - accuracy: 0.962 - ETA: 11s - loss: 0.1067 - accuracy: 0.962 - ETA: 11s - loss: 0.1061 - accuracy: 0.962 - ETA: 11s - loss: 0.1054 - accuracy: 0.962 - ETA: 11s - loss: 0.1048 - accuracy: 0.962 - ETA: 11s - loss: 0.1045 - accuracy: 0.963 - ETA: 11s - loss: 0.1036 - accuracy: 0.963 - ETA: 11s - loss: 0.1034 - accuracy: 0.963 - ETA: 11s - loss: 0.1033 - accuracy: 0.963 - ETA: 11s - loss: 0.1031 - accuracy: 0.963 - ETA: 11s - loss: 0.1032 - accuracy: 0.963 - ETA: 11s - loss: 0.1032 - accuracy: 0.963 - ETA: 11s - loss: 0.1046 - accuracy: 0.962 - ETA: 11s - loss: 0.1048 - accuracy: 0.962 - ETA: 10s - loss: 0.1043 - accuracy: 0.962 - ETA: 10s - loss: 0.1054 - accuracy: 0.962 - ETA: 10s - loss: 0.1062 - accuracy: 0.961 - ETA: 10s - loss: 0.1062 - accuracy: 0.961 - ETA: 10s - loss: 0.1058 - accuracy: 0.961 - ETA: 10s - loss: 0.1056 - accuracy: 0.961 - ETA: 10s - loss: 0.1064 - accuracy: 0.961 - ETA: 10s - loss: 0.1067 - accuracy: 0.961 - ETA: 10s - loss: 0.1065 - accuracy: 0.961 - ETA: 10s - loss: 0.1071 - accuracy: 0.961 - ETA: 10s - loss: 0.1071 - accuracy: 0.961 - ETA: 10s - loss: 0.1065 - accuracy: 0.961 - ETA: 10s - loss: 0.1060 - accuracy: 0.961 - ETA: 10s - loss: 0.1059 - accuracy: 0.962 - ETA: 10s - loss: 0.1055 - accuracy: 0.961 - ETA: 9s - loss: 0.1050 - accuracy: 0.962 - ETA: 9s - loss: 0.1051 - accuracy: 0.96 - ETA: 9s - loss: 0.1061 - accuracy: 0.96 - ETA: 9s - loss: 0.1055 - accuracy: 0.96 - ETA: 9s - loss: 0.1049 - accuracy: 0.96 - ETA: 9s - loss: 0.1054 - accuracy: 0.96 - ETA: 9s - loss: 0.1050 - accuracy: 0.96 - ETA: 9s - loss: 0.1047 - accuracy: 0.96 - ETA: 9s - loss: 0.1043 - accuracy: 0.96 - ETA: 9s - loss: 0.1045 - accuracy: 0.96 - ETA: 9s - loss: 0.1045 - accuracy: 0.96 - ETA: 9s - loss: 0.1040 - accuracy: 0.96 - ETA: 9s - loss: 0.1042 - accuracy: 0.96 - ETA: 9s - loss: 0.1043 - accuracy: 0.96 - ETA: 9s - loss: 0.1047 - accuracy: 0.96 - ETA: 8s - loss: 0.1052 - accuracy: 0.96 - ETA: 8s - loss: 0.1059 - accuracy: 0.96 - ETA: 8s - loss: 0.1066 - accuracy: 0.96 - ETA: 8s - loss: 0.1063 - accuracy: 0.96 - ETA: 8s - loss: 0.1061 - accuracy: 0.96 - ETA: 8s - loss: 0.1059 - accuracy: 0.96 - ETA: 8s - loss: 0.1057 - accuracy: 0.96 - ETA: 8s - loss: 0.1053 - accuracy: 0.96 - ETA: 8s - loss: 0.1053 - accuracy: 0.96 - ETA: 8s - loss: 0.1051 - accuracy: 0.96 - ETA: 8s - loss: 0.1047 - accuracy: 0.96 - ETA: 8s - loss: 0.1049 - accuracy: 0.96 - ETA: 8s - loss: 0.1055 - accuracy: 0.96 - ETA: 8s - loss: 0.1054 - accuracy: 0.96 - ETA: 8s - loss: 0.1049 - accuracy: 0.96 - ETA: 7s - loss: 0.1046 - accuracy: 0.96 - ETA: 7s - loss: 0.1042 - accuracy: 0.96 - ETA: 7s - loss: 0.1039 - accuracy: 0.96 - ETA: 7s - loss: 0.1036 - accuracy: 0.96 - ETA: 7s - loss: 0.1034 - accuracy: 0.96 - ETA: 7s - loss: 0.1034 - accuracy: 0.96 - ETA: 7s - loss: 0.1033 - accuracy: 0.96 - ETA: 7s - loss: 0.1028 - accuracy: 0.96 - ETA: 7s - loss: 0.1026 - accuracy: 0.96 - ETA: 7s - loss: 0.1022 - accuracy: 0.96 - ETA: 7s - loss: 0.1019 - accuracy: 0.96 - ETA: 7s - loss: 0.1013 - accuracy: 0.96 - ETA: 7s - loss: 0.1022 - accuracy: 0.96 - ETA: 7s - loss: 0.1018 - accuracy: 0.96 - ETA: 6s - loss: 0.1017 - accuracy: 0.96 - ETA: 6s - loss: 0.1017 - accuracy: 0.96 - ETA: 6s - loss: 0.1020 - accuracy: 0.96 - ETA: 6s - loss: 0.1022 - accuracy: 0.96 - ETA: 6s - loss: 0.1023 - accuracy: 0.96 - ETA: 6s - loss: 0.1023 - accuracy: 0.96 - ETA: 6s - loss: 0.1019 - accuracy: 0.96 - ETA: 6s - loss: 0.1017 - accuracy: 0.96 - ETA: 6s - loss: 0.1019 - accuracy: 0.96 - ETA: 6s - loss: 0.1019 - accuracy: 0.96 - ETA: 6s - loss: 0.1021 - accuracy: 0.96 - ETA: 6s - loss: 0.1025 - accuracy: 0.96 - ETA: 6s - loss: 0.1026 - accuracy: 0.96 - ETA: 6s - loss: 0.1030 - accuracy: 0.96 - ETA: 6s - loss: 0.1035 - accuracy: 0.96 - ETA: 5s - loss: 0.1038 - accuracy: 0.96 - ETA: 5s - loss: 0.1034 - accuracy: 0.96 - ETA: 5s - loss: 0.1033 - accuracy: 0.96 - ETA: 5s - loss: 0.1033 - accuracy: 0.96 - ETA: 5s - loss: 0.1033 - accuracy: 0.96 - ETA: 5s - loss: 0.1037 - accuracy: 0.96 - ETA: 5s - loss: 0.1039 - accuracy: 0.96 - ETA: 5s - loss: 0.1040 - accuracy: 0.96 - ETA: 5s - loss: 0.1036 - accuracy: 0.96 - ETA: 5s - loss: 0.1043 - accuracy: 0.96 - ETA: 5s - loss: 0.1039 - accuracy: 0.96 - ETA: 5s - loss: 0.1039 - accuracy: 0.96 - ETA: 5s - loss: 0.1036 - accuracy: 0.96 - ETA: 5s - loss: 0.1036 - accuracy: 0.96 - ETA: 4s - loss: 0.1037 - accuracy: 0.9632781/781 [==============================] - ETA: 4s - loss: 0.1036 - accuracy: 0.96 - ETA: 4s - loss: 0.1035 - accuracy: 0.96 - ETA: 4s - loss: 0.1034 - accuracy: 0.96 - ETA: 4s - loss: 0.1034 - accuracy: 0.96 - ETA: 4s - loss: 0.1031 - accuracy: 0.96 - ETA: 4s - loss: 0.1034 - accuracy: 0.96 - ETA: 4s - loss: 0.1032 - accuracy: 0.96 - ETA: 4s - loss: 0.1031 - accuracy: 0.96 - ETA: 4s - loss: 0.1033 - accuracy: 0.96 - ETA: 4s - loss: 0.1039 - accuracy: 0.96 - ETA: 4s - loss: 0.1036 - accuracy: 0.96 - ETA: 4s - loss: 0.1038 - accuracy: 0.96 - ETA: 4s - loss: 0.1038 - accuracy: 0.96 - ETA: 4s - loss: 0.1040 - accuracy: 0.96 - ETA: 3s - loss: 0.1039 - accuracy: 0.96 - ETA: 3s - loss: 0.1037 - accuracy: 0.96 - ETA: 3s - loss: 0.1036 - accuracy: 0.96 - ETA: 3s - loss: 0.1036 - accuracy: 0.96 - ETA: 3s - loss: 0.1040 - accuracy: 0.96 - ETA: 3s - loss: 0.1038 - accuracy: 0.96 - ETA: 3s - loss: 0.1036 - accuracy: 0.96 - ETA: 3s - loss: 0.1036 - accuracy: 0.96 - ETA: 3s - loss: 0.1040 - accuracy: 0.96 - ETA: 3s - loss: 0.1036 - accuracy: 0.96 - ETA: 3s - loss: 0.1033 - accuracy: 0.96 - ETA: 3s - loss: 0.1033 - accuracy: 0.96 - ETA: 3s - loss: 0.1034 - accuracy: 0.96 - ETA: 3s - loss: 0.1034 - accuracy: 0.96 - ETA: 3s - loss: 0.1033 - accuracy: 0.96 - ETA: 2s - loss: 0.1033 - accuracy: 0.96 - ETA: 2s - loss: 0.1030 - accuracy: 0.96 - ETA: 2s - loss: 0.1029 - accuracy: 0.96 - ETA: 2s - loss: 0.1028 - accuracy: 0.96 - ETA: 2s - loss: 0.1027 - accuracy: 0.96 - ETA: 2s - loss: 0.1024 - accuracy: 0.96 - ETA: 2s - loss: 0.1021 - accuracy: 0.96 - ETA: 2s - loss: 0.1018 - accuracy: 0.96 - ETA: 2s - loss: 0.1016 - accuracy: 0.96 - ETA: 2s - loss: 0.1016 - accuracy: 0.96 - ETA: 2s - loss: 0.1012 - accuracy: 0.96 - ETA: 2s - loss: 0.1011 - accuracy: 0.96 - ETA: 2s - loss: 0.1008 - accuracy: 0.96 - ETA: 2s - loss: 0.1006 - accuracy: 0.96 - ETA: 2s - loss: 0.1008 - accuracy: 0.96 - ETA: 1s - loss: 0.1009 - accuracy: 0.96 - ETA: 1s - loss: 0.1009 - accuracy: 0.96 - ETA: 1s - loss: 0.1009 - accuracy: 0.96 - ETA: 1s - loss: 0.1011 - accuracy: 0.96 - ETA: 1s - loss: 0.1012 - accuracy: 0.96 - ETA: 1s - loss: 0.1009 - accuracy: 0.96 - ETA: 1s - loss: 0.1008 - accuracy: 0.96 - ETA: 1s - loss: 0.1008 - accuracy: 0.96 - ETA: 1s - loss: 0.1005 - accuracy: 0.96 - ETA: 1s - loss: 0.1005 - accuracy: 0.96 - ETA: 1s - loss: 0.1006 - accuracy: 0.96 - ETA: 1s - loss: 0.1005 - accuracy: 0.96 - ETA: 1s - loss: 0.1001 - accuracy: 0.96 - ETA: 1s - loss: 0.0999 - accuracy: 0.96 - ETA: 0s - loss: 0.0997 - accuracy: 0.96 - ETA: 0s - loss: 0.0999 - accuracy: 0.96 - ETA: 0s - loss: 0.0998 - accuracy: 0.96 - ETA: 0s - loss: 0.0995 - accuracy: 0.96 - ETA: 0s - loss: 0.0993 - accuracy: 0.96 - ETA: 0s - loss: 0.0995 - accuracy: 0.96 - ETA: 0s - loss: 0.0993 - accuracy: 0.96 - ETA: 0s - loss: 0.0992 - accuracy: 0.96 - ETA: 0s - loss: 0.0991 - accuracy: 0.96 - ETA: 0s - loss: 0.0991 - accuracy: 0.96 - ETA: 0s - loss: 0.0991 - accuracy: 0.96 - ETA: 0s - loss: 0.0994 - accuracy: 0.96 - ETA: 0s - loss: 0.0995 - accuracy: 0.96 - ETA: 0s - loss: 0.0994 - accuracy: 0.96 - ETA: 0s - loss: 0.0997 - accuracy: 0.96 - 18s 23ms/step - loss: 0.0996 - accuracy: 0.9657\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)\n",
    "np.save('history/sentiment_analysis.npy', history.history)\n",
    "model.save(\"models/sentiment_analysis.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies = {'accuracy': accuracy}\n",
    "model = keras.models.load_model('models/sentiment_analysis.h5')\n",
    "history = np.load('history/sentiment_analysis.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using manual masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "embed_size = 128\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 781 steps\n",
      "Epoch 1/5\n",
      "535/781 [===================>..........] - ETA: 1:15:07 - loss: 0.6919 - accuracy: 0.531 - ETA: 25:12 - loss: 0.6956 - accuracy: 0.4479  - ETA: 15:13 - loss: 0.6948 - accuracy: 0.462 - ETA: 10:56 - loss: 0.6942 - accuracy: 0.482 - ETA: 7:43 - loss: 0.6940 - accuracy: 0.475 - ETA: 5:59 - loss: 0.6941 - accuracy: 0.47 - ETA: 4:54 - loss: 0.6937 - accuracy: 0.48 - ETA: 4:09 - loss: 0.6932 - accuracy: 0.50 - ETA: 3:37 - loss: 0.6950 - accuracy: 0.48 - ETA: 3:12 - loss: 0.6955 - accuracy: 0.48 - ETA: 2:52 - loss: 0.6948 - accuracy: 0.49 - ETA: 2:37 - loss: 0.6948 - accuracy: 0.48 - ETA: 2:24 - loss: 0.6943 - accuracy: 0.49 - ETA: 2:13 - loss: 0.6944 - accuracy: 0.49 - ETA: 2:07 - loss: 0.6947 - accuracy: 0.48 - ETA: 1:58 - loss: 0.6944 - accuracy: 0.49 - ETA: 1:51 - loss: 0.6943 - accuracy: 0.49 - ETA: 1:45 - loss: 0.6936 - accuracy: 0.49 - ETA: 1:39 - loss: 0.6933 - accuracy: 0.49 - ETA: 1:34 - loss: 0.6931 - accuracy: 0.50 - ETA: 1:30 - loss: 0.6934 - accuracy: 0.49 - ETA: 1:25 - loss: 0.6928 - accuracy: 0.50 - ETA: 1:22 - loss: 0.6919 - accuracy: 0.50 - ETA: 1:18 - loss: 0.6911 - accuracy: 0.51 - ETA: 1:15 - loss: 0.6927 - accuracy: 0.50 - ETA: 1:13 - loss: 0.6922 - accuracy: 0.51 - ETA: 1:10 - loss: 0.6906 - accuracy: 0.51 - ETA: 1:08 - loss: 0.6905 - accuracy: 0.51 - ETA: 1:05 - loss: 0.6905 - accuracy: 0.51 - ETA: 1:03 - loss: 0.6901 - accuracy: 0.52 - ETA: 1:01 - loss: 0.6893 - accuracy: 0.52 - ETA: 1:00 - loss: 0.6882 - accuracy: 0.52 - ETA: 58s - loss: 0.6867 - accuracy: 0.5299 - ETA: 57s - loss: 0.6857 - accuracy: 0.533 - ETA: 55s - loss: 0.6832 - accuracy: 0.536 - ETA: 54s - loss: 0.6797 - accuracy: 0.541 - ETA: 52s - loss: 0.6799 - accuracy: 0.544 - ETA: 51s - loss: 0.6787 - accuracy: 0.547 - ETA: 50s - loss: 0.6776 - accuracy: 0.548 - ETA: 49s - loss: 0.6773 - accuracy: 0.550 - ETA: 48s - loss: 0.6765 - accuracy: 0.552 - ETA: 47s - loss: 0.6744 - accuracy: 0.557 - ETA: 46s - loss: 0.6718 - accuracy: 0.560 - ETA: 45s - loss: 0.6713 - accuracy: 0.563 - ETA: 44s - loss: 0.6690 - accuracy: 0.567 - ETA: 43s - loss: 0.6680 - accuracy: 0.569 - ETA: 42s - loss: 0.6668 - accuracy: 0.572 - ETA: 41s - loss: 0.6656 - accuracy: 0.575 - ETA: 40s - loss: 0.6637 - accuracy: 0.578 - ETA: 40s - loss: 0.6619 - accuracy: 0.580 - ETA: 39s - loss: 0.6613 - accuracy: 0.581 - ETA: 38s - loss: 0.6601 - accuracy: 0.583 - ETA: 38s - loss: 0.6575 - accuracy: 0.587 - ETA: 37s - loss: 0.6570 - accuracy: 0.587 - ETA: 37s - loss: 0.6559 - accuracy: 0.589 - ETA: 36s - loss: 0.6540 - accuracy: 0.592 - ETA: 35s - loss: 0.6510 - accuracy: 0.596 - ETA: 35s - loss: 0.6500 - accuracy: 0.598 - ETA: 34s - loss: 0.6503 - accuracy: 0.600 - ETA: 34s - loss: 0.6492 - accuracy: 0.602 - ETA: 33s - loss: 0.6480 - accuracy: 0.604 - ETA: 33s - loss: 0.6468 - accuracy: 0.605 - ETA: 32s - loss: 0.6453 - accuracy: 0.607 - ETA: 32s - loss: 0.6434 - accuracy: 0.610 - ETA: 31s - loss: 0.6414 - accuracy: 0.612 - ETA: 31s - loss: 0.6408 - accuracy: 0.612 - ETA: 30s - loss: 0.6403 - accuracy: 0.614 - ETA: 30s - loss: 0.6389 - accuracy: 0.615 - ETA: 30s - loss: 0.6363 - accuracy: 0.618 - ETA: 29s - loss: 0.6357 - accuracy: 0.619 - ETA: 29s - loss: 0.6352 - accuracy: 0.619 - ETA: 28s - loss: 0.6331 - accuracy: 0.622 - ETA: 28s - loss: 0.6317 - accuracy: 0.623 - ETA: 28s - loss: 0.6312 - accuracy: 0.625 - ETA: 27s - loss: 0.6304 - accuracy: 0.626 - ETA: 27s - loss: 0.6296 - accuracy: 0.627 - ETA: 27s - loss: 0.6276 - accuracy: 0.629 - ETA: 26s - loss: 0.6262 - accuracy: 0.631 - ETA: 26s - loss: 0.6252 - accuracy: 0.633 - ETA: 26s - loss: 0.6246 - accuracy: 0.633 - ETA: 25s - loss: 0.6228 - accuracy: 0.635 - ETA: 25s - loss: 0.6214 - accuracy: 0.637 - ETA: 25s - loss: 0.6201 - accuracy: 0.638 - ETA: 24s - loss: 0.6187 - accuracy: 0.640 - ETA: 24s - loss: 0.6181 - accuracy: 0.641 - ETA: 24s - loss: 0.6164 - accuracy: 0.643 - ETA: 24s - loss: 0.6146 - accuracy: 0.645 - ETA: 23s - loss: 0.6130 - accuracy: 0.646 - ETA: 23s - loss: 0.6118 - accuracy: 0.648 - ETA: 23s - loss: 0.6110 - accuracy: 0.649 - ETA: 23s - loss: 0.6097 - accuracy: 0.650 - ETA: 22s - loss: 0.6079 - accuracy: 0.652 - ETA: 22s - loss: 0.6073 - accuracy: 0.653 - ETA: 22s - loss: 0.6062 - accuracy: 0.654 - ETA: 21s - loss: 0.6062 - accuracy: 0.654 - ETA: 21s - loss: 0.6043 - accuracy: 0.656 - ETA: 21s - loss: 0.6042 - accuracy: 0.657 - ETA: 21s - loss: 0.6028 - accuracy: 0.658 - ETA: 21s - loss: 0.6025 - accuracy: 0.658 - ETA: 20s - loss: 0.6025 - accuracy: 0.658 - ETA: 20s - loss: 0.6014 - accuracy: 0.660 - ETA: 20s - loss: 0.5999 - accuracy: 0.661 - ETA: 20s - loss: 0.5988 - accuracy: 0.663 - ETA: 19s - loss: 0.5988 - accuracy: 0.663 - ETA: 19s - loss: 0.5977 - accuracy: 0.665 - ETA: 19s - loss: 0.5965 - accuracy: 0.665 - ETA: 19s - loss: 0.5958 - accuracy: 0.666 - ETA: 19s - loss: 0.5954 - accuracy: 0.667 - ETA: 18s - loss: 0.5948 - accuracy: 0.668 - ETA: 18s - loss: 0.5943 - accuracy: 0.668 - ETA: 18s - loss: 0.5936 - accuracy: 0.668 - ETA: 18s - loss: 0.5932 - accuracy: 0.669 - ETA: 18s - loss: 0.5922 - accuracy: 0.670 - ETA: 18s - loss: 0.5911 - accuracy: 0.671 - ETA: 17s - loss: 0.5904 - accuracy: 0.672 - ETA: 17s - loss: 0.5898 - accuracy: 0.672 - ETA: 17s - loss: 0.5884 - accuracy: 0.674 - ETA: 17s - loss: 0.5874 - accuracy: 0.675 - ETA: 17s - loss: 0.5877 - accuracy: 0.675 - ETA: 17s - loss: 0.5870 - accuracy: 0.675 - ETA: 16s - loss: 0.5865 - accuracy: 0.676 - ETA: 16s - loss: 0.5856 - accuracy: 0.677 - ETA: 16s - loss: 0.5854 - accuracy: 0.677 - ETA: 16s - loss: 0.5856 - accuracy: 0.677 - ETA: 16s - loss: 0.5843 - accuracy: 0.678 - ETA: 16s - loss: 0.5834 - accuracy: 0.679 - ETA: 15s - loss: 0.5826 - accuracy: 0.680 - ETA: 15s - loss: 0.5808 - accuracy: 0.681 - ETA: 15s - loss: 0.5806 - accuracy: 0.681 - ETA: 15s - loss: 0.5805 - accuracy: 0.682 - ETA: 15s - loss: 0.5801 - accuracy: 0.682 - ETA: 15s - loss: 0.5797 - accuracy: 0.683 - ETA: 14s - loss: 0.5791 - accuracy: 0.684 - ETA: 14s - loss: 0.5785 - accuracy: 0.684 - ETA: 14s - loss: 0.5775 - accuracy: 0.685 - ETA: 14s - loss: 0.5768 - accuracy: 0.686 - ETA: 14s - loss: 0.5770 - accuracy: 0.686 - ETA: 14s - loss: 0.5762 - accuracy: 0.687 - ETA: 14s - loss: 0.5756 - accuracy: 0.687 - ETA: 13s - loss: 0.5754 - accuracy: 0.688 - ETA: 13s - loss: 0.5748 - accuracy: 0.688 - ETA: 13s - loss: 0.5747 - accuracy: 0.689 - ETA: 13s - loss: 0.5739 - accuracy: 0.689 - ETA: 13s - loss: 0.5738 - accuracy: 0.689 - ETA: 13s - loss: 0.5729 - accuracy: 0.690 - ETA: 13s - loss: 0.5724 - accuracy: 0.691 - ETA: 13s - loss: 0.5723 - accuracy: 0.691 - ETA: 12s - loss: 0.5714 - accuracy: 0.692 - ETA: 12s - loss: 0.5705 - accuracy: 0.693 - ETA: 12s - loss: 0.5700 - accuracy: 0.694 - ETA: 12s - loss: 0.5689 - accuracy: 0.695 - ETA: 12s - loss: 0.5681 - accuracy: 0.695 - ETA: 12s - loss: 0.5669 - accuracy: 0.696 - ETA: 12s - loss: 0.5674 - accuracy: 0.696 - ETA: 11s - loss: 0.5663 - accuracy: 0.697 - ETA: 11s - loss: 0.5656 - accuracy: 0.698 - ETA: 11s - loss: 0.5646 - accuracy: 0.699 - ETA: 11s - loss: 0.5638 - accuracy: 0.700 - ETA: 11s - loss: 0.5635 - accuracy: 0.700 - ETA: 11s - loss: 0.5623 - accuracy: 0.701 - ETA: 11s - loss: 0.5628 - accuracy: 0.701 - ETA: 10s - loss: 0.5628 - accuracy: 0.701 - ETA: 10s - loss: 0.5621 - accuracy: 0.702 - ETA: 10s - loss: 0.5620 - accuracy: 0.702 - ETA: 10s - loss: 0.5617 - accuracy: 0.703 - ETA: 10s - loss: 0.5612 - accuracy: 0.703 - ETA: 10s - loss: 0.5615 - accuracy: 0.703 - ETA: 10s - loss: 0.5605 - accuracy: 0.704 - ETA: 10s - loss: 0.5598 - accuracy: 0.704 - ETA: 9s - loss: 0.5584 - accuracy: 0.705 - ETA: 9s - loss: 0.5583 - accuracy: 0.70 - ETA: 9s - loss: 0.5572 - accuracy: 0.70 - ETA: 9s - loss: 0.5569 - accuracy: 0.70 - ETA: 9s - loss: 0.5570 - accuracy: 0.70 - ETA: 9s - loss: 0.5566 - accuracy: 0.70 - ETA: 9s - loss: 0.5563 - accuracy: 0.70 - ETA: 9s - loss: 0.5557 - accuracy: 0.70 - ETA: 8s - loss: 0.5555 - accuracy: 0.70 - ETA: 8s - loss: 0.5549 - accuracy: 0.70 - ETA: 8s - loss: 0.5542 - accuracy: 0.70 - ETA: 8s - loss: 0.5542 - accuracy: 0.71 - ETA: 8s - loss: 0.5541 - accuracy: 0.71 - ETA: 8s - loss: 0.5535 - accuracy: 0.71 - ETA: 8s - loss: 0.5534 - accuracy: 0.71 - ETA: 8s - loss: 0.5528 - accuracy: 0.71781/781 [==============================] - ETA: 8s - loss: 0.5529 - accuracy: 0.71 - ETA: 7s - loss: 0.5527 - accuracy: 0.71 - ETA: 7s - loss: 0.5521 - accuracy: 0.71 - ETA: 7s - loss: 0.5519 - accuracy: 0.71 - ETA: 7s - loss: 0.5514 - accuracy: 0.71 - ETA: 7s - loss: 0.5508 - accuracy: 0.71 - ETA: 7s - loss: 0.5504 - accuracy: 0.71 - ETA: 7s - loss: 0.5500 - accuracy: 0.71 - ETA: 7s - loss: 0.5498 - accuracy: 0.71 - ETA: 6s - loss: 0.5496 - accuracy: 0.71 - ETA: 6s - loss: 0.5490 - accuracy: 0.71 - ETA: 6s - loss: 0.5487 - accuracy: 0.71 - ETA: 6s - loss: 0.5481 - accuracy: 0.71 - ETA: 6s - loss: 0.5476 - accuracy: 0.71 - ETA: 6s - loss: 0.5476 - accuracy: 0.71 - ETA: 6s - loss: 0.5476 - accuracy: 0.71 - ETA: 6s - loss: 0.5475 - accuracy: 0.71 - ETA: 6s - loss: 0.5475 - accuracy: 0.71 - ETA: 6s - loss: 0.5471 - accuracy: 0.71 - ETA: 5s - loss: 0.5467 - accuracy: 0.71 - ETA: 5s - loss: 0.5461 - accuracy: 0.71 - ETA: 5s - loss: 0.5455 - accuracy: 0.71 - ETA: 5s - loss: 0.5457 - accuracy: 0.71 - ETA: 5s - loss: 0.5457 - accuracy: 0.71 - ETA: 5s - loss: 0.5456 - accuracy: 0.71 - ETA: 5s - loss: 0.5453 - accuracy: 0.71 - ETA: 5s - loss: 0.5455 - accuracy: 0.71 - ETA: 5s - loss: 0.5451 - accuracy: 0.71 - ETA: 5s - loss: 0.5447 - accuracy: 0.71 - ETA: 4s - loss: 0.5447 - accuracy: 0.71 - ETA: 4s - loss: 0.5440 - accuracy: 0.71 - ETA: 4s - loss: 0.5437 - accuracy: 0.71 - ETA: 4s - loss: 0.5433 - accuracy: 0.71 - ETA: 4s - loss: 0.5428 - accuracy: 0.71 - ETA: 4s - loss: 0.5427 - accuracy: 0.71 - ETA: 4s - loss: 0.5423 - accuracy: 0.71 - ETA: 4s - loss: 0.5420 - accuracy: 0.72 - ETA: 4s - loss: 0.5418 - accuracy: 0.72 - ETA: 4s - loss: 0.5415 - accuracy: 0.72 - ETA: 3s - loss: 0.5408 - accuracy: 0.72 - ETA: 3s - loss: 0.5410 - accuracy: 0.72 - ETA: 3s - loss: 0.5403 - accuracy: 0.72 - ETA: 3s - loss: 0.5401 - accuracy: 0.72 - ETA: 3s - loss: 0.5399 - accuracy: 0.72 - ETA: 3s - loss: 0.5394 - accuracy: 0.72 - ETA: 3s - loss: 0.5392 - accuracy: 0.72 - ETA: 3s - loss: 0.5390 - accuracy: 0.72 - ETA: 3s - loss: 0.5387 - accuracy: 0.72 - ETA: 3s - loss: 0.5386 - accuracy: 0.72 - ETA: 2s - loss: 0.5386 - accuracy: 0.72 - ETA: 2s - loss: 0.5384 - accuracy: 0.72 - ETA: 2s - loss: 0.5383 - accuracy: 0.72 - ETA: 2s - loss: 0.5380 - accuracy: 0.72 - ETA: 2s - loss: 0.5378 - accuracy: 0.72 - ETA: 2s - loss: 0.5378 - accuracy: 0.72 - ETA: 2s - loss: 0.5375 - accuracy: 0.72 - ETA: 2s - loss: 0.5375 - accuracy: 0.72 - ETA: 2s - loss: 0.5373 - accuracy: 0.72 - ETA: 2s - loss: 0.5369 - accuracy: 0.72 - ETA: 2s - loss: 0.5362 - accuracy: 0.72 - ETA: 1s - loss: 0.5357 - accuracy: 0.72 - ETA: 1s - loss: 0.5359 - accuracy: 0.72 - ETA: 1s - loss: 0.5352 - accuracy: 0.72 - ETA: 1s - loss: 0.5351 - accuracy: 0.72 - ETA: 1s - loss: 0.5346 - accuracy: 0.72 - ETA: 1s - loss: 0.5345 - accuracy: 0.72 - ETA: 1s - loss: 0.5343 - accuracy: 0.72 - ETA: 1s - loss: 0.5344 - accuracy: 0.72 - ETA: 1s - loss: 0.5338 - accuracy: 0.72 - ETA: 1s - loss: 0.5335 - accuracy: 0.72 - ETA: 1s - loss: 0.5329 - accuracy: 0.72 - ETA: 0s - loss: 0.5322 - accuracy: 0.72 - ETA: 0s - loss: 0.5320 - accuracy: 0.72 - ETA: 0s - loss: 0.5323 - accuracy: 0.72 - ETA: 0s - loss: 0.5325 - accuracy: 0.72 - ETA: 0s - loss: 0.5322 - accuracy: 0.72 - ETA: 0s - loss: 0.5319 - accuracy: 0.72 - ETA: 0s - loss: 0.5317 - accuracy: 0.72 - ETA: 0s - loss: 0.5316 - accuracy: 0.72 - ETA: 0s - loss: 0.5314 - accuracy: 0.72 - ETA: 0s - loss: 0.5315 - accuracy: 0.72 - ETA: 0s - loss: 0.5316 - accuracy: 0.72 - 23s 30ms/step - loss: 0.5316 - accuracy: 0.7291\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558/781 [====================>.........] - ETA: 21s - loss: 0.6260 - accuracy: 0.656 - ETA: 19s - loss: 0.4490 - accuracy: 0.804 - ETA: 19s - loss: 0.4358 - accuracy: 0.807 - ETA: 19s - loss: 0.4381 - accuracy: 0.802 - ETA: 19s - loss: 0.4471 - accuracy: 0.792 - ETA: 19s - loss: 0.4691 - accuracy: 0.778 - ETA: 19s - loss: 0.4847 - accuracy: 0.769 - ETA: 18s - loss: 0.4757 - accuracy: 0.781 - ETA: 18s - loss: 0.4770 - accuracy: 0.777 - ETA: 18s - loss: 0.4725 - accuracy: 0.780 - ETA: 17s - loss: 0.4721 - accuracy: 0.781 - ETA: 17s - loss: 0.4743 - accuracy: 0.781 - ETA: 17s - loss: 0.4726 - accuracy: 0.783 - ETA: 17s - loss: 0.4653 - accuracy: 0.785 - ETA: 17s - loss: 0.4617 - accuracy: 0.790 - ETA: 16s - loss: 0.4664 - accuracy: 0.791 - ETA: 16s - loss: 0.4606 - accuracy: 0.792 - ETA: 16s - loss: 0.4597 - accuracy: 0.793 - ETA: 16s - loss: 0.4567 - accuracy: 0.795 - ETA: 16s - loss: 0.4529 - accuracy: 0.796 - ETA: 16s - loss: 0.4576 - accuracy: 0.794 - ETA: 16s - loss: 0.4524 - accuracy: 0.796 - ETA: 16s - loss: 0.4506 - accuracy: 0.794 - ETA: 16s - loss: 0.4491 - accuracy: 0.795 - ETA: 16s - loss: 0.4465 - accuracy: 0.796 - ETA: 16s - loss: 0.4480 - accuracy: 0.795 - ETA: 16s - loss: 0.4441 - accuracy: 0.796 - ETA: 16s - loss: 0.4390 - accuracy: 0.799 - ETA: 16s - loss: 0.4358 - accuracy: 0.800 - ETA: 16s - loss: 0.4355 - accuracy: 0.803 - ETA: 15s - loss: 0.4328 - accuracy: 0.804 - ETA: 15s - loss: 0.4303 - accuracy: 0.805 - ETA: 15s - loss: 0.4281 - accuracy: 0.808 - ETA: 15s - loss: 0.4218 - accuracy: 0.810 - ETA: 15s - loss: 0.4159 - accuracy: 0.814 - ETA: 15s - loss: 0.4102 - accuracy: 0.816 - ETA: 15s - loss: 0.4037 - accuracy: 0.819 - ETA: 14s - loss: 0.3997 - accuracy: 0.822 - ETA: 14s - loss: 0.4009 - accuracy: 0.822 - ETA: 14s - loss: 0.4015 - accuracy: 0.821 - ETA: 14s - loss: 0.4059 - accuracy: 0.819 - ETA: 14s - loss: 0.4083 - accuracy: 0.817 - ETA: 14s - loss: 0.4074 - accuracy: 0.817 - ETA: 14s - loss: 0.4056 - accuracy: 0.817 - ETA: 14s - loss: 0.4042 - accuracy: 0.818 - ETA: 14s - loss: 0.4025 - accuracy: 0.819 - ETA: 14s - loss: 0.4034 - accuracy: 0.818 - ETA: 13s - loss: 0.4039 - accuracy: 0.818 - ETA: 13s - loss: 0.4030 - accuracy: 0.819 - ETA: 13s - loss: 0.4038 - accuracy: 0.819 - ETA: 13s - loss: 0.4031 - accuracy: 0.819 - ETA: 13s - loss: 0.4008 - accuracy: 0.820 - ETA: 13s - loss: 0.3991 - accuracy: 0.821 - ETA: 13s - loss: 0.3979 - accuracy: 0.822 - ETA: 13s - loss: 0.3960 - accuracy: 0.823 - ETA: 13s - loss: 0.3952 - accuracy: 0.823 - ETA: 13s - loss: 0.3921 - accuracy: 0.825 - ETA: 13s - loss: 0.3910 - accuracy: 0.825 - ETA: 13s - loss: 0.3915 - accuracy: 0.826 - ETA: 13s - loss: 0.3909 - accuracy: 0.827 - ETA: 13s - loss: 0.3893 - accuracy: 0.828 - ETA: 12s - loss: 0.3884 - accuracy: 0.828 - ETA: 12s - loss: 0.3866 - accuracy: 0.830 - ETA: 12s - loss: 0.3845 - accuracy: 0.831 - ETA: 12s - loss: 0.3836 - accuracy: 0.831 - ETA: 12s - loss: 0.3830 - accuracy: 0.831 - ETA: 12s - loss: 0.3831 - accuracy: 0.831 - ETA: 12s - loss: 0.3808 - accuracy: 0.832 - ETA: 12s - loss: 0.3797 - accuracy: 0.832 - ETA: 12s - loss: 0.3793 - accuracy: 0.833 - ETA: 12s - loss: 0.3794 - accuracy: 0.833 - ETA: 12s - loss: 0.3791 - accuracy: 0.833 - ETA: 12s - loss: 0.3783 - accuracy: 0.834 - ETA: 12s - loss: 0.3788 - accuracy: 0.833 - ETA: 12s - loss: 0.3800 - accuracy: 0.833 - ETA: 12s - loss: 0.3800 - accuracy: 0.833 - ETA: 11s - loss: 0.3786 - accuracy: 0.834 - ETA: 11s - loss: 0.3777 - accuracy: 0.834 - ETA: 11s - loss: 0.3765 - accuracy: 0.835 - ETA: 11s - loss: 0.3745 - accuracy: 0.835 - ETA: 11s - loss: 0.3747 - accuracy: 0.836 - ETA: 11s - loss: 0.3742 - accuracy: 0.836 - ETA: 11s - loss: 0.3730 - accuracy: 0.837 - ETA: 11s - loss: 0.3727 - accuracy: 0.837 - ETA: 11s - loss: 0.3720 - accuracy: 0.837 - ETA: 11s - loss: 0.3711 - accuracy: 0.838 - ETA: 11s - loss: 0.3692 - accuracy: 0.838 - ETA: 11s - loss: 0.3687 - accuracy: 0.839 - ETA: 11s - loss: 0.3686 - accuracy: 0.839 - ETA: 11s - loss: 0.3683 - accuracy: 0.839 - ETA: 11s - loss: 0.3671 - accuracy: 0.840 - ETA: 11s - loss: 0.3660 - accuracy: 0.841 - ETA: 10s - loss: 0.3655 - accuracy: 0.841 - ETA: 10s - loss: 0.3657 - accuracy: 0.841 - ETA: 10s - loss: 0.3654 - accuracy: 0.841 - ETA: 10s - loss: 0.3652 - accuracy: 0.841 - ETA: 10s - loss: 0.3644 - accuracy: 0.842 - ETA: 10s - loss: 0.3644 - accuracy: 0.842 - ETA: 10s - loss: 0.3646 - accuracy: 0.842 - ETA: 10s - loss: 0.3642 - accuracy: 0.842 - ETA: 10s - loss: 0.3633 - accuracy: 0.842 - ETA: 10s - loss: 0.3620 - accuracy: 0.843 - ETA: 10s - loss: 0.3629 - accuracy: 0.843 - ETA: 10s - loss: 0.3623 - accuracy: 0.844 - ETA: 10s - loss: 0.3620 - accuracy: 0.844 - ETA: 10s - loss: 0.3623 - accuracy: 0.844 - ETA: 10s - loss: 0.3628 - accuracy: 0.843 - ETA: 10s - loss: 0.3634 - accuracy: 0.843 - ETA: 9s - loss: 0.3633 - accuracy: 0.843 - ETA: 9s - loss: 0.3633 - accuracy: 0.84 - ETA: 9s - loss: 0.3633 - accuracy: 0.84 - ETA: 9s - loss: 0.3623 - accuracy: 0.84 - ETA: 9s - loss: 0.3633 - accuracy: 0.84 - ETA: 9s - loss: 0.3631 - accuracy: 0.84 - ETA: 9s - loss: 0.3630 - accuracy: 0.84 - ETA: 9s - loss: 0.3627 - accuracy: 0.84 - ETA: 9s - loss: 0.3632 - accuracy: 0.84 - ETA: 9s - loss: 0.3635 - accuracy: 0.84 - ETA: 9s - loss: 0.3630 - accuracy: 0.84 - ETA: 9s - loss: 0.3627 - accuracy: 0.84 - ETA: 9s - loss: 0.3634 - accuracy: 0.84 - ETA: 9s - loss: 0.3632 - accuracy: 0.84 - ETA: 9s - loss: 0.3625 - accuracy: 0.84 - ETA: 9s - loss: 0.3618 - accuracy: 0.84 - ETA: 8s - loss: 0.3614 - accuracy: 0.84 - ETA: 8s - loss: 0.3599 - accuracy: 0.84 - ETA: 8s - loss: 0.3603 - accuracy: 0.84 - ETA: 8s - loss: 0.3602 - accuracy: 0.84 - ETA: 8s - loss: 0.3610 - accuracy: 0.84 - ETA: 8s - loss: 0.3613 - accuracy: 0.84 - ETA: 8s - loss: 0.3608 - accuracy: 0.84 - ETA: 8s - loss: 0.3604 - accuracy: 0.84 - ETA: 8s - loss: 0.3602 - accuracy: 0.84 - ETA: 8s - loss: 0.3607 - accuracy: 0.84 - ETA: 8s - loss: 0.3606 - accuracy: 0.84 - ETA: 8s - loss: 0.3603 - accuracy: 0.84 - ETA: 8s - loss: 0.3600 - accuracy: 0.84 - ETA: 8s - loss: 0.3599 - accuracy: 0.84 - ETA: 8s - loss: 0.3602 - accuracy: 0.84 - ETA: 8s - loss: 0.3598 - accuracy: 0.84 - ETA: 7s - loss: 0.3592 - accuracy: 0.84 - ETA: 7s - loss: 0.3591 - accuracy: 0.84 - ETA: 7s - loss: 0.3590 - accuracy: 0.84 - ETA: 7s - loss: 0.3585 - accuracy: 0.84 - ETA: 7s - loss: 0.3588 - accuracy: 0.84 - ETA: 7s - loss: 0.3589 - accuracy: 0.84 - ETA: 7s - loss: 0.3583 - accuracy: 0.84 - ETA: 7s - loss: 0.3578 - accuracy: 0.84 - ETA: 7s - loss: 0.3568 - accuracy: 0.84 - ETA: 7s - loss: 0.3558 - accuracy: 0.84 - ETA: 7s - loss: 0.3550 - accuracy: 0.85 - ETA: 7s - loss: 0.3552 - accuracy: 0.85 - ETA: 7s - loss: 0.3548 - accuracy: 0.85 - ETA: 7s - loss: 0.3540 - accuracy: 0.85 - ETA: 7s - loss: 0.3537 - accuracy: 0.85 - ETA: 7s - loss: 0.3532 - accuracy: 0.85 - ETA: 6s - loss: 0.3536 - accuracy: 0.85 - ETA: 6s - loss: 0.3539 - accuracy: 0.85 - ETA: 6s - loss: 0.3537 - accuracy: 0.85 - ETA: 6s - loss: 0.3536 - accuracy: 0.85 - ETA: 6s - loss: 0.3546 - accuracy: 0.85 - ETA: 6s - loss: 0.3546 - accuracy: 0.85 - ETA: 6s - loss: 0.3550 - accuracy: 0.85 - ETA: 6s - loss: 0.3554 - accuracy: 0.85 - ETA: 6s - loss: 0.3551 - accuracy: 0.85 - ETA: 6s - loss: 0.3547 - accuracy: 0.85 - ETA: 6s - loss: 0.3543 - accuracy: 0.85 - ETA: 6s - loss: 0.3540 - accuracy: 0.85 - ETA: 6s - loss: 0.3536 - accuracy: 0.85 - ETA: 6s - loss: 0.3546 - accuracy: 0.85 - ETA: 6s - loss: 0.3549 - accuracy: 0.85 - ETA: 5s - loss: 0.3546 - accuracy: 0.85 - ETA: 5s - loss: 0.3545 - accuracy: 0.85 - ETA: 5s - loss: 0.3542 - accuracy: 0.85 - ETA: 5s - loss: 0.3546 - accuracy: 0.85 - ETA: 5s - loss: 0.3544 - accuracy: 0.85 - ETA: 5s - loss: 0.3538 - accuracy: 0.85 - ETA: 5s - loss: 0.3540 - accuracy: 0.85 - ETA: 5s - loss: 0.3537 - accuracy: 0.85 - ETA: 5s - loss: 0.3537 - accuracy: 0.85 - ETA: 5s - loss: 0.3538 - accuracy: 0.85 - ETA: 5s - loss: 0.3537 - accuracy: 0.85 - ETA: 5s - loss: 0.3545 - accuracy: 0.85 - ETA: 5s - loss: 0.3547 - accuracy: 0.85 - ETA: 5s - loss: 0.3546 - accuracy: 0.85 - ETA: 5s - loss: 0.3546 - accuracy: 0.85 - ETA: 5s - loss: 0.3544 - accuracy: 0.85 - ETA: 4s - loss: 0.3540 - accuracy: 0.85 - ETA: 4s - loss: 0.3541 - accuracy: 0.8523"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 4s - loss: 0.3535 - accuracy: 0.85 - ETA: 4s - loss: 0.3536 - accuracy: 0.85 - ETA: 4s - loss: 0.3534 - accuracy: 0.85 - ETA: 4s - loss: 0.3527 - accuracy: 0.85 - ETA: 4s - loss: 0.3525 - accuracy: 0.85 - ETA: 4s - loss: 0.3518 - accuracy: 0.85 - ETA: 4s - loss: 0.3517 - accuracy: 0.85 - ETA: 4s - loss: 0.3518 - accuracy: 0.85 - ETA: 4s - loss: 0.3516 - accuracy: 0.85 - ETA: 4s - loss: 0.3523 - accuracy: 0.85 - ETA: 4s - loss: 0.3520 - accuracy: 0.85 - ETA: 4s - loss: 0.3521 - accuracy: 0.85 - ETA: 4s - loss: 0.3522 - accuracy: 0.85 - ETA: 3s - loss: 0.3519 - accuracy: 0.85 - ETA: 3s - loss: 0.3519 - accuracy: 0.85 - ETA: 3s - loss: 0.3515 - accuracy: 0.85 - ETA: 3s - loss: 0.3515 - accuracy: 0.85 - ETA: 3s - loss: 0.3517 - accuracy: 0.85 - ETA: 3s - loss: 0.3521 - accuracy: 0.85 - ETA: 3s - loss: 0.3524 - accuracy: 0.85 - ETA: 3s - loss: 0.3525 - accuracy: 0.85 - ETA: 3s - loss: 0.3527 - accuracy: 0.85 - ETA: 3s - loss: 0.3527 - accuracy: 0.85 - ETA: 3s - loss: 0.3527 - accuracy: 0.85 - ETA: 3s - loss: 0.3521 - accuracy: 0.85 - ETA: 3s - loss: 0.3515 - accuracy: 0.85 - ETA: 3s - loss: 0.3523 - accuracy: 0.85 - ETA: 3s - loss: 0.3521 - accuracy: 0.85 - ETA: 2s - loss: 0.3521 - accuracy: 0.85 - ETA: 2s - loss: 0.3520 - accuracy: 0.85 - ETA: 2s - loss: 0.3520 - accuracy: 0.85 - ETA: 2s - loss: 0.3516 - accuracy: 0.85 - ETA: 2s - loss: 0.3515 - accuracy: 0.85 - ETA: 2s - loss: 0.3511 - accuracy: 0.85 - ETA: 2s - loss: 0.3508 - accuracy: 0.85 - ETA: 2s - loss: 0.3506 - accuracy: 0.85 - ETA: 2s - loss: 0.3506 - accuracy: 0.85 - ETA: 2s - loss: 0.3504 - accuracy: 0.85 - ETA: 2s - loss: 0.3510 - accuracy: 0.85 - ETA: 2s - loss: 0.3507 - accuracy: 0.85 - ETA: 2s - loss: 0.3507 - accuracy: 0.85 - ETA: 2s - loss: 0.3509 - accuracy: 0.85 - ETA: 2s - loss: 0.3506 - accuracy: 0.85 - ETA: 1s - loss: 0.3509 - accuracy: 0.85 - ETA: 1s - loss: 0.3508 - accuracy: 0.85 - ETA: 1s - loss: 0.3505 - accuracy: 0.85 - ETA: 1s - loss: 0.3505 - accuracy: 0.85 - ETA: 1s - loss: 0.3506 - accuracy: 0.85 - ETA: 1s - loss: 0.3508 - accuracy: 0.85 - ETA: 1s - loss: 0.3511 - accuracy: 0.85 - ETA: 1s - loss: 0.3508 - accuracy: 0.85 - ETA: 1s - loss: 0.3505 - accuracy: 0.85 - ETA: 1s - loss: 0.3503 - accuracy: 0.85 - ETA: 1s - loss: 0.3501 - accuracy: 0.85 - ETA: 1s - loss: 0.3495 - accuracy: 0.85 - ETA: 1s - loss: 0.3497 - accuracy: 0.85 - ETA: 1s - loss: 0.3494 - accuracy: 0.85 - ETA: 1s - loss: 0.3493 - accuracy: 0.85 - ETA: 1s - loss: 0.3493 - accuracy: 0.85 - ETA: 0s - loss: 0.3497 - accuracy: 0.85 - ETA: 0s - loss: 0.3494 - accuracy: 0.85 - ETA: 0s - loss: 0.3492 - accuracy: 0.85 - ETA: 0s - loss: 0.3486 - accuracy: 0.85 - ETA: 0s - loss: 0.3485 - accuracy: 0.85 - ETA: 0s - loss: 0.3484 - accuracy: 0.85 - ETA: 0s - loss: 0.3483 - accuracy: 0.85 - ETA: 0s - loss: 0.3485 - accuracy: 0.85 - ETA: 0s - loss: 0.3487 - accuracy: 0.85 - ETA: 0s - loss: 0.3488 - accuracy: 0.85 - ETA: 0s - loss: 0.3487 - accuracy: 0.85 - ETA: 0s - loss: 0.3488 - accuracy: 0.85 - ETA: 0s - loss: 0.3486 - accuracy: 0.85 - ETA: 0s - loss: 0.3489 - accuracy: 0.85 - ETA: 0s - loss: 0.3491 - accuracy: 0.85 - 17s 22ms/step - loss: 0.3492 - accuracy: 0.8560\n",
      "Epoch 3/5\n",
      "565/781 [====================>.........] - ETA: 17s - loss: 0.3845 - accuracy: 0.843 - ETA: 17s - loss: 0.3115 - accuracy: 0.867 - ETA: 17s - loss: 0.2960 - accuracy: 0.875 - ETA: 17s - loss: 0.2890 - accuracy: 0.875 - ETA: 17s - loss: 0.3047 - accuracy: 0.872 - ETA: 17s - loss: 0.3217 - accuracy: 0.867 - ETA: 17s - loss: 0.3464 - accuracy: 0.855 - ETA: 17s - loss: 0.3468 - accuracy: 0.855 - ETA: 17s - loss: 0.3477 - accuracy: 0.853 - ETA: 17s - loss: 0.3411 - accuracy: 0.859 - ETA: 16s - loss: 0.3353 - accuracy: 0.861 - ETA: 16s - loss: 0.3321 - accuracy: 0.864 - ETA: 16s - loss: 0.3291 - accuracy: 0.867 - ETA: 16s - loss: 0.3250 - accuracy: 0.871 - ETA: 16s - loss: 0.3293 - accuracy: 0.870 - ETA: 16s - loss: 0.3205 - accuracy: 0.875 - ETA: 16s - loss: 0.3150 - accuracy: 0.877 - ETA: 16s - loss: 0.3115 - accuracy: 0.878 - ETA: 16s - loss: 0.3124 - accuracy: 0.877 - ETA: 16s - loss: 0.3090 - accuracy: 0.881 - ETA: 16s - loss: 0.3069 - accuracy: 0.882 - ETA: 16s - loss: 0.3052 - accuracy: 0.883 - ETA: 16s - loss: 0.3056 - accuracy: 0.883 - ETA: 15s - loss: 0.3039 - accuracy: 0.882 - ETA: 15s - loss: 0.3022 - accuracy: 0.882 - ETA: 15s - loss: 0.2997 - accuracy: 0.883 - ETA: 15s - loss: 0.2953 - accuracy: 0.886 - ETA: 15s - loss: 0.2927 - accuracy: 0.887 - ETA: 15s - loss: 0.2895 - accuracy: 0.889 - ETA: 15s - loss: 0.2832 - accuracy: 0.892 - ETA: 15s - loss: 0.2820 - accuracy: 0.891 - ETA: 15s - loss: 0.2786 - accuracy: 0.892 - ETA: 15s - loss: 0.2754 - accuracy: 0.894 - ETA: 15s - loss: 0.2700 - accuracy: 0.896 - ETA: 15s - loss: 0.2651 - accuracy: 0.898 - ETA: 15s - loss: 0.2594 - accuracy: 0.900 - ETA: 15s - loss: 0.2578 - accuracy: 0.901 - ETA: 14s - loss: 0.2584 - accuracy: 0.901 - ETA: 14s - loss: 0.2598 - accuracy: 0.901 - ETA: 14s - loss: 0.2628 - accuracy: 0.898 - ETA: 14s - loss: 0.2652 - accuracy: 0.897 - ETA: 14s - loss: 0.2677 - accuracy: 0.896 - ETA: 14s - loss: 0.2700 - accuracy: 0.894 - ETA: 14s - loss: 0.2703 - accuracy: 0.893 - ETA: 14s - loss: 0.2690 - accuracy: 0.894 - ETA: 14s - loss: 0.2693 - accuracy: 0.894 - ETA: 14s - loss: 0.2717 - accuracy: 0.893 - ETA: 14s - loss: 0.2730 - accuracy: 0.892 - ETA: 14s - loss: 0.2721 - accuracy: 0.892 - ETA: 14s - loss: 0.2717 - accuracy: 0.892 - ETA: 14s - loss: 0.2697 - accuracy: 0.893 - ETA: 14s - loss: 0.2678 - accuracy: 0.894 - ETA: 13s - loss: 0.2660 - accuracy: 0.895 - ETA: 13s - loss: 0.2646 - accuracy: 0.896 - ETA: 13s - loss: 0.2617 - accuracy: 0.898 - ETA: 13s - loss: 0.2601 - accuracy: 0.898 - ETA: 13s - loss: 0.2598 - accuracy: 0.898 - ETA: 13s - loss: 0.2581 - accuracy: 0.899 - ETA: 13s - loss: 0.2574 - accuracy: 0.900 - ETA: 13s - loss: 0.2555 - accuracy: 0.901 - ETA: 13s - loss: 0.2550 - accuracy: 0.901 - ETA: 13s - loss: 0.2531 - accuracy: 0.902 - ETA: 13s - loss: 0.2521 - accuracy: 0.902 - ETA: 13s - loss: 0.2502 - accuracy: 0.903 - ETA: 13s - loss: 0.2490 - accuracy: 0.903 - ETA: 13s - loss: 0.2483 - accuracy: 0.903 - ETA: 13s - loss: 0.2463 - accuracy: 0.904 - ETA: 13s - loss: 0.2449 - accuracy: 0.905 - ETA: 12s - loss: 0.2434 - accuracy: 0.906 - ETA: 12s - loss: 0.2415 - accuracy: 0.906 - ETA: 12s - loss: 0.2406 - accuracy: 0.907 - ETA: 12s - loss: 0.2398 - accuracy: 0.907 - ETA: 12s - loss: 0.2398 - accuracy: 0.907 - ETA: 12s - loss: 0.2385 - accuracy: 0.908 - ETA: 12s - loss: 0.2365 - accuracy: 0.909 - ETA: 12s - loss: 0.2373 - accuracy: 0.909 - ETA: 12s - loss: 0.2366 - accuracy: 0.909 - ETA: 12s - loss: 0.2349 - accuracy: 0.910 - ETA: 12s - loss: 0.2339 - accuracy: 0.910 - ETA: 12s - loss: 0.2324 - accuracy: 0.911 - ETA: 12s - loss: 0.2323 - accuracy: 0.911 - ETA: 12s - loss: 0.2315 - accuracy: 0.911 - ETA: 11s - loss: 0.2307 - accuracy: 0.912 - ETA: 11s - loss: 0.2292 - accuracy: 0.912 - ETA: 11s - loss: 0.2285 - accuracy: 0.913 - ETA: 11s - loss: 0.2283 - accuracy: 0.913 - ETA: 11s - loss: 0.2272 - accuracy: 0.913 - ETA: 11s - loss: 0.2266 - accuracy: 0.914 - ETA: 11s - loss: 0.2268 - accuracy: 0.914 - ETA: 11s - loss: 0.2256 - accuracy: 0.914 - ETA: 11s - loss: 0.2247 - accuracy: 0.915 - ETA: 11s - loss: 0.2240 - accuracy: 0.915 - ETA: 11s - loss: 0.2231 - accuracy: 0.916 - ETA: 11s - loss: 0.2223 - accuracy: 0.916 - ETA: 11s - loss: 0.2212 - accuracy: 0.917 - ETA: 11s - loss: 0.2208 - accuracy: 0.917 - ETA: 11s - loss: 0.2207 - accuracy: 0.917 - ETA: 10s - loss: 0.2198 - accuracy: 0.917 - ETA: 10s - loss: 0.2193 - accuracy: 0.918 - ETA: 10s - loss: 0.2180 - accuracy: 0.918 - ETA: 10s - loss: 0.2182 - accuracy: 0.918 - ETA: 10s - loss: 0.2177 - accuracy: 0.918 - ETA: 10s - loss: 0.2175 - accuracy: 0.918 - ETA: 10s - loss: 0.2174 - accuracy: 0.919 - ETA: 10s - loss: 0.2177 - accuracy: 0.918 - ETA: 10s - loss: 0.2185 - accuracy: 0.918 - ETA: 10s - loss: 0.2183 - accuracy: 0.918 - ETA: 10s - loss: 0.2184 - accuracy: 0.918 - ETA: 10s - loss: 0.2189 - accuracy: 0.918 - ETA: 10s - loss: 0.2183 - accuracy: 0.918 - ETA: 10s - loss: 0.2181 - accuracy: 0.918 - ETA: 10s - loss: 0.2173 - accuracy: 0.918 - ETA: 9s - loss: 0.2168 - accuracy: 0.919 - ETA: 9s - loss: 0.2164 - accuracy: 0.91 - ETA: 9s - loss: 0.2175 - accuracy: 0.91 - ETA: 9s - loss: 0.2173 - accuracy: 0.91 - ETA: 9s - loss: 0.2173 - accuracy: 0.91 - ETA: 9s - loss: 0.2169 - accuracy: 0.91 - ETA: 9s - loss: 0.2172 - accuracy: 0.91 - ETA: 9s - loss: 0.2171 - accuracy: 0.91 - ETA: 9s - loss: 0.2163 - accuracy: 0.91 - ETA: 9s - loss: 0.2155 - accuracy: 0.91 - ETA: 9s - loss: 0.2152 - accuracy: 0.92 - ETA: 9s - loss: 0.2140 - accuracy: 0.92 - ETA: 9s - loss: 0.2143 - accuracy: 0.92 - ETA: 9s - loss: 0.2144 - accuracy: 0.92 - ETA: 9s - loss: 0.2151 - accuracy: 0.92 - ETA: 8s - loss: 0.2150 - accuracy: 0.92 - ETA: 8s - loss: 0.2146 - accuracy: 0.92 - ETA: 8s - loss: 0.2140 - accuracy: 0.92 - ETA: 8s - loss: 0.2138 - accuracy: 0.92 - ETA: 8s - loss: 0.2141 - accuracy: 0.92 - ETA: 8s - loss: 0.2133 - accuracy: 0.92 - ETA: 8s - loss: 0.2127 - accuracy: 0.92 - ETA: 8s - loss: 0.2129 - accuracy: 0.92 - ETA: 8s - loss: 0.2129 - accuracy: 0.92 - ETA: 8s - loss: 0.2131 - accuracy: 0.92 - ETA: 8s - loss: 0.2129 - accuracy: 0.92 - ETA: 8s - loss: 0.2124 - accuracy: 0.92 - ETA: 8s - loss: 0.2121 - accuracy: 0.92 - ETA: 8s - loss: 0.2124 - accuracy: 0.92 - ETA: 8s - loss: 0.2119 - accuracy: 0.92 - ETA: 7s - loss: 0.2114 - accuracy: 0.92 - ETA: 7s - loss: 0.2112 - accuracy: 0.92 - ETA: 7s - loss: 0.2108 - accuracy: 0.92 - ETA: 7s - loss: 0.2099 - accuracy: 0.92 - ETA: 7s - loss: 0.2090 - accuracy: 0.92 - ETA: 7s - loss: 0.2082 - accuracy: 0.92 - ETA: 7s - loss: 0.2074 - accuracy: 0.92 - ETA: 7s - loss: 0.2076 - accuracy: 0.92 - ETA: 7s - loss: 0.2073 - accuracy: 0.92 - ETA: 7s - loss: 0.2071 - accuracy: 0.92 - ETA: 7s - loss: 0.2068 - accuracy: 0.92 - ETA: 7s - loss: 0.2062 - accuracy: 0.92 - ETA: 7s - loss: 0.2070 - accuracy: 0.92 - ETA: 7s - loss: 0.2073 - accuracy: 0.92 - ETA: 7s - loss: 0.2076 - accuracy: 0.92 - ETA: 6s - loss: 0.2078 - accuracy: 0.92 - ETA: 6s - loss: 0.2082 - accuracy: 0.92 - ETA: 6s - loss: 0.2081 - accuracy: 0.92 - ETA: 6s - loss: 0.2086 - accuracy: 0.92 - ETA: 6s - loss: 0.2086 - accuracy: 0.92 - ETA: 6s - loss: 0.2082 - accuracy: 0.92 - ETA: 6s - loss: 0.2079 - accuracy: 0.92 - ETA: 6s - loss: 0.2076 - accuracy: 0.92 - ETA: 6s - loss: 0.2071 - accuracy: 0.92 - ETA: 6s - loss: 0.2078 - accuracy: 0.92 - ETA: 6s - loss: 0.2078 - accuracy: 0.92 - ETA: 6s - loss: 0.2076 - accuracy: 0.92 - ETA: 6s - loss: 0.2075 - accuracy: 0.92 - ETA: 6s - loss: 0.2076 - accuracy: 0.92 - ETA: 6s - loss: 0.2078 - accuracy: 0.92 - ETA: 5s - loss: 0.2077 - accuracy: 0.92 - ETA: 5s - loss: 0.2076 - accuracy: 0.92 - ETA: 5s - loss: 0.2074 - accuracy: 0.92 - ETA: 5s - loss: 0.2069 - accuracy: 0.92 - ETA: 5s - loss: 0.2066 - accuracy: 0.92 - ETA: 5s - loss: 0.2067 - accuracy: 0.92 - ETA: 5s - loss: 0.2059 - accuracy: 0.92 - ETA: 5s - loss: 0.2060 - accuracy: 0.92 - ETA: 5s - loss: 0.2061 - accuracy: 0.92 - ETA: 5s - loss: 0.2058 - accuracy: 0.92 - ETA: 5s - loss: 0.2059 - accuracy: 0.92 - ETA: 5s - loss: 0.2058 - accuracy: 0.92 - ETA: 5s - loss: 0.2054 - accuracy: 0.92 - ETA: 5s - loss: 0.2053 - accuracy: 0.92 - ETA: 4s - loss: 0.2051 - accuracy: 0.92 - ETA: 4s - loss: 0.2048 - accuracy: 0.92 - ETA: 4s - loss: 0.2047 - accuracy: 0.9264781/781 [==============================] - ETA: 4s - loss: 0.2044 - accuracy: 0.92 - ETA: 4s - loss: 0.2041 - accuracy: 0.92 - ETA: 4s - loss: 0.2038 - accuracy: 0.92 - ETA: 4s - loss: 0.2030 - accuracy: 0.92 - ETA: 4s - loss: 0.2026 - accuracy: 0.92 - ETA: 4s - loss: 0.2029 - accuracy: 0.92 - ETA: 4s - loss: 0.2024 - accuracy: 0.92 - ETA: 4s - loss: 0.2022 - accuracy: 0.92 - ETA: 4s - loss: 0.2023 - accuracy: 0.92 - ETA: 4s - loss: 0.2019 - accuracy: 0.92 - ETA: 4s - loss: 0.2021 - accuracy: 0.92 - ETA: 4s - loss: 0.2017 - accuracy: 0.92 - ETA: 3s - loss: 0.2015 - accuracy: 0.92 - ETA: 3s - loss: 0.2016 - accuracy: 0.92 - ETA: 3s - loss: 0.2011 - accuracy: 0.92 - ETA: 3s - loss: 0.2011 - accuracy: 0.92 - ETA: 3s - loss: 0.2019 - accuracy: 0.92 - ETA: 3s - loss: 0.2020 - accuracy: 0.92 - ETA: 3s - loss: 0.2023 - accuracy: 0.92 - ETA: 3s - loss: 0.2024 - accuracy: 0.92 - ETA: 3s - loss: 0.2022 - accuracy: 0.92 - ETA: 3s - loss: 0.2022 - accuracy: 0.92 - ETA: 3s - loss: 0.2019 - accuracy: 0.92 - ETA: 3s - loss: 0.2018 - accuracy: 0.92 - ETA: 3s - loss: 0.2016 - accuracy: 0.92 - ETA: 3s - loss: 0.2011 - accuracy: 0.92 - ETA: 3s - loss: 0.2008 - accuracy: 0.92 - ETA: 2s - loss: 0.2009 - accuracy: 0.92 - ETA: 2s - loss: 0.2010 - accuracy: 0.92 - ETA: 2s - loss: 0.2005 - accuracy: 0.92 - ETA: 2s - loss: 0.2005 - accuracy: 0.92 - ETA: 2s - loss: 0.2003 - accuracy: 0.92 - ETA: 2s - loss: 0.2000 - accuracy: 0.92 - ETA: 2s - loss: 0.1999 - accuracy: 0.92 - ETA: 2s - loss: 0.1994 - accuracy: 0.92 - ETA: 2s - loss: 0.1994 - accuracy: 0.92 - ETA: 2s - loss: 0.1998 - accuracy: 0.92 - ETA: 2s - loss: 0.1995 - accuracy: 0.92 - ETA: 2s - loss: 0.2001 - accuracy: 0.92 - ETA: 2s - loss: 0.2000 - accuracy: 0.92 - ETA: 2s - loss: 0.1999 - accuracy: 0.92 - ETA: 2s - loss: 0.2000 - accuracy: 0.92 - ETA: 1s - loss: 0.1998 - accuracy: 0.92 - ETA: 1s - loss: 0.1996 - accuracy: 0.92 - ETA: 1s - loss: 0.1997 - accuracy: 0.92 - ETA: 1s - loss: 0.1996 - accuracy: 0.92 - ETA: 1s - loss: 0.2003 - accuracy: 0.92 - ETA: 1s - loss: 0.2001 - accuracy: 0.92 - ETA: 1s - loss: 0.2001 - accuracy: 0.92 - ETA: 1s - loss: 0.2001 - accuracy: 0.92 - ETA: 1s - loss: 0.1998 - accuracy: 0.92 - ETA: 1s - loss: 0.1995 - accuracy: 0.92 - ETA: 1s - loss: 0.1993 - accuracy: 0.92 - ETA: 1s - loss: 0.1993 - accuracy: 0.92 - ETA: 1s - loss: 0.1987 - accuracy: 0.92 - ETA: 1s - loss: 0.1988 - accuracy: 0.92 - ETA: 1s - loss: 0.1988 - accuracy: 0.92 - ETA: 0s - loss: 0.1989 - accuracy: 0.92 - ETA: 0s - loss: 0.1989 - accuracy: 0.92 - ETA: 0s - loss: 0.1987 - accuracy: 0.92 - ETA: 0s - loss: 0.1984 - accuracy: 0.92 - ETA: 0s - loss: 0.1984 - accuracy: 0.92 - ETA: 0s - loss: 0.1985 - accuracy: 0.92 - ETA: 0s - loss: 0.1982 - accuracy: 0.92 - ETA: 0s - loss: 0.1981 - accuracy: 0.92 - ETA: 0s - loss: 0.1979 - accuracy: 0.92 - ETA: 0s - loss: 0.1980 - accuracy: 0.92 - ETA: 0s - loss: 0.1983 - accuracy: 0.92 - ETA: 0s - loss: 0.1985 - accuracy: 0.92 - ETA: 0s - loss: 0.1983 - accuracy: 0.92 - ETA: 0s - loss: 0.1984 - accuracy: 0.92 - 18s 23ms/step - loss: 0.1986 - accuracy: 0.9292\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564/781 [====================>.........] - ETA: 18s - loss: 0.2124 - accuracy: 0.937 - ETA: 17s - loss: 0.1654 - accuracy: 0.960 - ETA: 17s - loss: 0.1503 - accuracy: 0.964 - ETA: 17s - loss: 0.1527 - accuracy: 0.959 - ETA: 17s - loss: 0.1649 - accuracy: 0.954 - ETA: 17s - loss: 0.1652 - accuracy: 0.947 - ETA: 17s - loss: 0.1809 - accuracy: 0.940 - ETA: 17s - loss: 0.1756 - accuracy: 0.941 - ETA: 17s - loss: 0.1798 - accuracy: 0.938 - ETA: 17s - loss: 0.1796 - accuracy: 0.939 - ETA: 17s - loss: 0.1783 - accuracy: 0.940 - ETA: 16s - loss: 0.1752 - accuracy: 0.943 - ETA: 16s - loss: 0.1726 - accuracy: 0.945 - ETA: 16s - loss: 0.1701 - accuracy: 0.947 - ETA: 16s - loss: 0.1726 - accuracy: 0.946 - ETA: 16s - loss: 0.1651 - accuracy: 0.949 - ETA: 16s - loss: 0.1640 - accuracy: 0.948 - ETA: 16s - loss: 0.1594 - accuracy: 0.950 - ETA: 16s - loss: 0.1608 - accuracy: 0.950 - ETA: 16s - loss: 0.1626 - accuracy: 0.949 - ETA: 16s - loss: 0.1598 - accuracy: 0.951 - ETA: 16s - loss: 0.1578 - accuracy: 0.951 - ETA: 16s - loss: 0.1587 - accuracy: 0.951 - ETA: 16s - loss: 0.1593 - accuracy: 0.950 - ETA: 16s - loss: 0.1567 - accuracy: 0.951 - ETA: 15s - loss: 0.1548 - accuracy: 0.951 - ETA: 15s - loss: 0.1508 - accuracy: 0.953 - ETA: 15s - loss: 0.1506 - accuracy: 0.953 - ETA: 15s - loss: 0.1476 - accuracy: 0.954 - ETA: 15s - loss: 0.1445 - accuracy: 0.954 - ETA: 15s - loss: 0.1425 - accuracy: 0.955 - ETA: 15s - loss: 0.1403 - accuracy: 0.955 - ETA: 15s - loss: 0.1402 - accuracy: 0.955 - ETA: 15s - loss: 0.1391 - accuracy: 0.955 - ETA: 15s - loss: 0.1363 - accuracy: 0.956 - ETA: 15s - loss: 0.1351 - accuracy: 0.956 - ETA: 15s - loss: 0.1360 - accuracy: 0.956 - ETA: 15s - loss: 0.1352 - accuracy: 0.956 - ETA: 14s - loss: 0.1372 - accuracy: 0.955 - ETA: 14s - loss: 0.1392 - accuracy: 0.954 - ETA: 14s - loss: 0.1397 - accuracy: 0.954 - ETA: 14s - loss: 0.1406 - accuracy: 0.954 - ETA: 14s - loss: 0.1410 - accuracy: 0.953 - ETA: 14s - loss: 0.1414 - accuracy: 0.953 - ETA: 14s - loss: 0.1408 - accuracy: 0.953 - ETA: 14s - loss: 0.1409 - accuracy: 0.954 - ETA: 14s - loss: 0.1405 - accuracy: 0.953 - ETA: 14s - loss: 0.1442 - accuracy: 0.952 - ETA: 14s - loss: 0.1437 - accuracy: 0.952 - ETA: 14s - loss: 0.1441 - accuracy: 0.952 - ETA: 14s - loss: 0.1422 - accuracy: 0.953 - ETA: 14s - loss: 0.1409 - accuracy: 0.953 - ETA: 14s - loss: 0.1410 - accuracy: 0.953 - ETA: 13s - loss: 0.1408 - accuracy: 0.953 - ETA: 13s - loss: 0.1402 - accuracy: 0.954 - ETA: 13s - loss: 0.1401 - accuracy: 0.954 - ETA: 13s - loss: 0.1384 - accuracy: 0.955 - ETA: 13s - loss: 0.1417 - accuracy: 0.954 - ETA: 13s - loss: 0.1423 - accuracy: 0.953 - ETA: 13s - loss: 0.1432 - accuracy: 0.953 - ETA: 13s - loss: 0.1445 - accuracy: 0.952 - ETA: 13s - loss: 0.1447 - accuracy: 0.952 - ETA: 13s - loss: 0.1451 - accuracy: 0.952 - ETA: 13s - loss: 0.1467 - accuracy: 0.951 - ETA: 13s - loss: 0.1481 - accuracy: 0.950 - ETA: 13s - loss: 0.1473 - accuracy: 0.950 - ETA: 13s - loss: 0.1468 - accuracy: 0.951 - ETA: 13s - loss: 0.1465 - accuracy: 0.951 - ETA: 12s - loss: 0.1473 - accuracy: 0.950 - ETA: 12s - loss: 0.1468 - accuracy: 0.951 - ETA: 12s - loss: 0.1471 - accuracy: 0.950 - ETA: 12s - loss: 0.1464 - accuracy: 0.950 - ETA: 12s - loss: 0.1465 - accuracy: 0.951 - ETA: 12s - loss: 0.1455 - accuracy: 0.951 - ETA: 12s - loss: 0.1449 - accuracy: 0.951 - ETA: 12s - loss: 0.1439 - accuracy: 0.952 - ETA: 12s - loss: 0.1427 - accuracy: 0.952 - ETA: 12s - loss: 0.1412 - accuracy: 0.953 - ETA: 12s - loss: 0.1404 - accuracy: 0.953 - ETA: 12s - loss: 0.1390 - accuracy: 0.954 - ETA: 12s - loss: 0.1383 - accuracy: 0.954 - ETA: 12s - loss: 0.1385 - accuracy: 0.954 - ETA: 12s - loss: 0.1377 - accuracy: 0.954 - ETA: 11s - loss: 0.1367 - accuracy: 0.954 - ETA: 11s - loss: 0.1369 - accuracy: 0.955 - ETA: 11s - loss: 0.1359 - accuracy: 0.955 - ETA: 11s - loss: 0.1350 - accuracy: 0.955 - ETA: 11s - loss: 0.1350 - accuracy: 0.955 - ETA: 11s - loss: 0.1344 - accuracy: 0.955 - ETA: 11s - loss: 0.1339 - accuracy: 0.956 - ETA: 11s - loss: 0.1329 - accuracy: 0.956 - ETA: 11s - loss: 0.1327 - accuracy: 0.956 - ETA: 11s - loss: 0.1323 - accuracy: 0.956 - ETA: 11s - loss: 0.1321 - accuracy: 0.956 - ETA: 11s - loss: 0.1318 - accuracy: 0.956 - ETA: 11s - loss: 0.1311 - accuracy: 0.956 - ETA: 11s - loss: 0.1312 - accuracy: 0.956 - ETA: 11s - loss: 0.1303 - accuracy: 0.957 - ETA: 10s - loss: 0.1304 - accuracy: 0.957 - ETA: 10s - loss: 0.1297 - accuracy: 0.957 - ETA: 10s - loss: 0.1309 - accuracy: 0.957 - ETA: 10s - loss: 0.1307 - accuracy: 0.956 - ETA: 10s - loss: 0.1306 - accuracy: 0.957 - ETA: 10s - loss: 0.1306 - accuracy: 0.957 - ETA: 10s - loss: 0.1305 - accuracy: 0.957 - ETA: 10s - loss: 0.1315 - accuracy: 0.956 - ETA: 10s - loss: 0.1308 - accuracy: 0.956 - ETA: 10s - loss: 0.1310 - accuracy: 0.956 - ETA: 10s - loss: 0.1313 - accuracy: 0.956 - ETA: 10s - loss: 0.1314 - accuracy: 0.956 - ETA: 10s - loss: 0.1311 - accuracy: 0.956 - ETA: 10s - loss: 0.1310 - accuracy: 0.956 - ETA: 10s - loss: 0.1311 - accuracy: 0.956 - ETA: 9s - loss: 0.1308 - accuracy: 0.956 - ETA: 9s - loss: 0.1318 - accuracy: 0.95 - ETA: 9s - loss: 0.1319 - accuracy: 0.95 - ETA: 9s - loss: 0.1323 - accuracy: 0.95 - ETA: 9s - loss: 0.1326 - accuracy: 0.95 - ETA: 9s - loss: 0.1326 - accuracy: 0.95 - ETA: 9s - loss: 0.1326 - accuracy: 0.95 - ETA: 9s - loss: 0.1328 - accuracy: 0.95 - ETA: 9s - loss: 0.1335 - accuracy: 0.95 - ETA: 9s - loss: 0.1337 - accuracy: 0.95 - ETA: 9s - loss: 0.1334 - accuracy: 0.95 - ETA: 9s - loss: 0.1340 - accuracy: 0.95 - ETA: 9s - loss: 0.1335 - accuracy: 0.95 - ETA: 9s - loss: 0.1338 - accuracy: 0.95 - ETA: 9s - loss: 0.1344 - accuracy: 0.95 - ETA: 8s - loss: 0.1341 - accuracy: 0.95 - ETA: 8s - loss: 0.1336 - accuracy: 0.95 - ETA: 8s - loss: 0.1334 - accuracy: 0.95 - ETA: 8s - loss: 0.1334 - accuracy: 0.95 - ETA: 8s - loss: 0.1334 - accuracy: 0.95 - ETA: 8s - loss: 0.1330 - accuracy: 0.95 - ETA: 8s - loss: 0.1339 - accuracy: 0.95 - ETA: 8s - loss: 0.1341 - accuracy: 0.95 - ETA: 8s - loss: 0.1345 - accuracy: 0.95 - ETA: 8s - loss: 0.1348 - accuracy: 0.95 - ETA: 8s - loss: 0.1347 - accuracy: 0.95 - ETA: 8s - loss: 0.1347 - accuracy: 0.95 - ETA: 8s - loss: 0.1350 - accuracy: 0.95 - ETA: 8s - loss: 0.1348 - accuracy: 0.95 - ETA: 7s - loss: 0.1346 - accuracy: 0.95 - ETA: 7s - loss: 0.1346 - accuracy: 0.95 - ETA: 7s - loss: 0.1344 - accuracy: 0.95 - ETA: 7s - loss: 0.1337 - accuracy: 0.95 - ETA: 7s - loss: 0.1330 - accuracy: 0.95 - ETA: 7s - loss: 0.1322 - accuracy: 0.95 - ETA: 7s - loss: 0.1321 - accuracy: 0.95 - ETA: 7s - loss: 0.1322 - accuracy: 0.95 - ETA: 7s - loss: 0.1321 - accuracy: 0.95 - ETA: 7s - loss: 0.1317 - accuracy: 0.95 - ETA: 7s - loss: 0.1313 - accuracy: 0.95 - ETA: 7s - loss: 0.1317 - accuracy: 0.95 - ETA: 7s - loss: 0.1313 - accuracy: 0.95 - ETA: 7s - loss: 0.1312 - accuracy: 0.95 - ETA: 7s - loss: 0.1314 - accuracy: 0.95 - ETA: 6s - loss: 0.1321 - accuracy: 0.95 - ETA: 6s - loss: 0.1324 - accuracy: 0.95 - ETA: 6s - loss: 0.1323 - accuracy: 0.95 - ETA: 6s - loss: 0.1323 - accuracy: 0.95 - ETA: 6s - loss: 0.1324 - accuracy: 0.95 - ETA: 6s - loss: 0.1321 - accuracy: 0.95 - ETA: 6s - loss: 0.1319 - accuracy: 0.95 - ETA: 6s - loss: 0.1317 - accuracy: 0.95 - ETA: 6s - loss: 0.1314 - accuracy: 0.95 - ETA: 6s - loss: 0.1312 - accuracy: 0.95 - ETA: 6s - loss: 0.1314 - accuracy: 0.95 - ETA: 6s - loss: 0.1309 - accuracy: 0.95 - ETA: 6s - loss: 0.1307 - accuracy: 0.95 - ETA: 6s - loss: 0.1304 - accuracy: 0.95 - ETA: 6s - loss: 0.1301 - accuracy: 0.95 - ETA: 5s - loss: 0.1309 - accuracy: 0.95 - ETA: 5s - loss: 0.1309 - accuracy: 0.95 - ETA: 5s - loss: 0.1303 - accuracy: 0.95 - ETA: 5s - loss: 0.1305 - accuracy: 0.95 - ETA: 5s - loss: 0.1305 - accuracy: 0.95 - ETA: 5s - loss: 0.1305 - accuracy: 0.95 - ETA: 5s - loss: 0.1305 - accuracy: 0.95 - ETA: 5s - loss: 0.1301 - accuracy: 0.95 - ETA: 5s - loss: 0.1306 - accuracy: 0.95 - ETA: 5s - loss: 0.1304 - accuracy: 0.95 - ETA: 5s - loss: 0.1305 - accuracy: 0.95 - ETA: 5s - loss: 0.1302 - accuracy: 0.95 - ETA: 5s - loss: 0.1305 - accuracy: 0.95 - ETA: 5s - loss: 0.1303 - accuracy: 0.95 - ETA: 5s - loss: 0.1301 - accuracy: 0.95 - ETA: 4s - loss: 0.1298 - accuracy: 0.95 - ETA: 4s - loss: 0.1300 - accuracy: 0.9566"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 4s - loss: 0.1300 - accuracy: 0.95 - ETA: 4s - loss: 0.1299 - accuracy: 0.95 - ETA: 4s - loss: 0.1297 - accuracy: 0.95 - ETA: 4s - loss: 0.1293 - accuracy: 0.95 - ETA: 4s - loss: 0.1294 - accuracy: 0.95 - ETA: 4s - loss: 0.1289 - accuracy: 0.95 - ETA: 4s - loss: 0.1288 - accuracy: 0.95 - ETA: 4s - loss: 0.1294 - accuracy: 0.95 - ETA: 4s - loss: 0.1293 - accuracy: 0.95 - ETA: 4s - loss: 0.1292 - accuracy: 0.95 - ETA: 4s - loss: 0.1289 - accuracy: 0.95 - ETA: 4s - loss: 0.1290 - accuracy: 0.95 - ETA: 4s - loss: 0.1287 - accuracy: 0.95 - ETA: 3s - loss: 0.1286 - accuracy: 0.95 - ETA: 3s - loss: 0.1286 - accuracy: 0.95 - ETA: 3s - loss: 0.1292 - accuracy: 0.95 - ETA: 3s - loss: 0.1293 - accuracy: 0.95 - ETA: 3s - loss: 0.1294 - accuracy: 0.95 - ETA: 3s - loss: 0.1298 - accuracy: 0.95 - ETA: 3s - loss: 0.1299 - accuracy: 0.95 - ETA: 3s - loss: 0.1295 - accuracy: 0.95 - ETA: 3s - loss: 0.1296 - accuracy: 0.95 - ETA: 3s - loss: 0.1300 - accuracy: 0.95 - ETA: 3s - loss: 0.1296 - accuracy: 0.95 - ETA: 3s - loss: 0.1294 - accuracy: 0.95 - ETA: 3s - loss: 0.1296 - accuracy: 0.95 - ETA: 3s - loss: 0.1296 - accuracy: 0.95 - ETA: 2s - loss: 0.1296 - accuracy: 0.95 - ETA: 2s - loss: 0.1303 - accuracy: 0.95 - ETA: 2s - loss: 0.1304 - accuracy: 0.95 - ETA: 2s - loss: 0.1305 - accuracy: 0.95 - ETA: 2s - loss: 0.1306 - accuracy: 0.95 - ETA: 2s - loss: 0.1307 - accuracy: 0.95 - ETA: 2s - loss: 0.1309 - accuracy: 0.95 - ETA: 2s - loss: 0.1306 - accuracy: 0.95 - ETA: 2s - loss: 0.1303 - accuracy: 0.95 - ETA: 2s - loss: 0.1300 - accuracy: 0.95 - ETA: 2s - loss: 0.1304 - accuracy: 0.95 - ETA: 2s - loss: 0.1304 - accuracy: 0.95 - ETA: 2s - loss: 0.1304 - accuracy: 0.95 - ETA: 2s - loss: 0.1301 - accuracy: 0.95 - ETA: 2s - loss: 0.1303 - accuracy: 0.95 - ETA: 1s - loss: 0.1303 - accuracy: 0.95 - ETA: 1s - loss: 0.1304 - accuracy: 0.95 - ETA: 1s - loss: 0.1305 - accuracy: 0.95 - ETA: 1s - loss: 0.1306 - accuracy: 0.95 - ETA: 1s - loss: 0.1309 - accuracy: 0.95 - ETA: 1s - loss: 0.1309 - accuracy: 0.95 - ETA: 1s - loss: 0.1312 - accuracy: 0.95 - ETA: 1s - loss: 0.1312 - accuracy: 0.95 - ETA: 1s - loss: 0.1315 - accuracy: 0.95 - ETA: 1s - loss: 0.1312 - accuracy: 0.95 - ETA: 1s - loss: 0.1312 - accuracy: 0.95 - ETA: 1s - loss: 0.1309 - accuracy: 0.95 - ETA: 1s - loss: 0.1308 - accuracy: 0.95 - ETA: 1s - loss: 0.1305 - accuracy: 0.95 - ETA: 1s - loss: 0.1303 - accuracy: 0.95 - ETA: 0s - loss: 0.1304 - accuracy: 0.95 - ETA: 0s - loss: 0.1305 - accuracy: 0.95 - ETA: 0s - loss: 0.1305 - accuracy: 0.95 - ETA: 0s - loss: 0.1301 - accuracy: 0.95 - ETA: 0s - loss: 0.1298 - accuracy: 0.95 - ETA: 0s - loss: 0.1297 - accuracy: 0.95 - ETA: 0s - loss: 0.1294 - accuracy: 0.95 - ETA: 0s - loss: 0.1296 - accuracy: 0.95 - ETA: 0s - loss: 0.1293 - accuracy: 0.95 - ETA: 0s - loss: 0.1291 - accuracy: 0.95 - ETA: 0s - loss: 0.1298 - accuracy: 0.95 - ETA: 0s - loss: 0.1304 - accuracy: 0.95 - ETA: 0s - loss: 0.1303 - accuracy: 0.95 - ETA: 0s - loss: 0.1301 - accuracy: 0.95 - ETA: 0s - loss: 0.1304 - accuracy: 0.95 - 18s 22ms/step - loss: 0.1305 - accuracy: 0.9553\n",
      "Epoch 5/5\n",
      "565/781 [====================>.........] - ETA: 17s - loss: 0.1512 - accuracy: 0.937 - ETA: 17s - loss: 0.0846 - accuracy: 0.976 - ETA: 17s - loss: 0.0817 - accuracy: 0.973 - ETA: 17s - loss: 0.0753 - accuracy: 0.978 - ETA: 17s - loss: 0.0860 - accuracy: 0.976 - ETA: 17s - loss: 0.1065 - accuracy: 0.970 - ETA: 17s - loss: 0.1162 - accuracy: 0.967 - ETA: 16s - loss: 0.1222 - accuracy: 0.961 - ETA: 16s - loss: 0.1239 - accuracy: 0.961 - ETA: 16s - loss: 0.1169 - accuracy: 0.964 - ETA: 16s - loss: 0.1126 - accuracy: 0.964 - ETA: 16s - loss: 0.1134 - accuracy: 0.965 - ETA: 16s - loss: 0.1123 - accuracy: 0.965 - ETA: 16s - loss: 0.1154 - accuracy: 0.960 - ETA: 16s - loss: 0.1162 - accuracy: 0.959 - ETA: 16s - loss: 0.1137 - accuracy: 0.960 - ETA: 16s - loss: 0.1157 - accuracy: 0.959 - ETA: 16s - loss: 0.1167 - accuracy: 0.958 - ETA: 16s - loss: 0.1166 - accuracy: 0.959 - ETA: 16s - loss: 0.1163 - accuracy: 0.960 - ETA: 15s - loss: 0.1189 - accuracy: 0.959 - ETA: 15s - loss: 0.1223 - accuracy: 0.958 - ETA: 15s - loss: 0.1241 - accuracy: 0.957 - ETA: 15s - loss: 0.1219 - accuracy: 0.958 - ETA: 15s - loss: 0.1212 - accuracy: 0.958 - ETA: 15s - loss: 0.1191 - accuracy: 0.958 - ETA: 15s - loss: 0.1179 - accuracy: 0.959 - ETA: 15s - loss: 0.1167 - accuracy: 0.960 - ETA: 15s - loss: 0.1138 - accuracy: 0.961 - ETA: 15s - loss: 0.1113 - accuracy: 0.962 - ETA: 15s - loss: 0.1104 - accuracy: 0.962 - ETA: 15s - loss: 0.1090 - accuracy: 0.962 - ETA: 15s - loss: 0.1074 - accuracy: 0.963 - ETA: 15s - loss: 0.1062 - accuracy: 0.963 - ETA: 15s - loss: 0.1057 - accuracy: 0.963 - ETA: 14s - loss: 0.1039 - accuracy: 0.963 - ETA: 14s - loss: 0.1029 - accuracy: 0.964 - ETA: 14s - loss: 0.1030 - accuracy: 0.964 - ETA: 14s - loss: 0.1014 - accuracy: 0.964 - ETA: 14s - loss: 0.1000 - accuracy: 0.965 - ETA: 14s - loss: 0.1021 - accuracy: 0.965 - ETA: 14s - loss: 0.1032 - accuracy: 0.965 - ETA: 14s - loss: 0.1039 - accuracy: 0.964 - ETA: 14s - loss: 0.1034 - accuracy: 0.964 - ETA: 14s - loss: 0.1032 - accuracy: 0.965 - ETA: 14s - loss: 0.1044 - accuracy: 0.964 - ETA: 14s - loss: 0.1037 - accuracy: 0.964 - ETA: 14s - loss: 0.1041 - accuracy: 0.964 - ETA: 14s - loss: 0.1032 - accuracy: 0.964 - ETA: 14s - loss: 0.1029 - accuracy: 0.964 - ETA: 13s - loss: 0.1030 - accuracy: 0.964 - ETA: 13s - loss: 0.1026 - accuracy: 0.964 - ETA: 13s - loss: 0.1077 - accuracy: 0.963 - ETA: 13s - loss: 0.1076 - accuracy: 0.963 - ETA: 13s - loss: 0.1066 - accuracy: 0.964 - ETA: 13s - loss: 0.1100 - accuracy: 0.962 - ETA: 13s - loss: 0.1118 - accuracy: 0.961 - ETA: 13s - loss: 0.1118 - accuracy: 0.962 - ETA: 13s - loss: 0.1125 - accuracy: 0.961 - ETA: 13s - loss: 0.1181 - accuracy: 0.959 - ETA: 13s - loss: 0.1187 - accuracy: 0.958 - ETA: 13s - loss: 0.1194 - accuracy: 0.958 - ETA: 13s - loss: 0.1218 - accuracy: 0.956 - ETA: 13s - loss: 0.1239 - accuracy: 0.955 - ETA: 13s - loss: 0.1233 - accuracy: 0.955 - ETA: 12s - loss: 0.1222 - accuracy: 0.956 - ETA: 12s - loss: 0.1230 - accuracy: 0.955 - ETA: 12s - loss: 0.1227 - accuracy: 0.955 - ETA: 12s - loss: 0.1223 - accuracy: 0.955 - ETA: 12s - loss: 0.1216 - accuracy: 0.956 - ETA: 12s - loss: 0.1212 - accuracy: 0.956 - ETA: 12s - loss: 0.1216 - accuracy: 0.956 - ETA: 12s - loss: 0.1219 - accuracy: 0.956 - ETA: 12s - loss: 0.1215 - accuracy: 0.956 - ETA: 12s - loss: 0.1209 - accuracy: 0.956 - ETA: 12s - loss: 0.1201 - accuracy: 0.956 - ETA: 12s - loss: 0.1196 - accuracy: 0.957 - ETA: 12s - loss: 0.1184 - accuracy: 0.957 - ETA: 12s - loss: 0.1180 - accuracy: 0.957 - ETA: 12s - loss: 0.1183 - accuracy: 0.957 - ETA: 11s - loss: 0.1180 - accuracy: 0.957 - ETA: 11s - loss: 0.1169 - accuracy: 0.958 - ETA: 11s - loss: 0.1168 - accuracy: 0.958 - ETA: 11s - loss: 0.1171 - accuracy: 0.958 - ETA: 11s - loss: 0.1167 - accuracy: 0.958 - ETA: 11s - loss: 0.1158 - accuracy: 0.958 - ETA: 11s - loss: 0.1155 - accuracy: 0.958 - ETA: 11s - loss: 0.1155 - accuracy: 0.958 - ETA: 11s - loss: 0.1149 - accuracy: 0.958 - ETA: 11s - loss: 0.1143 - accuracy: 0.959 - ETA: 11s - loss: 0.1137 - accuracy: 0.959 - ETA: 11s - loss: 0.1136 - accuracy: 0.959 - ETA: 11s - loss: 0.1146 - accuracy: 0.959 - ETA: 11s - loss: 0.1144 - accuracy: 0.959 - ETA: 11s - loss: 0.1144 - accuracy: 0.959 - ETA: 10s - loss: 0.1138 - accuracy: 0.959 - ETA: 10s - loss: 0.1135 - accuracy: 0.959 - ETA: 10s - loss: 0.1138 - accuracy: 0.959 - ETA: 10s - loss: 0.1141 - accuracy: 0.959 - ETA: 10s - loss: 0.1141 - accuracy: 0.959 - ETA: 10s - loss: 0.1144 - accuracy: 0.959 - ETA: 10s - loss: 0.1155 - accuracy: 0.958 - ETA: 10s - loss: 0.1153 - accuracy: 0.958 - ETA: 10s - loss: 0.1146 - accuracy: 0.959 - ETA: 10s - loss: 0.1142 - accuracy: 0.959 - ETA: 10s - loss: 0.1148 - accuracy: 0.959 - ETA: 10s - loss: 0.1146 - accuracy: 0.959 - ETA: 10s - loss: 0.1147 - accuracy: 0.958 - ETA: 10s - loss: 0.1148 - accuracy: 0.958 - ETA: 10s - loss: 0.1142 - accuracy: 0.959 - ETA: 9s - loss: 0.1141 - accuracy: 0.959 - ETA: 9s - loss: 0.1138 - accuracy: 0.95 - ETA: 9s - loss: 0.1132 - accuracy: 0.95 - ETA: 9s - loss: 0.1129 - accuracy: 0.95 - ETA: 9s - loss: 0.1124 - accuracy: 0.95 - ETA: 9s - loss: 0.1121 - accuracy: 0.95 - ETA: 9s - loss: 0.1120 - accuracy: 0.95 - ETA: 9s - loss: 0.1124 - accuracy: 0.95 - ETA: 9s - loss: 0.1126 - accuracy: 0.95 - ETA: 9s - loss: 0.1128 - accuracy: 0.95 - ETA: 9s - loss: 0.1120 - accuracy: 0.95 - ETA: 9s - loss: 0.1119 - accuracy: 0.95 - ETA: 9s - loss: 0.1114 - accuracy: 0.96 - ETA: 9s - loss: 0.1111 - accuracy: 0.96 - ETA: 9s - loss: 0.1115 - accuracy: 0.96 - ETA: 8s - loss: 0.1118 - accuracy: 0.95 - ETA: 8s - loss: 0.1122 - accuracy: 0.95 - ETA: 8s - loss: 0.1125 - accuracy: 0.95 - ETA: 8s - loss: 0.1124 - accuracy: 0.95 - ETA: 8s - loss: 0.1117 - accuracy: 0.95 - ETA: 8s - loss: 0.1119 - accuracy: 0.95 - ETA: 8s - loss: 0.1123 - accuracy: 0.95 - ETA: 8s - loss: 0.1121 - accuracy: 0.95 - ETA: 8s - loss: 0.1114 - accuracy: 0.96 - ETA: 8s - loss: 0.1116 - accuracy: 0.96 - ETA: 8s - loss: 0.1115 - accuracy: 0.96 - ETA: 8s - loss: 0.1120 - accuracy: 0.96 - ETA: 8s - loss: 0.1118 - accuracy: 0.96 - ETA: 8s - loss: 0.1114 - accuracy: 0.96 - ETA: 8s - loss: 0.1109 - accuracy: 0.96 - ETA: 8s - loss: 0.1110 - accuracy: 0.96 - ETA: 7s - loss: 0.1109 - accuracy: 0.96 - ETA: 7s - loss: 0.1106 - accuracy: 0.96 - ETA: 7s - loss: 0.1103 - accuracy: 0.96 - ETA: 7s - loss: 0.1099 - accuracy: 0.96 - ETA: 7s - loss: 0.1096 - accuracy: 0.96 - ETA: 7s - loss: 0.1096 - accuracy: 0.96 - ETA: 7s - loss: 0.1091 - accuracy: 0.96 - ETA: 7s - loss: 0.1088 - accuracy: 0.96 - ETA: 7s - loss: 0.1085 - accuracy: 0.96 - ETA: 7s - loss: 0.1081 - accuracy: 0.96 - ETA: 7s - loss: 0.1077 - accuracy: 0.96 - ETA: 7s - loss: 0.1077 - accuracy: 0.96 - ETA: 7s - loss: 0.1094 - accuracy: 0.96 - ETA: 7s - loss: 0.1091 - accuracy: 0.96 - ETA: 7s - loss: 0.1088 - accuracy: 0.96 - ETA: 6s - loss: 0.1095 - accuracy: 0.96 - ETA: 6s - loss: 0.1097 - accuracy: 0.96 - ETA: 6s - loss: 0.1102 - accuracy: 0.96 - ETA: 6s - loss: 0.1108 - accuracy: 0.96 - ETA: 6s - loss: 0.1106 - accuracy: 0.96 - ETA: 6s - loss: 0.1110 - accuracy: 0.96 - ETA: 6s - loss: 0.1112 - accuracy: 0.96 - ETA: 6s - loss: 0.1112 - accuracy: 0.96 - ETA: 6s - loss: 0.1108 - accuracy: 0.96 - ETA: 6s - loss: 0.1105 - accuracy: 0.96 - ETA: 6s - loss: 0.1105 - accuracy: 0.96 - ETA: 6s - loss: 0.1104 - accuracy: 0.96 - ETA: 6s - loss: 0.1102 - accuracy: 0.96 - ETA: 6s - loss: 0.1101 - accuracy: 0.96 - ETA: 6s - loss: 0.1100 - accuracy: 0.96 - ETA: 5s - loss: 0.1097 - accuracy: 0.96 - ETA: 5s - loss: 0.1100 - accuracy: 0.96 - ETA: 5s - loss: 0.1096 - accuracy: 0.96 - ETA: 5s - loss: 0.1091 - accuracy: 0.96 - ETA: 5s - loss: 0.1088 - accuracy: 0.96 - ETA: 5s - loss: 0.1088 - accuracy: 0.96 - ETA: 5s - loss: 0.1093 - accuracy: 0.96 - ETA: 5s - loss: 0.1091 - accuracy: 0.96 - ETA: 5s - loss: 0.1087 - accuracy: 0.96 - ETA: 5s - loss: 0.1091 - accuracy: 0.96 - ETA: 5s - loss: 0.1095 - accuracy: 0.96 - ETA: 5s - loss: 0.1090 - accuracy: 0.96 - ETA: 5s - loss: 0.1088 - accuracy: 0.96 - ETA: 5s - loss: 0.1095 - accuracy: 0.96 - ETA: 5s - loss: 0.1095 - accuracy: 0.96 - ETA: 4s - loss: 0.1093 - accuracy: 0.96 - ETA: 4s - loss: 0.1089 - accuracy: 0.96 - ETA: 4s - loss: 0.1088 - accuracy: 0.9617781/781 [==============================] - ETA: 4s - loss: 0.1086 - accuracy: 0.96 - ETA: 4s - loss: 0.1089 - accuracy: 0.96 - ETA: 4s - loss: 0.1091 - accuracy: 0.96 - ETA: 4s - loss: 0.1089 - accuracy: 0.96 - ETA: 4s - loss: 0.1089 - accuracy: 0.96 - ETA: 4s - loss: 0.1088 - accuracy: 0.96 - ETA: 4s - loss: 0.1085 - accuracy: 0.96 - ETA: 4s - loss: 0.1086 - accuracy: 0.96 - ETA: 4s - loss: 0.1085 - accuracy: 0.96 - ETA: 4s - loss: 0.1084 - accuracy: 0.96 - ETA: 4s - loss: 0.1081 - accuracy: 0.96 - ETA: 4s - loss: 0.1076 - accuracy: 0.96 - ETA: 3s - loss: 0.1074 - accuracy: 0.96 - ETA: 3s - loss: 0.1073 - accuracy: 0.96 - ETA: 3s - loss: 0.1074 - accuracy: 0.96 - ETA: 3s - loss: 0.1071 - accuracy: 0.96 - ETA: 3s - loss: 0.1073 - accuracy: 0.96 - ETA: 3s - loss: 0.1077 - accuracy: 0.96 - ETA: 3s - loss: 0.1075 - accuracy: 0.96 - ETA: 3s - loss: 0.1075 - accuracy: 0.96 - ETA: 3s - loss: 0.1073 - accuracy: 0.96 - ETA: 3s - loss: 0.1076 - accuracy: 0.96 - ETA: 3s - loss: 0.1071 - accuracy: 0.96 - ETA: 3s - loss: 0.1070 - accuracy: 0.96 - ETA: 3s - loss: 0.1068 - accuracy: 0.96 - ETA: 3s - loss: 0.1065 - accuracy: 0.96 - ETA: 3s - loss: 0.1063 - accuracy: 0.96 - ETA: 2s - loss: 0.1068 - accuracy: 0.96 - ETA: 2s - loss: 0.1067 - accuracy: 0.96 - ETA: 2s - loss: 0.1064 - accuracy: 0.96 - ETA: 2s - loss: 0.1066 - accuracy: 0.96 - ETA: 2s - loss: 0.1067 - accuracy: 0.96 - ETA: 2s - loss: 0.1063 - accuracy: 0.96 - ETA: 2s - loss: 0.1061 - accuracy: 0.96 - ETA: 2s - loss: 0.1059 - accuracy: 0.96 - ETA: 2s - loss: 0.1058 - accuracy: 0.96 - ETA: 2s - loss: 0.1056 - accuracy: 0.96 - ETA: 2s - loss: 0.1056 - accuracy: 0.96 - ETA: 2s - loss: 0.1054 - accuracy: 0.96 - ETA: 2s - loss: 0.1052 - accuracy: 0.96 - ETA: 2s - loss: 0.1048 - accuracy: 0.96 - ETA: 2s - loss: 0.1048 - accuracy: 0.96 - ETA: 1s - loss: 0.1051 - accuracy: 0.96 - ETA: 1s - loss: 0.1049 - accuracy: 0.96 - ETA: 1s - loss: 0.1051 - accuracy: 0.96 - ETA: 1s - loss: 0.1052 - accuracy: 0.96 - ETA: 1s - loss: 0.1053 - accuracy: 0.96 - ETA: 1s - loss: 0.1054 - accuracy: 0.96 - ETA: 1s - loss: 0.1052 - accuracy: 0.96 - ETA: 1s - loss: 0.1051 - accuracy: 0.96 - ETA: 1s - loss: 0.1054 - accuracy: 0.96 - ETA: 1s - loss: 0.1056 - accuracy: 0.96 - ETA: 1s - loss: 0.1055 - accuracy: 0.96 - ETA: 1s - loss: 0.1054 - accuracy: 0.96 - ETA: 1s - loss: 0.1052 - accuracy: 0.96 - ETA: 1s - loss: 0.1052 - accuracy: 0.96 - ETA: 1s - loss: 0.1055 - accuracy: 0.96 - ETA: 0s - loss: 0.1053 - accuracy: 0.96 - ETA: 0s - loss: 0.1051 - accuracy: 0.96 - ETA: 0s - loss: 0.1054 - accuracy: 0.96 - ETA: 0s - loss: 0.1051 - accuracy: 0.96 - ETA: 0s - loss: 0.1051 - accuracy: 0.96 - ETA: 0s - loss: 0.1049 - accuracy: 0.96 - ETA: 0s - loss: 0.1048 - accuracy: 0.96 - ETA: 0s - loss: 0.1049 - accuracy: 0.96 - ETA: 0s - loss: 0.1048 - accuracy: 0.96 - ETA: 0s - loss: 0.1048 - accuracy: 0.96 - ETA: 0s - loss: 0.1051 - accuracy: 0.96 - ETA: 0s - loss: 0.1054 - accuracy: 0.96 - ETA: 0s - loss: 0.1051 - accuracy: 0.96 - ETA: 0s - loss: 0.1050 - accuracy: 0.96 - 18s 22ms/step - loss: 0.1049 - accuracy: 0.9626\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)\n",
    "np.save('history/manual_masking.npy', history.history)\n",
    "model.save(\"models/manual_masking.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {}#'accuracy': accuracy}\n",
    "model = keras.models.load_model('models/manual_masking.h5', custom_objects=dependencies)\n",
    "history = np.load('history/manual_masking.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/83/a7df82744a794107641dad1decaad017d82e25f0e1f761ac9204829eef96/tensorflow_hub-0.9.0-py2.py3-none-any.whl (103kB)\n",
      "Requirement already satisfied: six>=1.12.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_hub) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_hub) (3.11.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_hub) (1.16.5)\n",
      "Requirement already satisfied: setuptools in e:\\programdata\\anaconda3\\lib\\site-packages (from protobuf>=3.8.0->tensorflow_hub) (41.4.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\")\n",
    "embeddings = embed([\"cat is on the mat\", \"dog is in the fog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=float32, numpy=\n",
       "array([[ 0.16589954,  0.0254965 ,  0.1574857 ,  0.17688066,  0.02911299,\n",
       "        -0.03092718,  0.19445257, -0.05709129, -0.08631689, -0.04391516,\n",
       "         0.13032274,  0.10905275, -0.08515751,  0.01056632, -0.17220995,\n",
       "        -0.17925954,  0.19556305,  0.0802278 , -0.03247919, -0.49176937,\n",
       "        -0.07767699, -0.03160921, -0.13952136,  0.05959712,  0.06858718,\n",
       "         0.22386682, -0.16653948,  0.19412343, -0.05491862,  0.10997339,\n",
       "        -0.15811177, -0.02576607, -0.07910853, -0.258499  , -0.04206644,\n",
       "        -0.20052543,  0.1705603 , -0.15314153,  0.0039225 , -0.28694248,\n",
       "         0.02468278,  0.11069503,  0.03733957,  0.01433943, -0.11048374,\n",
       "         0.11931834, -0.11552787, -0.11110869,  0.02384969, -0.07074881],\n",
       "       [ 0.1437864 ,  0.08291595,  0.10897306,  0.04464385, -0.03630389,\n",
       "        -0.12605834,  0.20263346,  0.12862863, -0.07873426, -0.01195358,\n",
       "         0.0020956 , -0.03080653, -0.08019945, -0.18797135, -0.11973457,\n",
       "        -0.26926652,  0.05157408, -0.15541205, -0.12221853, -0.27182642,\n",
       "         0.08750801, -0.05013347,  0.03012378,  0.20534228,  0.10000334,\n",
       "         0.18292566, -0.18280756,  0.0780353 ,  0.10936535, -0.10147726,\n",
       "        -0.19995196,  0.0398768 , -0.15377024, -0.1095404 , -0.18498933,\n",
       "        -0.15899731,  0.0558111 ,  0.15251887,  0.02046264, -0.25878936,\n",
       "        -0.13057052,  0.0782799 ,  0.04044291,  0.14456013,  0.00264394,\n",
       "         0.1399635 , -0.04803645, -0.17253871, -0.03153546,  0.09077   ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\saved_model.pb\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\assets\\tokens.txt\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\variables\\variables.data-00000-of-00001\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\variables\\variables.index\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].repeat().batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 781 steps\n",
      "Epoch 1/10\n",
      "781/781 [==============================] - ETA: 44:15 - loss: 0.7482 - accuracy: 0.437 - ETA: 8:56 - loss: 0.7029 - accuracy: 0.525 - ETA: 3:45 - loss: 0.6877 - accuracy: 0.54 - ETA: 2:22 - loss: 0.6841 - accuracy: 0.55 - ETA: 1:45 - loss: 0.6794 - accuracy: 0.57 - ETA: 1:23 - loss: 0.6732 - accuracy: 0.59 - ETA: 1:07 - loss: 0.6705 - accuracy: 0.59 - ETA: 57s - loss: 0.6649 - accuracy: 0.6100 - ETA: 50s - loss: 0.6631 - accuracy: 0.615 - ETA: 44s - loss: 0.6594 - accuracy: 0.621 - ETA: 40s - loss: 0.6591 - accuracy: 0.623 - ETA: 36s - loss: 0.6549 - accuracy: 0.629 - ETA: 33s - loss: 0.6523 - accuracy: 0.631 - ETA: 30s - loss: 0.6487 - accuracy: 0.635 - ETA: 28s - loss: 0.6446 - accuracy: 0.641 - ETA: 26s - loss: 0.6410 - accuracy: 0.646 - ETA: 24s - loss: 0.6379 - accuracy: 0.650 - ETA: 23s - loss: 0.6345 - accuracy: 0.655 - ETA: 21s - loss: 0.6322 - accuracy: 0.658 - ETA: 20s - loss: 0.6302 - accuracy: 0.658 - ETA: 19s - loss: 0.6299 - accuracy: 0.658 - ETA: 18s - loss: 0.6277 - accuracy: 0.658 - ETA: 18s - loss: 0.6265 - accuracy: 0.658 - ETA: 17s - loss: 0.6245 - accuracy: 0.661 - ETA: 16s - loss: 0.6218 - accuracy: 0.665 - ETA: 15s - loss: 0.6190 - accuracy: 0.669 - ETA: 15s - loss: 0.6173 - accuracy: 0.670 - ETA: 14s - loss: 0.6168 - accuracy: 0.670 - ETA: 14s - loss: 0.6156 - accuracy: 0.672 - ETA: 13s - loss: 0.6139 - accuracy: 0.673 - ETA: 13s - loss: 0.6128 - accuracy: 0.673 - ETA: 12s - loss: 0.6118 - accuracy: 0.674 - ETA: 12s - loss: 0.6098 - accuracy: 0.675 - ETA: 12s - loss: 0.6088 - accuracy: 0.675 - ETA: 11s - loss: 0.6065 - accuracy: 0.678 - ETA: 11s - loss: 0.6046 - accuracy: 0.679 - ETA: 10s - loss: 0.6024 - accuracy: 0.681 - ETA: 10s - loss: 0.6010 - accuracy: 0.682 - ETA: 10s - loss: 0.5992 - accuracy: 0.684 - ETA: 9s - loss: 0.5972 - accuracy: 0.686 - ETA: 9s - loss: 0.5956 - accuracy: 0.68 - ETA: 9s - loss: 0.5944 - accuracy: 0.68 - ETA: 9s - loss: 0.5942 - accuracy: 0.68 - ETA: 8s - loss: 0.5920 - accuracy: 0.69 - ETA: 8s - loss: 0.5905 - accuracy: 0.69 - ETA: 8s - loss: 0.5895 - accuracy: 0.69 - ETA: 8s - loss: 0.5886 - accuracy: 0.69 - ETA: 7s - loss: 0.5879 - accuracy: 0.69 - ETA: 7s - loss: 0.5870 - accuracy: 0.69 - ETA: 7s - loss: 0.5857 - accuracy: 0.69 - ETA: 7s - loss: 0.5844 - accuracy: 0.69 - ETA: 7s - loss: 0.5834 - accuracy: 0.69 - ETA: 6s - loss: 0.5816 - accuracy: 0.70 - ETA: 6s - loss: 0.5814 - accuracy: 0.70 - ETA: 6s - loss: 0.5803 - accuracy: 0.70 - ETA: 6s - loss: 0.5798 - accuracy: 0.70 - ETA: 6s - loss: 0.5795 - accuracy: 0.70 - ETA: 5s - loss: 0.5777 - accuracy: 0.70 - ETA: 5s - loss: 0.5762 - accuracy: 0.70 - ETA: 5s - loss: 0.5751 - accuracy: 0.70 - ETA: 5s - loss: 0.5742 - accuracy: 0.70 - ETA: 5s - loss: 0.5738 - accuracy: 0.70 - ETA: 5s - loss: 0.5730 - accuracy: 0.70 - ETA: 4s - loss: 0.5718 - accuracy: 0.70 - ETA: 4s - loss: 0.5713 - accuracy: 0.70 - ETA: 4s - loss: 0.5707 - accuracy: 0.70 - ETA: 4s - loss: 0.5700 - accuracy: 0.70 - ETA: 4s - loss: 0.5688 - accuracy: 0.70 - ETA: 4s - loss: 0.5681 - accuracy: 0.71 - ETA: 4s - loss: 0.5668 - accuracy: 0.71 - ETA: 3s - loss: 0.5664 - accuracy: 0.71 - ETA: 3s - loss: 0.5651 - accuracy: 0.71 - ETA: 3s - loss: 0.5653 - accuracy: 0.71 - ETA: 3s - loss: 0.5646 - accuracy: 0.71 - ETA: 3s - loss: 0.5637 - accuracy: 0.71 - ETA: 3s - loss: 0.5633 - accuracy: 0.71 - ETA: 3s - loss: 0.5629 - accuracy: 0.71 - ETA: 3s - loss: 0.5620 - accuracy: 0.71 - ETA: 3s - loss: 0.5620 - accuracy: 0.71 - ETA: 2s - loss: 0.5618 - accuracy: 0.71 - ETA: 2s - loss: 0.5611 - accuracy: 0.71 - ETA: 2s - loss: 0.5603 - accuracy: 0.71 - ETA: 2s - loss: 0.5597 - accuracy: 0.71 - ETA: 2s - loss: 0.5583 - accuracy: 0.71 - ETA: 2s - loss: 0.5578 - accuracy: 0.71 - ETA: 2s - loss: 0.5579 - accuracy: 0.71 - ETA: 2s - loss: 0.5573 - accuracy: 0.71 - ETA: 1s - loss: 0.5570 - accuracy: 0.71 - ETA: 1s - loss: 0.5565 - accuracy: 0.71 - ETA: 1s - loss: 0.5553 - accuracy: 0.72 - ETA: 1s - loss: 0.5551 - accuracy: 0.72 - ETA: 1s - loss: 0.5551 - accuracy: 0.72 - ETA: 1s - loss: 0.5549 - accuracy: 0.72 - ETA: 1s - loss: 0.5541 - accuracy: 0.72 - ETA: 1s - loss: 0.5536 - accuracy: 0.72 - ETA: 1s - loss: 0.5532 - accuracy: 0.72 - ETA: 1s - loss: 0.5526 - accuracy: 0.72 - ETA: 1s - loss: 0.5518 - accuracy: 0.72 - ETA: 0s - loss: 0.5515 - accuracy: 0.72 - ETA: 0s - loss: 0.5513 - accuracy: 0.72 - ETA: 0s - loss: 0.5505 - accuracy: 0.72 - ETA: 0s - loss: 0.5504 - accuracy: 0.72 - ETA: 0s - loss: 0.5495 - accuracy: 0.72 - ETA: 0s - loss: 0.5492 - accuracy: 0.72 - ETA: 0s - loss: 0.5489 - accuracy: 0.72 - ETA: 0s - loss: 0.5490 - accuracy: 0.72 - ETA: 0s - loss: 0.5488 - accuracy: 0.72 - ETA: 0s - loss: 0.5484 - accuracy: 0.72 - ETA: 0s - loss: 0.5481 - accuracy: 0.72 - 9s 12ms/step - loss: 0.5482 - accuracy: 0.7253\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 4s - loss: 0.5799 - accuracy: 0.68 - ETA: 5s - loss: 0.5053 - accuracy: 0.75 - ETA: 5s - loss: 0.5093 - accuracy: 0.75 - ETA: 5s - loss: 0.5074 - accuracy: 0.74 - ETA: 5s - loss: 0.5004 - accuracy: 0.75 - ETA: 5s - loss: 0.5107 - accuracy: 0.75 - ETA: 5s - loss: 0.5146 - accuracy: 0.75 - ETA: 5s - loss: 0.5133 - accuracy: 0.75 - ETA: 5s - loss: 0.5282 - accuracy: 0.74 - ETA: 5s - loss: 0.5273 - accuracy: 0.74 - ETA: 5s - loss: 0.5319 - accuracy: 0.74 - ETA: 5s - loss: 0.5320 - accuracy: 0.73 - ETA: 5s - loss: 0.5326 - accuracy: 0.73 - ETA: 4s - loss: 0.5293 - accuracy: 0.73 - ETA: 4s - loss: 0.5286 - accuracy: 0.74 - ETA: 4s - loss: 0.5294 - accuracy: 0.73 - ETA: 4s - loss: 0.5280 - accuracy: 0.73 - ETA: 4s - loss: 0.5288 - accuracy: 0.73 - ETA: 4s - loss: 0.5280 - accuracy: 0.73 - ETA: 4s - loss: 0.5292 - accuracy: 0.73 - ETA: 4s - loss: 0.5296 - accuracy: 0.73 - ETA: 4s - loss: 0.5308 - accuracy: 0.73 - ETA: 4s - loss: 0.5279 - accuracy: 0.73 - ETA: 4s - loss: 0.5274 - accuracy: 0.73 - ETA: 4s - loss: 0.5263 - accuracy: 0.74 - ETA: 4s - loss: 0.5252 - accuracy: 0.73 - ETA: 4s - loss: 0.5270 - accuracy: 0.73 - ETA: 4s - loss: 0.5277 - accuracy: 0.73 - ETA: 4s - loss: 0.5296 - accuracy: 0.73 - ETA: 4s - loss: 0.5284 - accuracy: 0.73 - ETA: 4s - loss: 0.5299 - accuracy: 0.73 - ETA: 4s - loss: 0.5293 - accuracy: 0.73 - ETA: 3s - loss: 0.5303 - accuracy: 0.73 - ETA: 3s - loss: 0.5296 - accuracy: 0.73 - ETA: 3s - loss: 0.5292 - accuracy: 0.73 - ETA: 3s - loss: 0.5277 - accuracy: 0.73 - ETA: 3s - loss: 0.5280 - accuracy: 0.73 - ETA: 3s - loss: 0.5263 - accuracy: 0.73 - ETA: 3s - loss: 0.5249 - accuracy: 0.73 - ETA: 3s - loss: 0.5245 - accuracy: 0.73 - ETA: 3s - loss: 0.5237 - accuracy: 0.74 - ETA: 3s - loss: 0.5249 - accuracy: 0.73 - ETA: 3s - loss: 0.5246 - accuracy: 0.73 - ETA: 3s - loss: 0.5235 - accuracy: 0.73 - ETA: 3s - loss: 0.5235 - accuracy: 0.73 - ETA: 3s - loss: 0.5234 - accuracy: 0.74 - ETA: 3s - loss: 0.5236 - accuracy: 0.73 - ETA: 3s - loss: 0.5236 - accuracy: 0.73 - ETA: 3s - loss: 0.5229 - accuracy: 0.73 - ETA: 3s - loss: 0.5222 - accuracy: 0.74 - ETA: 3s - loss: 0.5224 - accuracy: 0.74 - ETA: 2s - loss: 0.5219 - accuracy: 0.74 - ETA: 2s - loss: 0.5221 - accuracy: 0.74 - ETA: 2s - loss: 0.5222 - accuracy: 0.74 - ETA: 2s - loss: 0.5222 - accuracy: 0.74 - ETA: 2s - loss: 0.5225 - accuracy: 0.74 - ETA: 2s - loss: 0.5214 - accuracy: 0.74 - ETA: 2s - loss: 0.5207 - accuracy: 0.74 - ETA: 2s - loss: 0.5199 - accuracy: 0.74 - ETA: 2s - loss: 0.5197 - accuracy: 0.74 - ETA: 2s - loss: 0.5202 - accuracy: 0.74 - ETA: 2s - loss: 0.5199 - accuracy: 0.74 - ETA: 2s - loss: 0.5198 - accuracy: 0.74 - ETA: 2s - loss: 0.5199 - accuracy: 0.74 - ETA: 2s - loss: 0.5197 - accuracy: 0.74 - ETA: 2s - loss: 0.5198 - accuracy: 0.74 - ETA: 2s - loss: 0.5196 - accuracy: 0.74 - ETA: 2s - loss: 0.5192 - accuracy: 0.74 - ETA: 2s - loss: 0.5185 - accuracy: 0.74 - ETA: 2s - loss: 0.5181 - accuracy: 0.74 - ETA: 2s - loss: 0.5179 - accuracy: 0.74 - ETA: 1s - loss: 0.5178 - accuracy: 0.74 - ETA: 1s - loss: 0.5176 - accuracy: 0.74 - ETA: 1s - loss: 0.5173 - accuracy: 0.74 - ETA: 1s - loss: 0.5169 - accuracy: 0.74 - ETA: 1s - loss: 0.5178 - accuracy: 0.74 - ETA: 1s - loss: 0.5172 - accuracy: 0.74 - ETA: 1s - loss: 0.5173 - accuracy: 0.74 - ETA: 1s - loss: 0.5174 - accuracy: 0.74 - ETA: 1s - loss: 0.5173 - accuracy: 0.74 - ETA: 1s - loss: 0.5173 - accuracy: 0.74 - ETA: 1s - loss: 0.5173 - accuracy: 0.74 - ETA: 1s - loss: 0.5163 - accuracy: 0.74 - ETA: 1s - loss: 0.5161 - accuracy: 0.74 - ETA: 1s - loss: 0.5166 - accuracy: 0.74 - ETA: 1s - loss: 0.5164 - accuracy: 0.74 - ETA: 1s - loss: 0.5164 - accuracy: 0.74 - ETA: 1s - loss: 0.5163 - accuracy: 0.74 - ETA: 1s - loss: 0.5153 - accuracy: 0.74 - ETA: 0s - loss: 0.5156 - accuracy: 0.74 - ETA: 0s - loss: 0.5160 - accuracy: 0.74 - ETA: 0s - loss: 0.5157 - accuracy: 0.74 - ETA: 0s - loss: 0.5152 - accuracy: 0.74 - ETA: 0s - loss: 0.5153 - accuracy: 0.74 - ETA: 0s - loss: 0.5153 - accuracy: 0.74 - ETA: 0s - loss: 0.5144 - accuracy: 0.74 - ETA: 0s - loss: 0.5146 - accuracy: 0.74 - ETA: 0s - loss: 0.5146 - accuracy: 0.74 - ETA: 0s - loss: 0.5141 - accuracy: 0.74 - ETA: 0s - loss: 0.5139 - accuracy: 0.74 - ETA: 0s - loss: 0.5134 - accuracy: 0.74 - ETA: 0s - loss: 0.5134 - accuracy: 0.74 - ETA: 0s - loss: 0.5134 - accuracy: 0.74 - ETA: 0s - loss: 0.5137 - accuracy: 0.74 - ETA: 0s - loss: 0.5139 - accuracy: 0.74 - ETA: 0s - loss: 0.5136 - accuracy: 0.74 - ETA: 0s - loss: 0.5137 - accuracy: 0.74 - 6s 7ms/step - loss: 0.5137 - accuracy: 0.7479\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 6s - loss: 0.6044 - accuracy: 0.68 - ETA: 6s - loss: 0.5112 - accuracy: 0.75 - ETA: 5s - loss: 0.5043 - accuracy: 0.76 - ETA: 5s - loss: 0.5044 - accuracy: 0.75 - ETA: 5s - loss: 0.4952 - accuracy: 0.76 - ETA: 5s - loss: 0.5045 - accuracy: 0.75 - ETA: 5s - loss: 0.5101 - accuracy: 0.75 - ETA: 5s - loss: 0.5125 - accuracy: 0.75 - ETA: 5s - loss: 0.5233 - accuracy: 0.75 - ETA: 5s - loss: 0.5231 - accuracy: 0.74 - ETA: 5s - loss: 0.5253 - accuracy: 0.74 - ETA: 5s - loss: 0.5272 - accuracy: 0.74 - ETA: 5s - loss: 0.5279 - accuracy: 0.74 - ETA: 5s - loss: 0.5241 - accuracy: 0.74 - ETA: 4s - loss: 0.5238 - accuracy: 0.74 - ETA: 4s - loss: 0.5243 - accuracy: 0.74 - ETA: 4s - loss: 0.5223 - accuracy: 0.74 - ETA: 4s - loss: 0.5225 - accuracy: 0.74 - ETA: 4s - loss: 0.5220 - accuracy: 0.74 - ETA: 4s - loss: 0.5221 - accuracy: 0.74 - ETA: 4s - loss: 0.5234 - accuracy: 0.74 - ETA: 4s - loss: 0.5240 - accuracy: 0.74 - ETA: 4s - loss: 0.5222 - accuracy: 0.74 - ETA: 4s - loss: 0.5212 - accuracy: 0.74 - ETA: 4s - loss: 0.5201 - accuracy: 0.74 - ETA: 4s - loss: 0.5193 - accuracy: 0.74 - ETA: 4s - loss: 0.5217 - accuracy: 0.74 - ETA: 4s - loss: 0.5221 - accuracy: 0.74 - ETA: 4s - loss: 0.5236 - accuracy: 0.74 - ETA: 4s - loss: 0.5232 - accuracy: 0.74 - ETA: 4s - loss: 0.5240 - accuracy: 0.74 - ETA: 4s - loss: 0.5231 - accuracy: 0.74 - ETA: 3s - loss: 0.5244 - accuracy: 0.74 - ETA: 3s - loss: 0.5232 - accuracy: 0.74 - ETA: 3s - loss: 0.5222 - accuracy: 0.74 - ETA: 3s - loss: 0.5217 - accuracy: 0.74 - ETA: 3s - loss: 0.5217 - accuracy: 0.74 - ETA: 3s - loss: 0.5199 - accuracy: 0.74 - ETA: 3s - loss: 0.5183 - accuracy: 0.74 - ETA: 3s - loss: 0.5184 - accuracy: 0.74 - ETA: 3s - loss: 0.5181 - accuracy: 0.74 - ETA: 3s - loss: 0.5189 - accuracy: 0.74 - ETA: 3s - loss: 0.5182 - accuracy: 0.74 - ETA: 3s - loss: 0.5175 - accuracy: 0.74 - ETA: 3s - loss: 0.5170 - accuracy: 0.74 - ETA: 3s - loss: 0.5177 - accuracy: 0.74 - ETA: 3s - loss: 0.5174 - accuracy: 0.74 - ETA: 3s - loss: 0.5170 - accuracy: 0.74 - ETA: 3s - loss: 0.5170 - accuracy: 0.74 - ETA: 3s - loss: 0.5168 - accuracy: 0.74 - ETA: 3s - loss: 0.5160 - accuracy: 0.74 - ETA: 3s - loss: 0.5160 - accuracy: 0.74 - ETA: 3s - loss: 0.5162 - accuracy: 0.74 - ETA: 2s - loss: 0.5162 - accuracy: 0.74 - ETA: 2s - loss: 0.5161 - accuracy: 0.74 - ETA: 2s - loss: 0.5166 - accuracy: 0.74 - ETA: 2s - loss: 0.5163 - accuracy: 0.74 - ETA: 2s - loss: 0.5148 - accuracy: 0.74 - ETA: 2s - loss: 0.5141 - accuracy: 0.74 - ETA: 2s - loss: 0.5141 - accuracy: 0.74 - ETA: 2s - loss: 0.5140 - accuracy: 0.74 - ETA: 2s - loss: 0.5139 - accuracy: 0.74 - ETA: 2s - loss: 0.5137 - accuracy: 0.74 - ETA: 2s - loss: 0.5137 - accuracy: 0.74 - ETA: 2s - loss: 0.5137 - accuracy: 0.74 - ETA: 2s - loss: 0.5140 - accuracy: 0.74 - ETA: 2s - loss: 0.5139 - accuracy: 0.74 - ETA: 2s - loss: 0.5133 - accuracy: 0.74 - ETA: 2s - loss: 0.5125 - accuracy: 0.74 - ETA: 2s - loss: 0.5123 - accuracy: 0.74 - ETA: 2s - loss: 0.5120 - accuracy: 0.74 - ETA: 1s - loss: 0.5119 - accuracy: 0.74 - ETA: 1s - loss: 0.5117 - accuracy: 0.74 - ETA: 1s - loss: 0.5116 - accuracy: 0.74 - ETA: 1s - loss: 0.5112 - accuracy: 0.74 - ETA: 1s - loss: 0.5121 - accuracy: 0.74 - ETA: 1s - loss: 0.5116 - accuracy: 0.74 - ETA: 1s - loss: 0.5116 - accuracy: 0.74 - ETA: 1s - loss: 0.5117 - accuracy: 0.74 - ETA: 1s - loss: 0.5112 - accuracy: 0.74 - ETA: 1s - loss: 0.5117 - accuracy: 0.74 - ETA: 1s - loss: 0.5115 - accuracy: 0.74 - ETA: 1s - loss: 0.5109 - accuracy: 0.74 - ETA: 1s - loss: 0.5105 - accuracy: 0.74 - ETA: 1s - loss: 0.5102 - accuracy: 0.75 - ETA: 1s - loss: 0.5111 - accuracy: 0.74 - ETA: 1s - loss: 0.5110 - accuracy: 0.74 - ETA: 1s - loss: 0.5108 - accuracy: 0.74 - ETA: 1s - loss: 0.5104 - accuracy: 0.74 - ETA: 1s - loss: 0.5101 - accuracy: 0.75 - ETA: 0s - loss: 0.5104 - accuracy: 0.74 - ETA: 0s - loss: 0.5104 - accuracy: 0.75 - ETA: 0s - loss: 0.5100 - accuracy: 0.75 - ETA: 0s - loss: 0.5096 - accuracy: 0.75 - ETA: 0s - loss: 0.5097 - accuracy: 0.75 - ETA: 0s - loss: 0.5096 - accuracy: 0.75 - ETA: 0s - loss: 0.5090 - accuracy: 0.75 - ETA: 0s - loss: 0.5093 - accuracy: 0.75 - ETA: 0s - loss: 0.5092 - accuracy: 0.75 - ETA: 0s - loss: 0.5088 - accuracy: 0.75 - ETA: 0s - loss: 0.5087 - accuracy: 0.75 - ETA: 0s - loss: 0.5081 - accuracy: 0.75 - ETA: 0s - loss: 0.5082 - accuracy: 0.75 - ETA: 0s - loss: 0.5081 - accuracy: 0.75 - ETA: 0s - loss: 0.5080 - accuracy: 0.75 - ETA: 0s - loss: 0.5085 - accuracy: 0.75 - ETA: 0s - loss: 0.5085 - accuracy: 0.75 - ETA: 0s - loss: 0.5082 - accuracy: 0.75 - ETA: 0s - loss: 0.5084 - accuracy: 0.75 - 6s 7ms/step - loss: 0.5083 - accuracy: 0.7519\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 5s - loss: 0.6226 - accuracy: 0.68 - ETA: 5s - loss: 0.4950 - accuracy: 0.78 - ETA: 5s - loss: 0.5017 - accuracy: 0.77 - ETA: 5s - loss: 0.5037 - accuracy: 0.76 - ETA: 5s - loss: 0.4963 - accuracy: 0.77 - ETA: 5s - loss: 0.5050 - accuracy: 0.76 - ETA: 5s - loss: 0.5060 - accuracy: 0.76 - ETA: 5s - loss: 0.5056 - accuracy: 0.76 - ETA: 5s - loss: 0.5199 - accuracy: 0.75 - ETA: 5s - loss: 0.5197 - accuracy: 0.75 - ETA: 5s - loss: 0.5231 - accuracy: 0.75 - ETA: 5s - loss: 0.5228 - accuracy: 0.75 - ETA: 5s - loss: 0.5229 - accuracy: 0.74 - ETA: 5s - loss: 0.5216 - accuracy: 0.74 - ETA: 4s - loss: 0.5211 - accuracy: 0.75 - ETA: 4s - loss: 0.5202 - accuracy: 0.74 - ETA: 4s - loss: 0.5181 - accuracy: 0.75 - ETA: 4s - loss: 0.5185 - accuracy: 0.75 - ETA: 4s - loss: 0.5165 - accuracy: 0.75 - ETA: 4s - loss: 0.5176 - accuracy: 0.74 - ETA: 4s - loss: 0.5193 - accuracy: 0.74 - ETA: 4s - loss: 0.5194 - accuracy: 0.74 - ETA: 4s - loss: 0.5189 - accuracy: 0.74 - ETA: 4s - loss: 0.5179 - accuracy: 0.74 - ETA: 4s - loss: 0.5155 - accuracy: 0.74 - ETA: 4s - loss: 0.5156 - accuracy: 0.74 - ETA: 4s - loss: 0.5171 - accuracy: 0.74 - ETA: 4s - loss: 0.5174 - accuracy: 0.74 - ETA: 4s - loss: 0.5199 - accuracy: 0.74 - ETA: 4s - loss: 0.5188 - accuracy: 0.74 - ETA: 4s - loss: 0.5198 - accuracy: 0.74 - ETA: 4s - loss: 0.5205 - accuracy: 0.74 - ETA: 4s - loss: 0.5200 - accuracy: 0.74 - ETA: 4s - loss: 0.5196 - accuracy: 0.74 - ETA: 3s - loss: 0.5195 - accuracy: 0.74 - ETA: 3s - loss: 0.5184 - accuracy: 0.74 - ETA: 3s - loss: 0.5174 - accuracy: 0.74 - ETA: 3s - loss: 0.5174 - accuracy: 0.74 - ETA: 3s - loss: 0.5156 - accuracy: 0.74 - ETA: 3s - loss: 0.5150 - accuracy: 0.74 - ETA: 3s - loss: 0.5139 - accuracy: 0.74 - ETA: 3s - loss: 0.5152 - accuracy: 0.74 - ETA: 3s - loss: 0.5154 - accuracy: 0.74 - ETA: 3s - loss: 0.5139 - accuracy: 0.74 - ETA: 3s - loss: 0.5135 - accuracy: 0.74 - ETA: 3s - loss: 0.5141 - accuracy: 0.74 - ETA: 3s - loss: 0.5139 - accuracy: 0.74 - ETA: 3s - loss: 0.5137 - accuracy: 0.74 - ETA: 3s - loss: 0.5137 - accuracy: 0.74 - ETA: 3s - loss: 0.5135 - accuracy: 0.74 - ETA: 3s - loss: 0.5128 - accuracy: 0.74 - ETA: 3s - loss: 0.5124 - accuracy: 0.74 - ETA: 2s - loss: 0.5129 - accuracy: 0.74 - ETA: 2s - loss: 0.5126 - accuracy: 0.74 - ETA: 2s - loss: 0.5125 - accuracy: 0.74 - ETA: 2s - loss: 0.5130 - accuracy: 0.74 - ETA: 2s - loss: 0.5118 - accuracy: 0.74 - ETA: 2s - loss: 0.5111 - accuracy: 0.75 - ETA: 2s - loss: 0.5102 - accuracy: 0.75 - ETA: 2s - loss: 0.5102 - accuracy: 0.75 - ETA: 2s - loss: 0.5106 - accuracy: 0.75 - ETA: 2s - loss: 0.5101 - accuracy: 0.75 - ETA: 2s - loss: 0.5099 - accuracy: 0.75 - ETA: 2s - loss: 0.5101 - accuracy: 0.75 - ETA: 2s - loss: 0.5104 - accuracy: 0.75 - ETA: 2s - loss: 0.5101 - accuracy: 0.75 - ETA: 2s - loss: 0.5098 - accuracy: 0.75 - ETA: 2s - loss: 0.5092 - accuracy: 0.75 - ETA: 2s - loss: 0.5085 - accuracy: 0.75 - ETA: 2s - loss: 0.5086 - accuracy: 0.75 - ETA: 1s - loss: 0.5080 - accuracy: 0.75 - ETA: 1s - loss: 0.5080 - accuracy: 0.75 - ETA: 1s - loss: 0.5078 - accuracy: 0.75 - ETA: 1s - loss: 0.5078 - accuracy: 0.75 - ETA: 1s - loss: 0.5083 - accuracy: 0.75 - ETA: 1s - loss: 0.5076 - accuracy: 0.75 - ETA: 1s - loss: 0.5082 - accuracy: 0.75 - ETA: 1s - loss: 0.5083 - accuracy: 0.75 - ETA: 1s - loss: 0.5083 - accuracy: 0.75 - ETA: 1s - loss: 0.5081 - accuracy: 0.75 - ETA: 1s - loss: 0.5081 - accuracy: 0.75 - ETA: 1s - loss: 0.5070 - accuracy: 0.75 - ETA: 1s - loss: 0.5068 - accuracy: 0.75 - ETA: 1s - loss: 0.5074 - accuracy: 0.75 - ETA: 1s - loss: 0.5073 - accuracy: 0.75 - ETA: 1s - loss: 0.5072 - accuracy: 0.75 - ETA: 1s - loss: 0.5070 - accuracy: 0.75 - ETA: 1s - loss: 0.5060 - accuracy: 0.75 - ETA: 0s - loss: 0.5067 - accuracy: 0.75 - ETA: 0s - loss: 0.5069 - accuracy: 0.75 - ETA: 0s - loss: 0.5065 - accuracy: 0.75 - ETA: 0s - loss: 0.5060 - accuracy: 0.75 - ETA: 0s - loss: 0.5062 - accuracy: 0.75 - ETA: 0s - loss: 0.5061 - accuracy: 0.75 - ETA: 0s - loss: 0.5055 - accuracy: 0.75 - ETA: 0s - loss: 0.5057 - accuracy: 0.75 - ETA: 0s - loss: 0.5058 - accuracy: 0.75 - ETA: 0s - loss: 0.5052 - accuracy: 0.75 - ETA: 0s - loss: 0.5052 - accuracy: 0.75 - ETA: 0s - loss: 0.5045 - accuracy: 0.75 - ETA: 0s - loss: 0.5046 - accuracy: 0.75 - ETA: 0s - loss: 0.5045 - accuracy: 0.75 - ETA: 0s - loss: 0.5051 - accuracy: 0.75 - ETA: 0s - loss: 0.5050 - accuracy: 0.75 - ETA: 0s - loss: 0.5048 - accuracy: 0.75 - ETA: 0s - loss: 0.5047 - accuracy: 0.75 - 6s 7ms/step - loss: 0.5049 - accuracy: 0.7548\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 5s - loss: 0.5376 - accuracy: 0.65 - ETA: 5s - loss: 0.5033 - accuracy: 0.77 - ETA: 5s - loss: 0.5018 - accuracy: 0.77 - ETA: 5s - loss: 0.5001 - accuracy: 0.76 - ETA: 5s - loss: 0.4880 - accuracy: 0.77 - ETA: 5s - loss: 0.5019 - accuracy: 0.76 - ETA: 5s - loss: 0.5032 - accuracy: 0.76 - ETA: 5s - loss: 0.5062 - accuracy: 0.76 - ETA: 5s - loss: 0.5145 - accuracy: 0.75 - ETA: 5s - loss: 0.5155 - accuracy: 0.75 - ETA: 5s - loss: 0.5190 - accuracy: 0.75 - ETA: 5s - loss: 0.5194 - accuracy: 0.75 - ETA: 5s - loss: 0.5209 - accuracy: 0.74 - ETA: 5s - loss: 0.5194 - accuracy: 0.74 - ETA: 5s - loss: 0.5186 - accuracy: 0.74 - ETA: 5s - loss: 0.5154 - accuracy: 0.75 - ETA: 4s - loss: 0.5158 - accuracy: 0.74 - ETA: 4s - loss: 0.5152 - accuracy: 0.75 - ETA: 4s - loss: 0.5140 - accuracy: 0.75 - ETA: 4s - loss: 0.5140 - accuracy: 0.75 - ETA: 4s - loss: 0.5148 - accuracy: 0.74 - ETA: 4s - loss: 0.5148 - accuracy: 0.74 - ETA: 4s - loss: 0.5162 - accuracy: 0.74 - ETA: 4s - loss: 0.5145 - accuracy: 0.74 - ETA: 4s - loss: 0.5121 - accuracy: 0.75 - ETA: 4s - loss: 0.5125 - accuracy: 0.75 - ETA: 4s - loss: 0.5125 - accuracy: 0.75 - ETA: 4s - loss: 0.5147 - accuracy: 0.74 - ETA: 4s - loss: 0.5159 - accuracy: 0.74 - ETA: 4s - loss: 0.5156 - accuracy: 0.74 - ETA: 4s - loss: 0.5164 - accuracy: 0.74 - ETA: 4s - loss: 0.5172 - accuracy: 0.74 - ETA: 4s - loss: 0.5173 - accuracy: 0.74 - ETA: 4s - loss: 0.5171 - accuracy: 0.74 - ETA: 3s - loss: 0.5171 - accuracy: 0.74 - ETA: 3s - loss: 0.5148 - accuracy: 0.74 - ETA: 3s - loss: 0.5145 - accuracy: 0.74 - ETA: 3s - loss: 0.5146 - accuracy: 0.74 - ETA: 3s - loss: 0.5128 - accuracy: 0.74 - ETA: 3s - loss: 0.5116 - accuracy: 0.75 - ETA: 3s - loss: 0.5109 - accuracy: 0.75 - ETA: 3s - loss: 0.5120 - accuracy: 0.75 - ETA: 3s - loss: 0.5122 - accuracy: 0.74 - ETA: 3s - loss: 0.5107 - accuracy: 0.75 - ETA: 3s - loss: 0.5104 - accuracy: 0.75 - ETA: 3s - loss: 0.5108 - accuracy: 0.75 - ETA: 3s - loss: 0.5106 - accuracy: 0.75 - ETA: 3s - loss: 0.5106 - accuracy: 0.75 - ETA: 3s - loss: 0.5107 - accuracy: 0.75 - ETA: 3s - loss: 0.5104 - accuracy: 0.75 - ETA: 3s - loss: 0.5095 - accuracy: 0.75 - ETA: 3s - loss: 0.5095 - accuracy: 0.75 - ETA: 2s - loss: 0.5098 - accuracy: 0.75 - ETA: 2s - loss: 0.5097 - accuracy: 0.75 - ETA: 2s - loss: 0.5095 - accuracy: 0.75 - ETA: 2s - loss: 0.5097 - accuracy: 0.75 - ETA: 2s - loss: 0.5098 - accuracy: 0.75 - ETA: 2s - loss: 0.5082 - accuracy: 0.75 - ETA: 2s - loss: 0.5074 - accuracy: 0.75 - ETA: 2s - loss: 0.5076 - accuracy: 0.75 - ETA: 2s - loss: 0.5072 - accuracy: 0.75 - ETA: 2s - loss: 0.5073 - accuracy: 0.75 - ETA: 2s - loss: 0.5072 - accuracy: 0.75 - ETA: 2s - loss: 0.5068 - accuracy: 0.75 - ETA: 2s - loss: 0.5069 - accuracy: 0.75 - ETA: 2s - loss: 0.5072 - accuracy: 0.75 - ETA: 2s - loss: 0.5071 - accuracy: 0.75 - ETA: 2s - loss: 0.5068 - accuracy: 0.75 - ETA: 2s - loss: 0.5060 - accuracy: 0.75 - ETA: 2s - loss: 0.5054 - accuracy: 0.75 - ETA: 2s - loss: 0.5058 - accuracy: 0.75 - ETA: 1s - loss: 0.5047 - accuracy: 0.75 - ETA: 1s - loss: 0.5052 - accuracy: 0.75 - ETA: 1s - loss: 0.5050 - accuracy: 0.75 - ETA: 1s - loss: 0.5047 - accuracy: 0.75 - ETA: 1s - loss: 0.5054 - accuracy: 0.75 - ETA: 1s - loss: 0.5050 - accuracy: 0.75 - ETA: 1s - loss: 0.5045 - accuracy: 0.75 - ETA: 1s - loss: 0.5052 - accuracy: 0.75 - ETA: 1s - loss: 0.5046 - accuracy: 0.75 - ETA: 1s - loss: 0.5051 - accuracy: 0.75 - ETA: 1s - loss: 0.5051 - accuracy: 0.75 - ETA: 1s - loss: 0.5044 - accuracy: 0.75 - ETA: 1s - loss: 0.5040 - accuracy: 0.75 - ETA: 1s - loss: 0.5036 - accuracy: 0.75 - ETA: 1s - loss: 0.5044 - accuracy: 0.75 - ETA: 1s - loss: 0.5043 - accuracy: 0.75 - ETA: 1s - loss: 0.5042 - accuracy: 0.75 - ETA: 1s - loss: 0.5037 - accuracy: 0.75 - ETA: 1s - loss: 0.5035 - accuracy: 0.75 - ETA: 0s - loss: 0.5037 - accuracy: 0.75 - ETA: 0s - loss: 0.5038 - accuracy: 0.75 - ETA: 0s - loss: 0.5035 - accuracy: 0.75 - ETA: 0s - loss: 0.5029 - accuracy: 0.75 - ETA: 0s - loss: 0.5033 - accuracy: 0.75 - ETA: 0s - loss: 0.5032 - accuracy: 0.75 - ETA: 0s - loss: 0.5025 - accuracy: 0.75 - ETA: 0s - loss: 0.5030 - accuracy: 0.75 - ETA: 0s - loss: 0.5027 - accuracy: 0.75 - ETA: 0s - loss: 0.5023 - accuracy: 0.75 - ETA: 0s - loss: 0.5021 - accuracy: 0.75 - ETA: 0s - loss: 0.5016 - accuracy: 0.75 - ETA: 0s - loss: 0.5016 - accuracy: 0.75 - ETA: 0s - loss: 0.5016 - accuracy: 0.75 - ETA: 0s - loss: 0.5019 - accuracy: 0.75 - ETA: 0s - loss: 0.5020 - accuracy: 0.75 - ETA: 0s - loss: 0.5019 - accuracy: 0.75 - ETA: 0s - loss: 0.5019 - accuracy: 0.75 - 6s 7ms/step - loss: 0.5020 - accuracy: 0.7558\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 6s - loss: 0.4812 - accuracy: 0.71 - ETA: 6s - loss: 0.4995 - accuracy: 0.78 - ETA: 5s - loss: 0.4953 - accuracy: 0.77 - ETA: 5s - loss: 0.4979 - accuracy: 0.76 - ETA: 5s - loss: 0.4845 - accuracy: 0.76 - ETA: 5s - loss: 0.4938 - accuracy: 0.76 - ETA: 5s - loss: 0.4961 - accuracy: 0.76 - ETA: 5s - loss: 0.5012 - accuracy: 0.76 - ETA: 5s - loss: 0.5101 - accuracy: 0.75 - ETA: 5s - loss: 0.5109 - accuracy: 0.75 - ETA: 5s - loss: 0.5176 - accuracy: 0.75 - ETA: 5s - loss: 0.5139 - accuracy: 0.75 - ETA: 5s - loss: 0.5158 - accuracy: 0.75 - ETA: 5s - loss: 0.5161 - accuracy: 0.74 - ETA: 5s - loss: 0.5150 - accuracy: 0.75 - ETA: 5s - loss: 0.5124 - accuracy: 0.75 - ETA: 5s - loss: 0.5113 - accuracy: 0.75 - ETA: 4s - loss: 0.5097 - accuracy: 0.75 - ETA: 4s - loss: 0.5093 - accuracy: 0.75 - ETA: 4s - loss: 0.5094 - accuracy: 0.75 - ETA: 4s - loss: 0.5110 - accuracy: 0.75 - ETA: 4s - loss: 0.5115 - accuracy: 0.75 - ETA: 4s - loss: 0.5126 - accuracy: 0.74 - ETA: 4s - loss: 0.5111 - accuracy: 0.75 - ETA: 4s - loss: 0.5083 - accuracy: 0.75 - ETA: 4s - loss: 0.5093 - accuracy: 0.75 - ETA: 4s - loss: 0.5082 - accuracy: 0.75 - ETA: 4s - loss: 0.5111 - accuracy: 0.75 - ETA: 4s - loss: 0.5114 - accuracy: 0.75 - ETA: 4s - loss: 0.5121 - accuracy: 0.75 - ETA: 4s - loss: 0.5127 - accuracy: 0.75 - ETA: 4s - loss: 0.5131 - accuracy: 0.75 - ETA: 4s - loss: 0.5126 - accuracy: 0.75 - ETA: 4s - loss: 0.5136 - accuracy: 0.74 - ETA: 4s - loss: 0.5129 - accuracy: 0.74 - ETA: 3s - loss: 0.5126 - accuracy: 0.75 - ETA: 3s - loss: 0.5112 - accuracy: 0.75 - ETA: 3s - loss: 0.5114 - accuracy: 0.75 - ETA: 3s - loss: 0.5097 - accuracy: 0.75 - ETA: 3s - loss: 0.5079 - accuracy: 0.75 - ETA: 3s - loss: 0.5082 - accuracy: 0.75 - ETA: 3s - loss: 0.5078 - accuracy: 0.75 - ETA: 3s - loss: 0.5089 - accuracy: 0.75 - ETA: 3s - loss: 0.5078 - accuracy: 0.75 - ETA: 3s - loss: 0.5071 - accuracy: 0.75 - ETA: 3s - loss: 0.5070 - accuracy: 0.75 - ETA: 3s - loss: 0.5071 - accuracy: 0.75 - ETA: 3s - loss: 0.5076 - accuracy: 0.75 - ETA: 3s - loss: 0.5076 - accuracy: 0.75 - ETA: 3s - loss: 0.5075 - accuracy: 0.75 - ETA: 3s - loss: 0.5065 - accuracy: 0.75 - ETA: 3s - loss: 0.5063 - accuracy: 0.75 - ETA: 3s - loss: 0.5064 - accuracy: 0.75 - ETA: 2s - loss: 0.5061 - accuracy: 0.75 - ETA: 2s - loss: 0.5064 - accuracy: 0.75 - ETA: 2s - loss: 0.5064 - accuracy: 0.75 - ETA: 2s - loss: 0.5067 - accuracy: 0.75 - ETA: 2s - loss: 0.5055 - accuracy: 0.75 - ETA: 2s - loss: 0.5047 - accuracy: 0.75 - ETA: 2s - loss: 0.5041 - accuracy: 0.75 - ETA: 2s - loss: 0.5039 - accuracy: 0.75 - ETA: 2s - loss: 0.5044 - accuracy: 0.75 - ETA: 2s - loss: 0.5041 - accuracy: 0.75 - ETA: 2s - loss: 0.5038 - accuracy: 0.75 - ETA: 2s - loss: 0.5040 - accuracy: 0.75 - ETA: 2s - loss: 0.5044 - accuracy: 0.75 - ETA: 2s - loss: 0.5040 - accuracy: 0.75 - ETA: 2s - loss: 0.5041 - accuracy: 0.75 - ETA: 2s - loss: 0.5036 - accuracy: 0.75 - ETA: 2s - loss: 0.5028 - accuracy: 0.75 - ETA: 2s - loss: 0.5025 - accuracy: 0.75 - ETA: 2s - loss: 0.5023 - accuracy: 0.75 - ETA: 1s - loss: 0.5020 - accuracy: 0.75 - ETA: 1s - loss: 0.5019 - accuracy: 0.75 - ETA: 1s - loss: 0.5017 - accuracy: 0.75 - ETA: 1s - loss: 0.5014 - accuracy: 0.75 - ETA: 1s - loss: 0.5022 - accuracy: 0.75 - ETA: 1s - loss: 0.5018 - accuracy: 0.75 - ETA: 1s - loss: 0.5016 - accuracy: 0.75 - ETA: 1s - loss: 0.5019 - accuracy: 0.75 - ETA: 1s - loss: 0.5015 - accuracy: 0.75 - ETA: 1s - loss: 0.5021 - accuracy: 0.75 - ETA: 1s - loss: 0.5020 - accuracy: 0.75 - ETA: 1s - loss: 0.5015 - accuracy: 0.75 - ETA: 1s - loss: 0.5010 - accuracy: 0.75 - ETA: 1s - loss: 0.5007 - accuracy: 0.75 - ETA: 1s - loss: 0.5014 - accuracy: 0.75 - ETA: 1s - loss: 0.5014 - accuracy: 0.75 - ETA: 1s - loss: 0.5010 - accuracy: 0.75 - ETA: 1s - loss: 0.5006 - accuracy: 0.75 - ETA: 1s - loss: 0.5001 - accuracy: 0.75 - ETA: 0s - loss: 0.5005 - accuracy: 0.75 - ETA: 0s - loss: 0.5008 - accuracy: 0.75 - ETA: 0s - loss: 0.5005 - accuracy: 0.75 - ETA: 0s - loss: 0.5000 - accuracy: 0.75 - ETA: 0s - loss: 0.5002 - accuracy: 0.75 - ETA: 0s - loss: 0.5003 - accuracy: 0.75 - ETA: 0s - loss: 0.4997 - accuracy: 0.75 - ETA: 0s - loss: 0.5000 - accuracy: 0.75 - ETA: 0s - loss: 0.4996 - accuracy: 0.75 - ETA: 0s - loss: 0.4993 - accuracy: 0.75 - ETA: 0s - loss: 0.4992 - accuracy: 0.75 - ETA: 0s - loss: 0.4988 - accuracy: 0.75 - ETA: 0s - loss: 0.4989 - accuracy: 0.75 - ETA: 0s - loss: 0.4988 - accuracy: 0.75 - ETA: 0s - loss: 0.4988 - accuracy: 0.75 - ETA: 0s - loss: 0.4991 - accuracy: 0.75 - ETA: 0s - loss: 0.4992 - accuracy: 0.75 - ETA: 0s - loss: 0.4989 - accuracy: 0.75 - ETA: 0s - loss: 0.4988 - accuracy: 0.75 - 6s 7ms/step - loss: 0.4991 - accuracy: 0.7583\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 8s - loss: 0.4103 - accuracy: 0.75 - ETA: 6s - loss: 0.4936 - accuracy: 0.78 - ETA: 5s - loss: 0.4880 - accuracy: 0.77 - ETA: 5s - loss: 0.4932 - accuracy: 0.76 - ETA: 5s - loss: 0.4830 - accuracy: 0.77 - ETA: 5s - loss: 0.4893 - accuracy: 0.77 - ETA: 5s - loss: 0.4940 - accuracy: 0.76 - ETA: 5s - loss: 0.4994 - accuracy: 0.76 - ETA: 5s - loss: 0.5068 - accuracy: 0.76 - ETA: 5s - loss: 0.5070 - accuracy: 0.75 - ETA: 5s - loss: 0.5152 - accuracy: 0.75 - ETA: 5s - loss: 0.5121 - accuracy: 0.75 - ETA: 5s - loss: 0.5150 - accuracy: 0.75 - ETA: 5s - loss: 0.5131 - accuracy: 0.75 - ETA: 5s - loss: 0.5118 - accuracy: 0.75 - ETA: 5s - loss: 0.5092 - accuracy: 0.75 - ETA: 4s - loss: 0.5079 - accuracy: 0.75 - ETA: 4s - loss: 0.5070 - accuracy: 0.75 - ETA: 4s - loss: 0.5065 - accuracy: 0.75 - ETA: 4s - loss: 0.5067 - accuracy: 0.75 - ETA: 4s - loss: 0.5087 - accuracy: 0.75 - ETA: 4s - loss: 0.5083 - accuracy: 0.75 - ETA: 4s - loss: 0.5098 - accuracy: 0.75 - ETA: 4s - loss: 0.5084 - accuracy: 0.75 - ETA: 4s - loss: 0.5061 - accuracy: 0.75 - ETA: 4s - loss: 0.5065 - accuracy: 0.75 - ETA: 4s - loss: 0.5059 - accuracy: 0.75 - ETA: 4s - loss: 0.5082 - accuracy: 0.75 - ETA: 4s - loss: 0.5085 - accuracy: 0.75 - ETA: 4s - loss: 0.5091 - accuracy: 0.75 - ETA: 4s - loss: 0.5098 - accuracy: 0.75 - ETA: 4s - loss: 0.5105 - accuracy: 0.75 - ETA: 4s - loss: 0.5108 - accuracy: 0.75 - ETA: 4s - loss: 0.5107 - accuracy: 0.75 - ETA: 3s - loss: 0.5102 - accuracy: 0.75 - ETA: 3s - loss: 0.5087 - accuracy: 0.75 - ETA: 3s - loss: 0.5079 - accuracy: 0.75 - ETA: 3s - loss: 0.5081 - accuracy: 0.75 - ETA: 3s - loss: 0.5064 - accuracy: 0.75 - ETA: 3s - loss: 0.5053 - accuracy: 0.75 - ETA: 3s - loss: 0.5053 - accuracy: 0.75 - ETA: 3s - loss: 0.5056 - accuracy: 0.75 - ETA: 3s - loss: 0.5059 - accuracy: 0.75 - ETA: 3s - loss: 0.5047 - accuracy: 0.75 - ETA: 3s - loss: 0.5042 - accuracy: 0.75 - ETA: 3s - loss: 0.5042 - accuracy: 0.75 - ETA: 3s - loss: 0.5044 - accuracy: 0.75 - ETA: 3s - loss: 0.5049 - accuracy: 0.75 - ETA: 3s - loss: 0.5052 - accuracy: 0.75 - ETA: 3s - loss: 0.5045 - accuracy: 0.75 - ETA: 3s - loss: 0.5036 - accuracy: 0.75 - ETA: 3s - loss: 0.5034 - accuracy: 0.75 - ETA: 3s - loss: 0.5034 - accuracy: 0.75 - ETA: 2s - loss: 0.5035 - accuracy: 0.75 - ETA: 2s - loss: 0.5037 - accuracy: 0.75 - ETA: 2s - loss: 0.5038 - accuracy: 0.75 - ETA: 2s - loss: 0.5042 - accuracy: 0.75 - ETA: 2s - loss: 0.5030 - accuracy: 0.75 - ETA: 2s - loss: 0.5023 - accuracy: 0.75 - ETA: 2s - loss: 0.5016 - accuracy: 0.75 - ETA: 2s - loss: 0.5015 - accuracy: 0.75 - ETA: 2s - loss: 0.5019 - accuracy: 0.75 - ETA: 2s - loss: 0.5018 - accuracy: 0.75 - ETA: 2s - loss: 0.5015 - accuracy: 0.75 - ETA: 2s - loss: 0.5015 - accuracy: 0.75 - ETA: 2s - loss: 0.5017 - accuracy: 0.75 - ETA: 2s - loss: 0.5014 - accuracy: 0.75 - ETA: 2s - loss: 0.5013 - accuracy: 0.75 - ETA: 2s - loss: 0.5009 - accuracy: 0.75 - ETA: 2s - loss: 0.5000 - accuracy: 0.75 - ETA: 2s - loss: 0.5000 - accuracy: 0.75 - ETA: 2s - loss: 0.4995 - accuracy: 0.75 - ETA: 1s - loss: 0.4997 - accuracy: 0.75 - ETA: 1s - loss: 0.4996 - accuracy: 0.75 - ETA: 1s - loss: 0.4992 - accuracy: 0.75 - ETA: 1s - loss: 0.4992 - accuracy: 0.75 - ETA: 1s - loss: 0.4997 - accuracy: 0.75 - ETA: 1s - loss: 0.4993 - accuracy: 0.75 - ETA: 1s - loss: 0.4994 - accuracy: 0.75 - ETA: 1s - loss: 0.4995 - accuracy: 0.75 - ETA: 1s - loss: 0.4995 - accuracy: 0.75 - ETA: 1s - loss: 0.4996 - accuracy: 0.75 - ETA: 1s - loss: 0.4992 - accuracy: 0.75 - ETA: 1s - loss: 0.4988 - accuracy: 0.75 - ETA: 1s - loss: 0.4985 - accuracy: 0.75 - ETA: 1s - loss: 0.4981 - accuracy: 0.75 - ETA: 1s - loss: 0.4989 - accuracy: 0.75 - ETA: 1s - loss: 0.4990 - accuracy: 0.75 - ETA: 1s - loss: 0.4985 - accuracy: 0.75 - ETA: 1s - loss: 0.4985 - accuracy: 0.75 - ETA: 1s - loss: 0.4975 - accuracy: 0.75 - ETA: 0s - loss: 0.4980 - accuracy: 0.75 - ETA: 0s - loss: 0.4981 - accuracy: 0.75 - ETA: 0s - loss: 0.4982 - accuracy: 0.75 - ETA: 0s - loss: 0.4975 - accuracy: 0.75 - ETA: 0s - loss: 0.4977 - accuracy: 0.75 - ETA: 0s - loss: 0.4977 - accuracy: 0.75 - ETA: 0s - loss: 0.4972 - accuracy: 0.76 - ETA: 0s - loss: 0.4974 - accuracy: 0.75 - ETA: 0s - loss: 0.4971 - accuracy: 0.75 - ETA: 0s - loss: 0.4969 - accuracy: 0.75 - ETA: 0s - loss: 0.4968 - accuracy: 0.75 - ETA: 0s - loss: 0.4964 - accuracy: 0.76 - ETA: 0s - loss: 0.4964 - accuracy: 0.76 - ETA: 0s - loss: 0.4962 - accuracy: 0.76 - ETA: 0s - loss: 0.4963 - accuracy: 0.76 - ETA: 0s - loss: 0.4968 - accuracy: 0.75 - ETA: 0s - loss: 0.4968 - accuracy: 0.75 - ETA: 0s - loss: 0.4966 - accuracy: 0.76 - ETA: 0s - loss: 0.4966 - accuracy: 0.76 - 6s 7ms/step - loss: 0.4966 - accuracy: 0.7602\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 6s - loss: 0.3923 - accuracy: 0.84 - ETA: 6s - loss: 0.4935 - accuracy: 0.78 - ETA: 5s - loss: 0.4879 - accuracy: 0.77 - ETA: 5s - loss: 0.4861 - accuracy: 0.76 - ETA: 5s - loss: 0.4797 - accuracy: 0.77 - ETA: 5s - loss: 0.4860 - accuracy: 0.77 - ETA: 5s - loss: 0.4938 - accuracy: 0.77 - ETA: 5s - loss: 0.4974 - accuracy: 0.76 - ETA: 5s - loss: 0.5055 - accuracy: 0.76 - ETA: 5s - loss: 0.5065 - accuracy: 0.75 - ETA: 5s - loss: 0.5120 - accuracy: 0.75 - ETA: 5s - loss: 0.5099 - accuracy: 0.75 - ETA: 5s - loss: 0.5124 - accuracy: 0.75 - ETA: 5s - loss: 0.5113 - accuracy: 0.75 - ETA: 5s - loss: 0.5101 - accuracy: 0.75 - ETA: 5s - loss: 0.5065 - accuracy: 0.75 - ETA: 4s - loss: 0.5057 - accuracy: 0.75 - ETA: 4s - loss: 0.5049 - accuracy: 0.75 - ETA: 4s - loss: 0.5045 - accuracy: 0.75 - ETA: 4s - loss: 0.5043 - accuracy: 0.75 - ETA: 4s - loss: 0.5055 - accuracy: 0.75 - ETA: 4s - loss: 0.5064 - accuracy: 0.75 - ETA: 4s - loss: 0.5069 - accuracy: 0.75 - ETA: 4s - loss: 0.5053 - accuracy: 0.75 - ETA: 4s - loss: 0.5027 - accuracy: 0.75 - ETA: 4s - loss: 0.5033 - accuracy: 0.75 - ETA: 4s - loss: 0.5048 - accuracy: 0.75 - ETA: 4s - loss: 0.5054 - accuracy: 0.75 - ETA: 4s - loss: 0.5074 - accuracy: 0.75 - ETA: 4s - loss: 0.5066 - accuracy: 0.75 - ETA: 4s - loss: 0.5076 - accuracy: 0.75 - ETA: 4s - loss: 0.5082 - accuracy: 0.75 - ETA: 4s - loss: 0.5082 - accuracy: 0.75 - ETA: 3s - loss: 0.5077 - accuracy: 0.75 - ETA: 3s - loss: 0.5071 - accuracy: 0.75 - ETA: 3s - loss: 0.5059 - accuracy: 0.75 - ETA: 3s - loss: 0.5058 - accuracy: 0.75 - ETA: 3s - loss: 0.5052 - accuracy: 0.75 - ETA: 3s - loss: 0.5034 - accuracy: 0.75 - ETA: 3s - loss: 0.5029 - accuracy: 0.75 - ETA: 3s - loss: 0.5024 - accuracy: 0.75 - ETA: 3s - loss: 0.5026 - accuracy: 0.75 - ETA: 3s - loss: 0.5032 - accuracy: 0.75 - ETA: 3s - loss: 0.5017 - accuracy: 0.75 - ETA: 3s - loss: 0.5020 - accuracy: 0.75 - ETA: 3s - loss: 0.5021 - accuracy: 0.75 - ETA: 3s - loss: 0.5016 - accuracy: 0.75 - ETA: 3s - loss: 0.5019 - accuracy: 0.75 - ETA: 3s - loss: 0.5021 - accuracy: 0.75 - ETA: 3s - loss: 0.5019 - accuracy: 0.75 - ETA: 3s - loss: 0.5009 - accuracy: 0.75 - ETA: 3s - loss: 0.5009 - accuracy: 0.75 - ETA: 2s - loss: 0.5011 - accuracy: 0.75 - ETA: 2s - loss: 0.5012 - accuracy: 0.75 - ETA: 2s - loss: 0.5012 - accuracy: 0.75 - ETA: 2s - loss: 0.5015 - accuracy: 0.75 - ETA: 2s - loss: 0.5008 - accuracy: 0.75 - ETA: 2s - loss: 0.4998 - accuracy: 0.75 - ETA: 2s - loss: 0.4992 - accuracy: 0.75 - ETA: 2s - loss: 0.4992 - accuracy: 0.75 - ETA: 2s - loss: 0.4989 - accuracy: 0.75 - ETA: 2s - loss: 0.4991 - accuracy: 0.75 - ETA: 2s - loss: 0.4990 - accuracy: 0.75 - ETA: 2s - loss: 0.4986 - accuracy: 0.75 - ETA: 2s - loss: 0.4987 - accuracy: 0.75 - ETA: 2s - loss: 0.4989 - accuracy: 0.75 - ETA: 2s - loss: 0.4987 - accuracy: 0.75 - ETA: 2s - loss: 0.4984 - accuracy: 0.75 - ETA: 2s - loss: 0.4979 - accuracy: 0.76 - ETA: 2s - loss: 0.4972 - accuracy: 0.76 - ETA: 2s - loss: 0.4972 - accuracy: 0.76 - ETA: 1s - loss: 0.4964 - accuracy: 0.76 - ETA: 1s - loss: 0.4968 - accuracy: 0.76 - ETA: 1s - loss: 0.4969 - accuracy: 0.76 - ETA: 1s - loss: 0.4965 - accuracy: 0.76 - ETA: 1s - loss: 0.4966 - accuracy: 0.76 - ETA: 1s - loss: 0.4970 - accuracy: 0.75 - ETA: 1s - loss: 0.4963 - accuracy: 0.76 - ETA: 1s - loss: 0.4969 - accuracy: 0.76 - ETA: 1s - loss: 0.4970 - accuracy: 0.75 - ETA: 1s - loss: 0.4970 - accuracy: 0.76 - ETA: 1s - loss: 0.4969 - accuracy: 0.76 - ETA: 1s - loss: 0.4969 - accuracy: 0.76 - ETA: 1s - loss: 0.4958 - accuracy: 0.76 - ETA: 1s - loss: 0.4956 - accuracy: 0.76 - ETA: 1s - loss: 0.4961 - accuracy: 0.76 - ETA: 1s - loss: 0.4960 - accuracy: 0.76 - ETA: 1s - loss: 0.4959 - accuracy: 0.76 - ETA: 1s - loss: 0.4956 - accuracy: 0.76 - ETA: 1s - loss: 0.4946 - accuracy: 0.76 - ETA: 0s - loss: 0.4952 - accuracy: 0.76 - ETA: 0s - loss: 0.4953 - accuracy: 0.76 - ETA: 0s - loss: 0.4955 - accuracy: 0.76 - ETA: 0s - loss: 0.4950 - accuracy: 0.76 - ETA: 0s - loss: 0.4949 - accuracy: 0.76 - ETA: 0s - loss: 0.4950 - accuracy: 0.76 - ETA: 0s - loss: 0.4947 - accuracy: 0.76 - ETA: 0s - loss: 0.4949 - accuracy: 0.76 - ETA: 0s - loss: 0.4944 - accuracy: 0.76 - ETA: 0s - loss: 0.4944 - accuracy: 0.76 - ETA: 0s - loss: 0.4942 - accuracy: 0.76 - ETA: 0s - loss: 0.4939 - accuracy: 0.76 - ETA: 0s - loss: 0.4938 - accuracy: 0.76 - ETA: 0s - loss: 0.4936 - accuracy: 0.76 - ETA: 0s - loss: 0.4937 - accuracy: 0.76 - ETA: 0s - loss: 0.4942 - accuracy: 0.76 - ETA: 0s - loss: 0.4941 - accuracy: 0.76 - ETA: 0s - loss: 0.4940 - accuracy: 0.76 - ETA: 0s - loss: 0.4938 - accuracy: 0.76 - 6s 7ms/step - loss: 0.4941 - accuracy: 0.7622\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 7s - loss: 0.4476 - accuracy: 0.78 - ETA: 5s - loss: 0.4986 - accuracy: 0.78 - ETA: 5s - loss: 0.4841 - accuracy: 0.77 - ETA: 5s - loss: 0.4891 - accuracy: 0.75 - ETA: 5s - loss: 0.4804 - accuracy: 0.76 - ETA: 5s - loss: 0.4857 - accuracy: 0.77 - ETA: 5s - loss: 0.4905 - accuracy: 0.76 - ETA: 5s - loss: 0.4937 - accuracy: 0.76 - ETA: 5s - loss: 0.5019 - accuracy: 0.76 - ETA: 5s - loss: 0.5037 - accuracy: 0.75 - ETA: 5s - loss: 0.5108 - accuracy: 0.75 - ETA: 5s - loss: 0.5072 - accuracy: 0.75 - ETA: 5s - loss: 0.5094 - accuracy: 0.75 - ETA: 5s - loss: 0.5098 - accuracy: 0.75 - ETA: 5s - loss: 0.5083 - accuracy: 0.75 - ETA: 4s - loss: 0.5045 - accuracy: 0.75 - ETA: 4s - loss: 0.5045 - accuracy: 0.75 - ETA: 4s - loss: 0.5040 - accuracy: 0.75 - ETA: 4s - loss: 0.5025 - accuracy: 0.75 - ETA: 4s - loss: 0.5021 - accuracy: 0.75 - ETA: 4s - loss: 0.5038 - accuracy: 0.75 - ETA: 4s - loss: 0.5037 - accuracy: 0.75 - ETA: 4s - loss: 0.5061 - accuracy: 0.75 - ETA: 4s - loss: 0.5035 - accuracy: 0.75 - ETA: 4s - loss: 0.5019 - accuracy: 0.75 - ETA: 4s - loss: 0.5026 - accuracy: 0.75 - ETA: 4s - loss: 0.5007 - accuracy: 0.75 - ETA: 4s - loss: 0.5032 - accuracy: 0.75 - ETA: 4s - loss: 0.5036 - accuracy: 0.75 - ETA: 4s - loss: 0.5044 - accuracy: 0.75 - ETA: 4s - loss: 0.5048 - accuracy: 0.75 - ETA: 4s - loss: 0.5055 - accuracy: 0.75 - ETA: 4s - loss: 0.5049 - accuracy: 0.75 - ETA: 4s - loss: 0.5055 - accuracy: 0.75 - ETA: 3s - loss: 0.5047 - accuracy: 0.75 - ETA: 3s - loss: 0.5047 - accuracy: 0.75 - ETA: 3s - loss: 0.5035 - accuracy: 0.75 - ETA: 3s - loss: 0.5035 - accuracy: 0.75 - ETA: 3s - loss: 0.5021 - accuracy: 0.75 - ETA: 3s - loss: 0.5006 - accuracy: 0.75 - ETA: 3s - loss: 0.5001 - accuracy: 0.75 - ETA: 3s - loss: 0.4996 - accuracy: 0.75 - ETA: 3s - loss: 0.5009 - accuracy: 0.75 - ETA: 3s - loss: 0.5004 - accuracy: 0.75 - ETA: 3s - loss: 0.4994 - accuracy: 0.75 - ETA: 3s - loss: 0.4991 - accuracy: 0.75 - ETA: 3s - loss: 0.4995 - accuracy: 0.75 - ETA: 3s - loss: 0.4994 - accuracy: 0.75 - ETA: 3s - loss: 0.4995 - accuracy: 0.75 - ETA: 3s - loss: 0.4997 - accuracy: 0.75 - ETA: 3s - loss: 0.4993 - accuracy: 0.75 - ETA: 3s - loss: 0.4985 - accuracy: 0.75 - ETA: 3s - loss: 0.4986 - accuracy: 0.75 - ETA: 3s - loss: 0.4988 - accuracy: 0.75 - ETA: 2s - loss: 0.4988 - accuracy: 0.75 - ETA: 2s - loss: 0.4987 - accuracy: 0.75 - ETA: 2s - loss: 0.4991 - accuracy: 0.75 - ETA: 2s - loss: 0.4985 - accuracy: 0.75 - ETA: 2s - loss: 0.4975 - accuracy: 0.76 - ETA: 2s - loss: 0.4966 - accuracy: 0.76 - ETA: 2s - loss: 0.4965 - accuracy: 0.76 - ETA: 2s - loss: 0.4964 - accuracy: 0.76 - ETA: 2s - loss: 0.4966 - accuracy: 0.76 - ETA: 2s - loss: 0.4965 - accuracy: 0.76 - ETA: 2s - loss: 0.4960 - accuracy: 0.76 - ETA: 2s - loss: 0.4961 - accuracy: 0.76 - ETA: 2s - loss: 0.4963 - accuracy: 0.76 - ETA: 2s - loss: 0.4961 - accuracy: 0.76 - ETA: 2s - loss: 0.4955 - accuracy: 0.76 - ETA: 2s - loss: 0.4955 - accuracy: 0.76 - ETA: 2s - loss: 0.4945 - accuracy: 0.76 - ETA: 2s - loss: 0.4947 - accuracy: 0.76 - ETA: 2s - loss: 0.4941 - accuracy: 0.76 - ETA: 1s - loss: 0.4943 - accuracy: 0.76 - ETA: 1s - loss: 0.4944 - accuracy: 0.76 - ETA: 1s - loss: 0.4941 - accuracy: 0.76 - ETA: 1s - loss: 0.4941 - accuracy: 0.76 - ETA: 1s - loss: 0.4944 - accuracy: 0.76 - ETA: 1s - loss: 0.4938 - accuracy: 0.76 - ETA: 1s - loss: 0.4943 - accuracy: 0.76 - ETA: 1s - loss: 0.4943 - accuracy: 0.76 - ETA: 1s - loss: 0.4946 - accuracy: 0.76 - ETA: 1s - loss: 0.4945 - accuracy: 0.76 - ETA: 1s - loss: 0.4944 - accuracy: 0.76 - ETA: 1s - loss: 0.4935 - accuracy: 0.76 - ETA: 1s - loss: 0.4933 - accuracy: 0.76 - ETA: 1s - loss: 0.4936 - accuracy: 0.76 - ETA: 1s - loss: 0.4934 - accuracy: 0.76 - ETA: 1s - loss: 0.4935 - accuracy: 0.76 - ETA: 1s - loss: 0.4932 - accuracy: 0.76 - ETA: 1s - loss: 0.4923 - accuracy: 0.76 - ETA: 0s - loss: 0.4927 - accuracy: 0.76 - ETA: 0s - loss: 0.4929 - accuracy: 0.76 - ETA: 0s - loss: 0.4931 - accuracy: 0.76 - ETA: 0s - loss: 0.4926 - accuracy: 0.76 - ETA: 0s - loss: 0.4925 - accuracy: 0.76 - ETA: 0s - loss: 0.4925 - accuracy: 0.76 - ETA: 0s - loss: 0.4924 - accuracy: 0.76 - ETA: 0s - loss: 0.4923 - accuracy: 0.76 - ETA: 0s - loss: 0.4923 - accuracy: 0.76 - ETA: 0s - loss: 0.4921 - accuracy: 0.76 - ETA: 0s - loss: 0.4918 - accuracy: 0.76 - ETA: 0s - loss: 0.4918 - accuracy: 0.76 - ETA: 0s - loss: 0.4912 - accuracy: 0.76 - ETA: 0s - loss: 0.4914 - accuracy: 0.76 - ETA: 0s - loss: 0.4914 - accuracy: 0.76 - ETA: 0s - loss: 0.4919 - accuracy: 0.76 - ETA: 0s - loss: 0.4920 - accuracy: 0.76 - ETA: 0s - loss: 0.4917 - accuracy: 0.76 - ETA: 0s - loss: 0.4916 - accuracy: 0.76 - 6s 7ms/step - loss: 0.4918 - accuracy: 0.7636\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - ETA: 7s - loss: 0.4432 - accuracy: 0.81 - ETA: 5s - loss: 0.4840 - accuracy: 0.80 - ETA: 5s - loss: 0.4787 - accuracy: 0.78 - ETA: 5s - loss: 0.4858 - accuracy: 0.76 - ETA: 5s - loss: 0.4754 - accuracy: 0.77 - ETA: 5s - loss: 0.4835 - accuracy: 0.77 - ETA: 5s - loss: 0.4894 - accuracy: 0.77 - ETA: 5s - loss: 0.4901 - accuracy: 0.77 - ETA: 5s - loss: 0.5018 - accuracy: 0.76 - ETA: 5s - loss: 0.5001 - accuracy: 0.76 - ETA: 5s - loss: 0.5045 - accuracy: 0.76 - ETA: 5s - loss: 0.5038 - accuracy: 0.76 - ETA: 5s - loss: 0.5058 - accuracy: 0.75 - ETA: 4s - loss: 0.5027 - accuracy: 0.76 - ETA: 4s - loss: 0.5018 - accuracy: 0.76 - ETA: 4s - loss: 0.5024 - accuracy: 0.76 - ETA: 4s - loss: 0.5001 - accuracy: 0.76 - ETA: 4s - loss: 0.4994 - accuracy: 0.76 - ETA: 4s - loss: 0.4990 - accuracy: 0.76 - ETA: 4s - loss: 0.5002 - accuracy: 0.76 - ETA: 4s - loss: 0.5002 - accuracy: 0.76 - ETA: 4s - loss: 0.5018 - accuracy: 0.75 - ETA: 4s - loss: 0.4999 - accuracy: 0.76 - ETA: 4s - loss: 0.4976 - accuracy: 0.76 - ETA: 4s - loss: 0.4985 - accuracy: 0.76 - ETA: 4s - loss: 0.4973 - accuracy: 0.76 - ETA: 4s - loss: 0.5007 - accuracy: 0.76 - ETA: 4s - loss: 0.5003 - accuracy: 0.76 - ETA: 4s - loss: 0.5011 - accuracy: 0.76 - ETA: 4s - loss: 0.5018 - accuracy: 0.75 - ETA: 4s - loss: 0.5021 - accuracy: 0.75 - ETA: 4s - loss: 0.5015 - accuracy: 0.75 - ETA: 3s - loss: 0.5025 - accuracy: 0.75 - ETA: 3s - loss: 0.5023 - accuracy: 0.75 - ETA: 3s - loss: 0.5005 - accuracy: 0.75 - ETA: 3s - loss: 0.4997 - accuracy: 0.75 - ETA: 3s - loss: 0.5000 - accuracy: 0.75 - ETA: 3s - loss: 0.4981 - accuracy: 0.75 - ETA: 3s - loss: 0.4973 - accuracy: 0.75 - ETA: 3s - loss: 0.4969 - accuracy: 0.76 - ETA: 3s - loss: 0.4971 - accuracy: 0.76 - ETA: 3s - loss: 0.4976 - accuracy: 0.76 - ETA: 3s - loss: 0.4963 - accuracy: 0.76 - ETA: 3s - loss: 0.4961 - accuracy: 0.76 - ETA: 3s - loss: 0.4964 - accuracy: 0.76 - ETA: 3s - loss: 0.4963 - accuracy: 0.76 - ETA: 3s - loss: 0.4965 - accuracy: 0.76 - ETA: 3s - loss: 0.4968 - accuracy: 0.76 - ETA: 3s - loss: 0.4963 - accuracy: 0.76 - ETA: 3s - loss: 0.4957 - accuracy: 0.76 - ETA: 2s - loss: 0.4954 - accuracy: 0.76 - ETA: 2s - loss: 0.4960 - accuracy: 0.76 - ETA: 2s - loss: 0.4959 - accuracy: 0.76 - ETA: 2s - loss: 0.4958 - accuracy: 0.76 - ETA: 2s - loss: 0.4962 - accuracy: 0.76 - ETA: 2s - loss: 0.4958 - accuracy: 0.76 - ETA: 2s - loss: 0.4948 - accuracy: 0.76 - ETA: 2s - loss: 0.4939 - accuracy: 0.76 - ETA: 2s - loss: 0.4940 - accuracy: 0.76 - ETA: 2s - loss: 0.4938 - accuracy: 0.76 - ETA: 2s - loss: 0.4938 - accuracy: 0.76 - ETA: 2s - loss: 0.4935 - accuracy: 0.76 - ETA: 2s - loss: 0.4934 - accuracy: 0.76 - ETA: 2s - loss: 0.4935 - accuracy: 0.76 - ETA: 2s - loss: 0.4937 - accuracy: 0.76 - ETA: 2s - loss: 0.4936 - accuracy: 0.76 - ETA: 2s - loss: 0.4932 - accuracy: 0.76 - ETA: 2s - loss: 0.4925 - accuracy: 0.76 - ETA: 2s - loss: 0.4918 - accuracy: 0.76 - ETA: 2s - loss: 0.4920 - accuracy: 0.76 - ETA: 1s - loss: 0.4911 - accuracy: 0.76 - ETA: 1s - loss: 0.4917 - accuracy: 0.76 - ETA: 1s - loss: 0.4916 - accuracy: 0.76 - ETA: 1s - loss: 0.4913 - accuracy: 0.76 - ETA: 1s - loss: 0.4916 - accuracy: 0.76 - ETA: 1s - loss: 0.4917 - accuracy: 0.76 - ETA: 1s - loss: 0.4912 - accuracy: 0.76 - ETA: 1s - loss: 0.4917 - accuracy: 0.76 - ETA: 1s - loss: 0.4915 - accuracy: 0.76 - ETA: 1s - loss: 0.4919 - accuracy: 0.76 - ETA: 1s - loss: 0.4918 - accuracy: 0.76 - ETA: 1s - loss: 0.4910 - accuracy: 0.76 - ETA: 1s - loss: 0.4908 - accuracy: 0.76 - ETA: 1s - loss: 0.4905 - accuracy: 0.76 - ETA: 1s - loss: 0.4911 - accuracy: 0.76 - ETA: 1s - loss: 0.4912 - accuracy: 0.76 - ETA: 1s - loss: 0.4908 - accuracy: 0.76 - ETA: 1s - loss: 0.4906 - accuracy: 0.76 - ETA: 0s - loss: 0.4900 - accuracy: 0.76 - ETA: 0s - loss: 0.4903 - accuracy: 0.76 - ETA: 0s - loss: 0.4906 - accuracy: 0.76 - ETA: 0s - loss: 0.4903 - accuracy: 0.76 - ETA: 0s - loss: 0.4898 - accuracy: 0.76 - ETA: 0s - loss: 0.4902 - accuracy: 0.76 - ETA: 0s - loss: 0.4903 - accuracy: 0.76 - ETA: 0s - loss: 0.4895 - accuracy: 0.76 - ETA: 0s - loss: 0.4901 - accuracy: 0.76 - ETA: 0s - loss: 0.4898 - accuracy: 0.76 - ETA: 0s - loss: 0.4894 - accuracy: 0.76 - ETA: 0s - loss: 0.4893 - accuracy: 0.76 - ETA: 0s - loss: 0.4888 - accuracy: 0.76 - ETA: 0s - loss: 0.4889 - accuracy: 0.76 - ETA: 0s - loss: 0.4889 - accuracy: 0.76 - ETA: 0s - loss: 0.4893 - accuracy: 0.76 - ETA: 0s - loss: 0.4894 - accuracy: 0.76 - ETA: 0s - loss: 0.4895 - accuracy: 0.76 - ETA: 0s - loss: 0.4892 - accuracy: 0.76 - 6s 7ms/step - loss: 0.4893 - accuracy: 0.7654\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=10)\n",
    "np.save('history/pretrained_embedding.npy', history.history)\n",
    "model.save(\"models/pretrained_embedding.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {'KerasLayer':hub.KerasLayer}#'accuracy': accuracy}\n",
    "model = keras.models.load_model('models/pretrained_embedding.h5', custom_objects=dependencies)\n",
    "history = np.load('history/pretrained_embedding.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "embed_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Downloading https://files.pythonhosted.org/packages/38/d1/0d8d8f81b318fb3516047af95eb4e853400bb8606a97c307e589b30d052c/tensorflow_addons-0.11.1-cp37-cp37m-win_amd64.whl (915kB)\n",
      "Collecting typeguard>=2.7 (from tensorflow_addons)\n",
      "  Downloading https://files.pythonhosted.org/packages/52/33/3755584541a18d954389447bfd5f9cb7fa20dfbf5094829aee4a103e580c/typeguard-2.9.1-py3-none-any.whl\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.11.1 typeguard-2.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.1.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.random' has no attribute 'Generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-8e92e93ce06e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Local project imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\activations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmish\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftshrink\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftshrink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrrelu\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrrelu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnake\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msnake\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparsemax\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparsemax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\activations\\rrelu.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mseed\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mrng\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m ) -> tf.Tensor:\n\u001b[0;32m     30\u001b[0m     \"\"\"rrelu function.\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.random' has no attribute 'Generator'"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-9308b5cee527>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tfa' is not defined"
     ]
    }
   ],
   "source": [
    "tfa.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.random' has no attribute 'Generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-ef00186aa6df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mencoder_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdecoder_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msequence_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Local project imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\activations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmish\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftshrink\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftshrink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrrelu\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrrelu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnake\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msnake\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparsemax\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparsemax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\activations\\rrelu.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mseed\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mrng\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m ) -> tf.Tensor:\n\u001b[0;32m     30\u001b[0m     \"\"\"rrelu function.\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.random' has no attribute 'Generator'"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "    outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected keras_layer_input to have 1 dimensions, but got array with shape (1000, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-e8527d149635>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mseq_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2410\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2412\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    571\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    574\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected keras_layer_input to have 1 dimensions, but got array with shape (1000, 10)"
     ]
    }
   ],
   "source": [
    "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
    "Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
    "X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n",
    "seq_lengths = np.full([1000], 15)\n",
    "\n",
    "history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Recurrent Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 201\n",
    "max_dims = 512\n",
    "pos_emb = PositionalEncoding(max_steps, max_dims)\n",
    "PE = pos_emb(np.zeros((1, max_steps, max_dims), np.float32))[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1, i2, crop_i = 100, 101, 150\n",
    "p1, p2, p3 = 22, 60, 35\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\n",
    "ax1.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\n",
    "ax1.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\n",
    "ax1.plot(p3, PE[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\n",
    "ax1.plot(PE[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\n",
    "ax1.plot(PE[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\n",
    "ax1.plot([p1, p2], [PE[p1, i1], PE[p2, i1]], \"bo\")\n",
    "ax1.plot([p1, p2], [PE[p1, i2], PE[p2, i2]], \"ro\")\n",
    "ax1.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\n",
    "ax1.set_ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.hlines(0, 0, max_steps - 1, color=\"k\", linewidth=1, alpha=0.3)\n",
    "ax1.axis([0, max_steps - 1, -1, 1])\n",
    "ax2.imshow(PE.T[:crop_i], cmap=\"gray\", interpolation=\"bilinear\", aspect=\"auto\")\n",
    "ax2.hlines(i1, 0, max_steps - 1, color=\"b\")\n",
    "cheat = 2 # need to raise the red line a bit, or else it hides the blue one\n",
    "ax2.hlines(i2+cheat, 0, max_steps - 1, color=\"r\")\n",
    "ax2.plot([p1, p1], [0, crop_i], \"k--\")\n",
    "ax2.plot([p2, p2], [0, crop_i], \"k--\", alpha=0.5)\n",
    "ax2.plot([p1, p2], [i2+cheat, i2+cheat], \"ro\")\n",
    "ax2.plot([p1, p2], [i1, i1], \"bo\")\n",
    "ax2.axis([0, max_steps - 1, 0, crop_i])\n",
    "ax2.set_xlabel(\"$p$\", fontsize=16)\n",
    "ax2.set_ylabel(\"$i$\", rotation=0, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512; max_steps = 500; vocab_size = 10000\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a (very) simplified Transformer (the actual architecture has skip connections, layer norm, dense nets, and most importantly it uses Multi-Head Attention instead of regular Attention):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = encoder_in\n",
    "for N in range(6):\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "\n",
    "encoder_outputs = Z\n",
    "Z = decoder_in\n",
    "for N in range(6):\n",
    "    Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
    "\n",
    "outputs = keras.layers.TimeDistributed(\n",
    "    keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a basic implementation of the `MultiHeadAttention` layer. One will likely be added to `keras.layers` in the near future. Note that `Conv1D` layers with `kernel_size=1` (and the default `padding=\"valid\"` and `strides=1`) is equivalent to a `TimeDistributed(Dense(...))` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    def __init__(self, n_heads, causal=False, use_scale=False, **kwargs):\n",
    "        self.n_heads = n_heads\n",
    "        self.causal = causal\n",
    "        self.use_scale = use_scale\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.dims = batch_input_shape[0][-1]\n",
    "        self.q_dims, self.v_dims, self.k_dims = [self.dims // self.n_heads] * 3 # could be hyperparameters instead\n",
    "        self.q_linear = keras.layers.Conv1D(self.n_heads * self.q_dims, kernel_size=1, use_bias=False)\n",
    "        self.v_linear = keras.layers.Conv1D(self.n_heads * self.v_dims, kernel_size=1, use_bias=False)\n",
    "        self.k_linear = keras.layers.Conv1D(self.n_heads * self.k_dims, kernel_size=1, use_bias=False)\n",
    "        self.attention = keras.layers.Attention(causal=self.causal, use_scale=self.use_scale)\n",
    "        self.out_linear = keras.layers.Conv1D(self.dims, kernel_size=1, use_bias=False)\n",
    "        super().build(batch_input_shape)\n",
    "    def _multi_head_linear(self, inputs, linear):\n",
    "        shape = K.concatenate([K.shape(inputs)[:-1], [self.n_heads, -1]])\n",
    "        projected = K.reshape(linear(inputs), shape)\n",
    "        perm = K.permute_dimensions(projected, [0, 2, 1, 3])\n",
    "        return K.reshape(perm, [shape[0] * self.n_heads, shape[1], -1])\n",
    "    def call(self, inputs):\n",
    "        q = inputs[0]\n",
    "        v = inputs[1]\n",
    "        k = inputs[2] if len(inputs) > 2 else v\n",
    "        shape = K.shape(q)\n",
    "        q_proj = self._multi_head_linear(q, self.q_linear)\n",
    "        v_proj = self._multi_head_linear(v, self.v_linear)\n",
    "        k_proj = self._multi_head_linear(k, self.k_linear)\n",
    "        multi_attended = self.attention([q_proj, v_proj, k_proj])\n",
    "        shape_attended = K.shape(multi_attended)\n",
    "        reshaped_attended = K.reshape(multi_attended, [shape[0], self.n_heads, shape_attended[1], shape_attended[2]])\n",
    "        perm = K.permute_dimensions(reshaped_attended, [0, 2, 1, 3])\n",
    "        concat = K.reshape(perm, [shape[0], shape_attended[1], -1])\n",
    "        return self.out_linear(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.random.rand(2, 50, 512)\n",
    "V = np.random.rand(2, 80, 512)\n",
    "multi_attn = MultiHeadAttention(8)\n",
    "multi_attn([Q, V]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
